{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d467ee1",
   "metadata": {},
   "source": [
    "1.Deep Learning.\n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the\n",
    "ELU activation function.\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but only on\n",
    "digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You\n",
    "will need a softmax output layer with five neurons, and as always make sure to save\n",
    "checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you can\n",
    "achieve.\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it\n",
    "converging faster than before? Does it produce a better model?\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer and try\n",
    "again. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344f223",
   "metadata": {},
   "source": [
    "#a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9040608",
   "metadata": {},
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8f6c7",
   "metadata": {},
   "source": [
    "#b.Using Adam optimization and early stopping, try training it on MNIST but only on\n",
    "digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You\n",
    "will need a softmax output layer with five neurons, and as always make sure to save\n",
    "checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "#Let's complete the graph with the cost function, the training op, and all the other usual components:\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09731b6",
   "metadata": {},
   "source": [
    "#Let's fetch the MNIST dataset:\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
    "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
    "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
    "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187d230",
   "metadata": {},
   "source": [
    "#Now let's create the training set, validation and test set (we need the validation set to implement early stopping):\n",
    "\n",
    "\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf8cd2",
   "metadata": {},
   "source": [
    "0\tValidation loss: 0.128663\tBest loss: 0.128663\tAccuracy: 96.64%\n",
    "1\tValidation loss: 0.448317\tBest loss: 0.128663\tAccuracy: 78.19%\n",
    "2\tValidation loss: 0.190859\tBest loss: 0.128663\tAccuracy: 95.54%\n",
    "3\tValidation loss: 0.146951\tBest loss: 0.128663\tAccuracy: 96.79%\n",
    "4\tValidation loss: 0.086076\tBest loss: 0.086076\tAccuracy: 97.69%\n",
    "5\tValidation loss: 0.115353\tBest loss: 0.086076\tAccuracy: 97.77%\n",
    "6\tValidation loss: 0.239142\tBest loss: 0.086076\tAccuracy: 95.15%\n",
    "7\tValidation loss: 0.088810\tBest loss: 0.086076\tAccuracy: 98.12%\n",
    "8\tValidation loss: 0.108763\tBest loss: 0.086076\tAccuracy: 97.81%\n",
    "9\tValidation loss: 0.300808\tBest loss: 0.086076\tAccuracy: 96.17%\n",
    "10\tValidation loss: 0.179260\tBest loss: 0.086076\tAccuracy: 97.46%\n",
    "11\tValidation loss: 0.125690\tBest loss: 0.086076\tAccuracy: 98.48%\n",
    "12\tValidation loss: 0.738371\tBest loss: 0.086076\tAccuracy: 77.72%\n",
    "13\tValidation loss: 1.894743\tBest loss: 0.086076\tAccuracy: 78.54%\n",
    "14\tValidation loss: 0.415678\tBest loss: 0.086076\tAccuracy: 78.50%\n",
    "15\tValidation loss: 0.537646\tBest loss: 0.086076\tAccuracy: 75.45%\n",
    "16\tValidation loss: 1.009708\tBest loss: 0.086076\tAccuracy: 53.99%\n",
    "17\tValidation loss: 1.228350\tBest loss: 0.086076\tAccuracy: 38.15%\n",
    "18\tValidation loss: 1.510606\tBest loss: 0.086076\tAccuracy: 29.44%\n",
    "19\tValidation loss: 1.632344\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "20\tValidation loss: 1.628246\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "21\tValidation loss: 1.626765\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "22\tValidation loss: 1.651615\tBest loss: 0.086076\tAccuracy: 18.73%\n",
    "23\tValidation loss: 1.663751\tBest loss: 0.086076\tAccuracy: 19.27%\n",
    "24\tValidation loss: 1.675138\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "Early stopping!\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
    "Final test accuracy: 98.05%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d102e6b",
   "metadata": {},
   "source": [
    "#We get 98.05% accuracy on the test set. That's not too bad, but let's see if we can do better by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a11af",
   "metadata": {},
   "source": [
    "#c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "Let's create a DNNClassifier class, compatible with Scikit-Learn's RandomizedSearchCV class, to perform hyperparameter tuning. Here are the key points of this implementation:\n",
    "\n",
    "the __init__() method (constructor) does nothing more than create instance variables for each of the hyperparameters.\n",
    "the fit() method creates the graph, starts a session and trains the model:\n",
    "it calls the _build_graph() method to build the graph (much lile the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.\n",
    "the _dnn() method builds the hidden layers, just like the dnn() function above, but also with support for batch normalization and dropout (for the next exercises).\n",
    "if the fit() method is given a validation set (X_valid and y_valid), then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the _get_model_params() method to get all the graph's variables and their values, and the _restore_model_params() method to restore the variable values (of the best model found). This trick helps speed up training.\n",
    "After the fit() method has finished training the model, it keeps the session open so that predictions can be made quickly, without having to save a model to disk and restore it for every prediction. You can close the session by calling the close_session() method.\n",
    "the predict_proba() method uses the trained model to predict the class probabilities.\n",
    "the predict() method calls predict_proba() and returns the class with the highest probability, for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d08f7f",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "\n",
    "        # extra ops for batch normalization\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0732e",
   "metadata": {},
   "source": [
    "#Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3ed12",
   "metadata": {},
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e79b6",
   "metadata": {},
   "source": [
    "0\tValidation loss: 0.128663\tBest loss: 0.128663\tAccuracy: 96.64%\n",
    "1\tValidation loss: 0.448317\tBest loss: 0.128663\tAccuracy: 78.19%\n",
    "2\tValidation loss: 0.190859\tBest loss: 0.128663\tAccuracy: 95.54%\n",
    "3\tValidation loss: 0.146951\tBest loss: 0.128663\tAccuracy: 96.79%\n",
    "4\tValidation loss: 0.086076\tBest loss: 0.086076\tAccuracy: 97.69%\n",
    "5\tValidation loss: 0.115353\tBest loss: 0.086076\tAccuracy: 97.77%\n",
    "6\tValidation loss: 0.239142\tBest loss: 0.086076\tAccuracy: 95.15%\n",
    "7\tValidation loss: 0.088810\tBest loss: 0.086076\tAccuracy: 98.12%\n",
    "8\tValidation loss: 0.108763\tBest loss: 0.086076\tAccuracy: 97.81%\n",
    "9\tValidation loss: 0.300808\tBest loss: 0.086076\tAccuracy: 96.17%\n",
    "10\tValidation loss: 0.179260\tBest loss: 0.086076\tAccuracy: 97.46%\n",
    "11\tValidation loss: 0.125690\tBest loss: 0.086076\tAccuracy: 98.48%\n",
    "12\tValidation loss: 0.738371\tBest loss: 0.086076\tAccuracy: 77.72%\n",
    "13\tValidation loss: 1.894743\tBest loss: 0.086076\tAccuracy: 78.54%\n",
    "14\tValidation loss: 0.415678\tBest loss: 0.086076\tAccuracy: 78.50%\n",
    "15\tValidation loss: 0.537646\tBest loss: 0.086076\tAccuracy: 75.45%\n",
    "16\tValidation loss: 1.009708\tBest loss: 0.086076\tAccuracy: 53.99%\n",
    "17\tValidation loss: 1.228350\tBest loss: 0.086076\tAccuracy: 38.15%\n",
    "18\tValidation loss: 1.510606\tBest loss: 0.086076\tAccuracy: 29.44%\n",
    "19\tValidation loss: 1.632344\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "20\tValidation loss: 1.628246\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "21\tValidation loss: 1.626765\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "22\tValidation loss: 1.651615\tBest loss: 0.086076\tAccuracy: 18.73%\n",
    "23\tValidation loss: 1.663751\tBest loss: 0.086076\tAccuracy: 19.27%\n",
    "24\tValidation loss: 1.675138\tBest loss: 0.086076\tAccuracy: 22.01%\n",
    "25\tValidation loss: 1.743664\tBest loss: 0.086076\tAccuracy: 18.73%\n",
    "Early stopping!\n",
    "\n",
    "DNNClassifier(activation=<function elu at 0x7f8e3a484268>,\n",
    "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283b52c",
   "metadata": {},
   "source": [
    "#The model is trained, let's see if it gets the same accuracy as earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c942b",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21715b07",
   "metadata": {},
   "source": [
    "0.98054096127651291"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afe2de",
   "metadata": {},
   "source": [
    "It is working fine. Now we can use Scikit-Learn's RandomizedSearchCV class to search for better hyperparameters (this may take over an hour, depending on your system):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4812144",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aad829",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
    "[CV] learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268> \n",
    "0\tValidation loss: 0.132355\tBest loss: 0.132355\tAccuracy: 96.44%\n",
    "1\tValidation loss: 0.126329\tBest loss: 0.126329\tAccuracy: 96.21%\n",
    "2\tValidation loss: 0.138284\tBest loss: 0.126329\tAccuracy: 96.76%\n",
    "3\tValidation loss: 0.142094\tBest loss: 0.126329\tAccuracy: 96.25%\n",
    "4\tValidation loss: 0.128141\tBest loss: 0.126329\tAccuracy: 96.76%\n",
    "5\tValidation loss: 0.119928\tBest loss: 0.119928\tAccuracy: 97.26%\n",
    "6\tValidation loss: 0.137134\tBest loss: 0.119928\tAccuracy: 96.72%\n",
    "7\tValidation loss: 0.156194\tBest loss: 0.119928\tAccuracy: 96.79%\n",
    "8\tValidation loss: 0.283938\tBest loss: 0.119928\tAccuracy: 94.53%\n",
    "9\tValidation loss: 1.104801\tBest loss: 0.119928\tAccuracy: 52.38%\n",
    "10\tValidation loss: 0.966833\tBest loss: 0.119928\tAccuracy: 53.09%\n",
    "11\tValidation loss: 0.854368\tBest loss: 0.119928\tAccuracy: 57.47%\n",
    "12\tValidation loss: 1.857330\tBest loss: 0.119928\tAccuracy: 38.98%\n",
    "13\tValidation loss: 1.642338\tBest loss: 0.119928\tAccuracy: 18.73%\n",
    "14\tValidation loss: 1.612854\tBest loss: 0.119928\tAccuracy: 22.01%\n",
    "15\tValidation loss: 1.617682\tBest loss: 0.119928\tAccuracy: 22.01%\n",
    "16\tValidation loss: 1.616873\tBest loss: 0.119928\tAccuracy: 22.01%\n",
    "17\tValidation loss: 1.618228\tBest loss: 0.119928\tAccuracy: 19.27%\n",
    "18\tValidation loss: 1.619055\tBest loss: 0.119928\tAccuracy: 19.27%\n",
    "19\tValidation loss: 1.643334\tBest loss: 0.119928\tAccuracy: 19.08%\n",
    "20\tValidation loss: 1.621200\tBest loss: 0.119928\tAccuracy: 19.08%\n",
    "21\tValidation loss: 1.629823\tBest loss: 0.119928\tAccuracy: 19.27%\n",
    "22\tValidation loss: 1.624553\tBest loss: 0.119928\tAccuracy: 18.73%\n",
    "23\tValidation loss: 1.610214\tBest loss: 0.119928\tAccuracy: 20.91%\n",
    "24\tValidation loss: 1.621143\tBest loss: 0.119928\tAccuracy: 22.01%\n",
    "25\tValidation loss: 1.623761\tBest loss: 0.119928\tAccuracy: 22.01%\n",
    "26\tValidation loss: 1.641760\tBest loss: 0.119928\tAccuracy: 18.73%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268>, total=   6.5s\n",
    "[CV] learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268> \n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.6s remaining:    0.0s\n",
    "0\tValidation loss: 0.153707\tBest loss: 0.153707\tAccuracy: 95.74%\n",
    "1\tValidation loss: 0.120703\tBest loss: 0.120703\tAccuracy: 96.56%\n",
    "2\tValidation loss: 0.164706\tBest loss: 0.120703\tAccuracy: 96.05%\n",
    "3\tValidation loss: 0.177875\tBest loss: 0.120703\tAccuracy: 95.19%\n",
    "4\tValidation loss: 0.171004\tBest loss: 0.120703\tAccuracy: 95.19%\n",
    "5\tValidation loss: 0.114746\tBest loss: 0.114746\tAccuracy: 96.83%\n",
    "6\tValidation loss: 0.109637\tBest loss: 0.109637\tAccuracy: 97.26%\n",
    "7\tValidation loss: 0.261533\tBest loss: 0.109637\tAccuracy: 94.96%\n",
    "8\tValidation loss: 0.316743\tBest loss: 0.109637\tAccuracy: 94.02%\n",
    "9\tValidation loss: 0.486484\tBest loss: 0.109637\tAccuracy: 77.56%\n",
    "10\tValidation loss: 4.635532\tBest loss: 0.109637\tAccuracy: 53.95%\n",
    "11\tValidation loss: 1.172422\tBest loss: 0.109637\tAccuracy: 48.36%\n",
    "12\tValidation loss: 1.029865\tBest loss: 0.109637\tAccuracy: 55.98%\n",
    "13\tValidation loss: 1.298800\tBest loss: 0.109637\tAccuracy: 36.08%\n",
    "14\tValidation loss: 1.141950\tBest loss: 0.109637\tAccuracy: 38.08%\n",
    "15\tValidation loss: 1.132486\tBest loss: 0.109637\tAccuracy: 38.90%\n",
    "16\tValidation loss: 1.078486\tBest loss: 0.109637\tAccuracy: 45.78%\n",
    "17\tValidation loss: 1.128344\tBest loss: 0.109637\tAccuracy: 45.07%\n",
    "18\tValidation loss: 1.336244\tBest loss: 0.109637\tAccuracy: 34.40%\n",
    "19\tValidation loss: 1.199178\tBest loss: 0.109637\tAccuracy: 39.87%\n",
    "20\tValidation loss: 1.175845\tBest loss: 0.109637\tAccuracy: 40.11%\n",
    "21\tValidation loss: 1.200430\tBest loss: 0.109637\tAccuracy: 40.30%\n",
    "22\tValidation loss: 1.390084\tBest loss: 0.109637\tAccuracy: 34.60%\n",
    "23\tValidation loss: 1.268129\tBest loss: 0.109637\tAccuracy: 40.23%\n",
    "24\tValidation loss: 1.192210\tBest loss: 0.109637\tAccuracy: 40.30%\n",
    "25\tValidation loss: 1.190541\tBest loss: 0.109637\tAccuracy: 41.99%\n",
    "26\tValidation loss: 1.227676\tBest loss: 0.109637\tAccuracy: 38.62%\n",
    "27\tValidation loss: 1.187587\tBest loss: 0.109637\tAccuracy: 39.44%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268>, total=   6.6s\n",
    "[CV] learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268> \n",
    "0\tValidation loss: 0.182619\tBest loss: 0.182619\tAccuracy: 94.29%\n",
    "1\tValidation loss: 0.152706\tBest loss: 0.152706\tAccuracy: 95.97%\n",
    "2\tValidation loss: 0.193820\tBest loss: 0.152706\tAccuracy: 93.82%\n",
    "3\tValidation loss: 0.195413\tBest loss: 0.152706\tAccuracy: 95.54%\n",
    "4\tValidation loss: 0.171277\tBest loss: 0.152706\tAccuracy: 95.19%\n",
    "5\tValidation loss: 0.140087\tBest loss: 0.140087\tAccuracy: 95.70%\n",
    "6\tValidation loss: 0.170798\tBest loss: 0.140087\tAccuracy: 95.00%\n",
    "7\tValidation loss: 0.163649\tBest loss: 0.140087\tAccuracy: 96.29%\n",
    "8\tValidation loss: 0.199048\tBest loss: 0.140087\tAccuracy: 96.09%\n",
    "9\tValidation loss: 1.552870\tBest loss: 0.140087\tAccuracy: 52.15%\n",
    "10\tValidation loss: 0.813273\tBest loss: 0.140087\tAccuracy: 60.40%\n",
    "11\tValidation loss: 0.775555\tBest loss: 0.140087\tAccuracy: 60.67%\n",
    "12\tValidation loss: 0.775275\tBest loss: 0.140087\tAccuracy: 59.77%\n",
    "13\tValidation loss: 0.770521\tBest loss: 0.140087\tAccuracy: 59.30%\n",
    "14\tValidation loss: 0.734035\tBest loss: 0.140087\tAccuracy: 59.85%\n",
    "15\tValidation loss: 0.744980\tBest loss: 0.140087\tAccuracy: 59.66%\n",
    "16\tValidation loss: 0.785848\tBest loss: 0.140087\tAccuracy: 59.66%\n",
    "17\tValidation loss: 0.776138\tBest loss: 0.140087\tAccuracy: 59.42%\n",
    "18\tValidation loss: 0.764496\tBest loss: 0.140087\tAccuracy: 59.46%\n",
    "19\tValidation loss: 0.763633\tBest loss: 0.140087\tAccuracy: 59.54%\n",
    "20\tValidation loss: 0.743879\tBest loss: 0.140087\tAccuracy: 60.75%\n",
    "21\tValidation loss: 0.763295\tBest loss: 0.140087\tAccuracy: 60.36%\n",
    "22\tValidation loss: 0.717175\tBest loss: 0.140087\tAccuracy: 60.63%\n",
    "23\tValidation loss: 1.869954\tBest loss: 0.140087\tAccuracy: 29.28%\n",
    "24\tValidation loss: 1.215518\tBest loss: 0.140087\tAccuracy: 38.86%\n",
    "25\tValidation loss: 1.196626\tBest loss: 0.140087\tAccuracy: 38.62%\n",
    "26\tValidation loss: 1.170714\tBest loss: 0.140087\tAccuracy: 42.38%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=100, n_neurons=10, activation=<function elu at 0x7f8e3a484268>, total=   7.3s\n",
    "[CV] learning_rate=0.02, batch_size=500, n_neurons=30, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 0.171512\tBest loss: 0.171512\tAccuracy: 95.07%\n",
    "1\tValidation loss: 0.095914\tBest loss: 0.095914\tAccuracy: 97.03%\n",
    "2\tValidation loss: 0.099199\tBest loss: 0.095914\tAccuracy: 96.91%\n",
    "3\tValidation loss: 0.093873\tBest loss: 0.093873\tAccuracy: 97.15%\n",
    "4\tValidation loss: 0.073461\tBest loss: 0.073461\tAccuracy: 98.01%\n",
    "5\tValidation loss: 0.084562\tBest loss: 0.073461\tAccuracy: 97.65%\n",
    "6\tValidation loss: 0.071800\tBest loss: 0.071800\tAccuracy: 98.01%\n",
    "7\tValidation loss: 0.088435\tBest loss: 0.071800\tAccuracy: 97.73%\n",
    "8\tValidation loss: 0.082038\tBest loss: 0.071800\tAccuracy: 97.77%\n",
    "9\tValidation loss: 0.080673\tBest loss: 0.071800\tAccuracy: 97.69%\n",
    "10\tValidation loss: 0.081036\tBest loss: 0.071800\tAccuracy: 97.93%\n",
    "11\tValidation loss: 0.092700\tBest loss: 0.071800\tAccuracy: 97.93%\n",
    "12\tValidation loss: 0.081003\tBest loss: 0.071800\tAccuracy: 98.20%\n",
    "13\tValidation loss: 0.075607\tBest loss: 0.071800\tAccuracy: 98.20%\n",
    "14\tValidation loss: 0.092970\tBest loss: 0.071800\tAccuracy: 98.08%\n",
    "15\tValidation loss: 0.108005\tBest loss: 0.071800\tAccuracy: 97.77%\n",
    "16\tValidation loss: 0.082602\tBest loss: 0.071800\tAccuracy: 98.05%\n",
    "17\tValidation loss: 0.114629\tBest loss: 0.071800\tAccuracy: 97.73%\n",
    "18\tValidation loss: 0.099099\tBest loss: 0.071800\tAccuracy: 97.69%\n",
    "19\tValidation loss: 0.075535\tBest loss: 0.071800\tAccuracy: 98.20%\n",
    "20\tValidation loss: 0.102847\tBest loss: 0.071800\tAccuracy: 98.08%\n",
    "21\tValidation loss: 0.089735\tBest loss: 0.071800\tAccuracy: 98.36%\n",
    "22\tValidation loss: 0.080781\tBest loss: 0.071800\tAccuracy: 97.93%\n",
    "23\tValidation loss: 0.073017\tBest loss: 0.071800\tAccuracy: 98.32%\n",
    "24\tValidation loss: 0.091643\tBest loss: 0.071800\tAccuracy: 97.93%\n",
    "25\tValidation loss: 0.113891\tBest loss: 0.071800\tAccuracy: 98.05%\n",
    "26\tValidation loss: 0.094774\tBest loss: 0.071800\tAccuracy: 98.28%\n",
    "27\tValidation loss: 0.086041\tBest loss: 0.071800\tAccuracy: 98.20%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.02, batch_size=500, n_neurons=30, activation=<function relu at 0x7f8e3a410268>, total=   7.9s\n",
    "[CV] learning_rate=0.02, batch_size=500, n_neurons=30, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 0.113188\tBest loss: 0.113188\tAccuracy: 96.60%\n",
    "1\tValidation loss: 0.081384\tBest loss: 0.081384\tAccuracy: 97.58%\n",
    "2\tValidation loss: 0.068770\tBest loss: 0.068770\tAccuracy: 98.12%\n",
    "3\tValidation loss: 0.077316\tBest loss: 0.068770\tAccuracy: 97.73%\n",
    "[...and much later...]\n",
    "43\tValidation loss: 6167.386230\tBest loss: 0.142484\tAccuracy: 91.83%\n",
    "44\tValidation loss: 19455.550781\tBest loss: 0.142484\tAccuracy: 84.75%\n",
    "45\tValidation loss: 8478.734375\tBest loss: 0.142484\tAccuracy: 92.26%\n",
    "46\tValidation loss: 7594.540039\tBest loss: 0.142484\tAccuracy: 91.79%\n",
    "47\tValidation loss: 18661.273438\tBest loss: 0.142484\tAccuracy: 90.15%\n",
    "48\tValidation loss: 10774.322266\tBest loss: 0.142484\tAccuracy: 93.28%\n",
    "49\tValidation loss: 8555.013672\tBest loss: 0.142484\tAccuracy: 92.03%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.1, batch_size=500, n_neurons=90, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e23bc8488>, total=  33.8s\n",
    "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 80.5min finished\n",
    "0\tValidation loss: 0.090732\tBest loss: 0.090732\tAccuracy: 97.22%\n",
    "1\tValidation loss: 0.052198\tBest loss: 0.052198\tAccuracy: 98.40%\n",
    "2\tValidation loss: 0.040040\tBest loss: 0.040040\tAccuracy: 98.94%\n",
    "3\tValidation loss: 0.057495\tBest loss: 0.040040\tAccuracy: 98.55%\n",
    "4\tValidation loss: 0.045600\tBest loss: 0.040040\tAccuracy: 98.75%\n",
    "5\tValidation loss: 0.062344\tBest loss: 0.040040\tAccuracy: 98.48%\n",
    "6\tValidation loss: 0.048719\tBest loss: 0.040040\tAccuracy: 98.67%\n",
    "7\tValidation loss: 0.050346\tBest loss: 0.040040\tAccuracy: 98.79%\n",
    "8\tValidation loss: 0.051224\tBest loss: 0.040040\tAccuracy: 98.79%\n",
    "9\tValidation loss: 0.036505\tBest loss: 0.036505\tAccuracy: 98.98%\n",
    "10\tValidation loss: 0.052532\tBest loss: 0.036505\tAccuracy: 98.71%\n",
    "11\tValidation loss: 0.057086\tBest loss: 0.036505\tAccuracy: 99.10%\n",
    "12\tValidation loss: 0.036754\tBest loss: 0.036505\tAccuracy: 99.06%\n",
    "13\tValidation loss: 0.046782\tBest loss: 0.036505\tAccuracy: 98.87%\n",
    "14\tValidation loss: 0.048929\tBest loss: 0.036505\tAccuracy: 98.91%\n",
    "15\tValidation loss: 0.052919\tBest loss: 0.036505\tAccuracy: 98.75%\n",
    "16\tValidation loss: 0.054287\tBest loss: 0.036505\tAccuracy: 98.67%\n",
    "17\tValidation loss: 0.047722\tBest loss: 0.036505\tAccuracy: 98.79%\n",
    "18\tValidation loss: 0.040474\tBest loss: 0.036505\tAccuracy: 99.14%\n",
    "19\tValidation loss: 0.033867\tBest loss: 0.033867\tAccuracy: 99.14%\n",
    "20\tValidation loss: 0.046808\tBest loss: 0.033867\tAccuracy: 98.83%\n",
    "21\tValidation loss: 0.052966\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "22\tValidation loss: 0.095892\tBest loss: 0.033867\tAccuracy: 98.08%\n",
    "23\tValidation loss: 0.054250\tBest loss: 0.033867\tAccuracy: 98.87%\n",
    "24\tValidation loss: 0.061026\tBest loss: 0.033867\tAccuracy: 98.87%\n",
    "25\tValidation loss: 0.081977\tBest loss: 0.033867\tAccuracy: 98.67%\n",
    "26\tValidation loss: 0.079819\tBest loss: 0.033867\tAccuracy: 98.71%\n",
    "27\tValidation loss: 0.059824\tBest loss: 0.033867\tAccuracy: 98.75%\n",
    "28\tValidation loss: 0.057758\tBest loss: 0.033867\tAccuracy: 98.94%\n",
    "29\tValidation loss: 0.087165\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "30\tValidation loss: 0.052274\tBest loss: 0.033867\tAccuracy: 99.10%\n",
    "31\tValidation loss: 0.059831\tBest loss: 0.033867\tAccuracy: 98.79%\n",
    "32\tValidation loss: 0.054240\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "33\tValidation loss: 0.048165\tBest loss: 0.033867\tAccuracy: 98.94%\n",
    "34\tValidation loss: 0.040565\tBest loss: 0.033867\tAccuracy: 99.18%\n",
    "35\tValidation loss: 0.103207\tBest loss: 0.033867\tAccuracy: 98.28%\n",
    "36\tValidation loss: 400.716797\tBest loss: 0.033867\tAccuracy: 71.46%\n",
    "37\tValidation loss: 11.996887\tBest loss: 0.033867\tAccuracy: 96.09%\n",
    "38\tValidation loss: 2.623182\tBest loss: 0.033867\tAccuracy: 96.56%\n",
    "39\tValidation loss: 1.344962\tBest loss: 0.033867\tAccuracy: 97.69%\n",
    "40\tValidation loss: 1.125381\tBest loss: 0.033867\tAccuracy: 97.42%\n",
    "Early stopping!\n",
    "Out[116]:\n",
    "RandomizedSearchCV(cv=None, error_score='raise',\n",
    "          estimator=DNNClassifier(activation=<function elu at 0x7f8e3a484268>,\n",
    "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42),\n",
    "          fit_params={'n_epochs': 1000, 'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       ...,\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'y_valid': array([0, 4, ..., 1, 2], dtype=uint8)},\n",
    "          iid=True, n_iter=50, n_jobs=1,\n",
    "          param_distributions={'batch_size': [10, 50, 100, 500], 'learning_rate': [0.01, 0.02, 0.05, 0.1], 'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160], 'activation': [<function relu at 0x7f8e3a410268>, <function elu at 0x7f8e3a484268>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e4469d510>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e23bc8488>]},\n",
    "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
    "          return_train_score=True, scoring=None, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3d3e7",
   "metadata": {},
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb861a",
   "metadata": {},
   "source": [
    "{'activation': <function __main__.leaky_relu.<locals>.parametrized_leaky_relu>,\n",
    " 'batch_size': 500,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_neurons': 140}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08019373",
   "metadata": {},
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75025f5",
   "metadata": {},
   "source": [
    "0.99318933644677954"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3ba4a",
   "metadata": {},
   "source": [
    "Tuning the hyperparameters got us up to 99.32% accuracy! It may not sound like a great improvement to go from 98.05% to 99.32% accuracy, but consider the error rate: it went from roughly 2% to 0.7%. That's a 65% reduction of the number of errors this model will produce!\n",
    "\n",
    "It's a good idea to save this model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1aed14",
   "metadata": {},
   "source": [
    "rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c12f5",
   "metadata": {},
   "source": [
    "#d. Now try adding Batch Normalization and compare the learning curves: is it\n",
    "converging faster than before? Does it produce a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874296f",
   "metadata": {},
   "source": [
    "#Let's train the best model found, once again, to see how fast it converges (alternatively, you could tweak the code above to make it write summaries for TensorBoard, so you can visualize the learning curve):\n",
    "\n",
    "\n",
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfcb73",
   "metadata": {},
   "source": [
    "0\tValidation loss: 0.090732\tBest loss: 0.090732\tAccuracy: 97.22%\n",
    "1\tValidation loss: 0.052198\tBest loss: 0.052198\tAccuracy: 98.40%\n",
    "2\tValidation loss: 0.040040\tBest loss: 0.040040\tAccuracy: 98.94%\n",
    "3\tValidation loss: 0.057495\tBest loss: 0.040040\tAccuracy: 98.55%\n",
    "4\tValidation loss: 0.045600\tBest loss: 0.040040\tAccuracy: 98.75%\n",
    "5\tValidation loss: 0.062344\tBest loss: 0.040040\tAccuracy: 98.48%\n",
    "6\tValidation loss: 0.048719\tBest loss: 0.040040\tAccuracy: 98.67%\n",
    "7\tValidation loss: 0.050346\tBest loss: 0.040040\tAccuracy: 98.79%\n",
    "8\tValidation loss: 0.051224\tBest loss: 0.040040\tAccuracy: 98.79%\n",
    "9\tValidation loss: 0.036505\tBest loss: 0.036505\tAccuracy: 98.98%\n",
    "10\tValidation loss: 0.052532\tBest loss: 0.036505\tAccuracy: 98.71%\n",
    "11\tValidation loss: 0.057086\tBest loss: 0.036505\tAccuracy: 99.10%\n",
    "12\tValidation loss: 0.036754\tBest loss: 0.036505\tAccuracy: 99.06%\n",
    "13\tValidation loss: 0.046782\tBest loss: 0.036505\tAccuracy: 98.87%\n",
    "14\tValidation loss: 0.048929\tBest loss: 0.036505\tAccuracy: 98.91%\n",
    "15\tValidation loss: 0.052919\tBest loss: 0.036505\tAccuracy: 98.75%\n",
    "16\tValidation loss: 0.054287\tBest loss: 0.036505\tAccuracy: 98.67%\n",
    "17\tValidation loss: 0.047722\tBest loss: 0.036505\tAccuracy: 98.79%\n",
    "18\tValidation loss: 0.040474\tBest loss: 0.036505\tAccuracy: 99.14%\n",
    "19\tValidation loss: 0.033867\tBest loss: 0.033867\tAccuracy: 99.14%\n",
    "20\tValidation loss: 0.046808\tBest loss: 0.033867\tAccuracy: 98.83%\n",
    "21\tValidation loss: 0.052966\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "22\tValidation loss: 0.095892\tBest loss: 0.033867\tAccuracy: 98.08%\n",
    "23\tValidation loss: 0.054250\tBest loss: 0.033867\tAccuracy: 98.87%\n",
    "24\tValidation loss: 0.061026\tBest loss: 0.033867\tAccuracy: 98.87%\n",
    "25\tValidation loss: 0.081977\tBest loss: 0.033867\tAccuracy: 98.67%\n",
    "26\tValidation loss: 0.079819\tBest loss: 0.033867\tAccuracy: 98.71%\n",
    "27\tValidation loss: 0.059824\tBest loss: 0.033867\tAccuracy: 98.75%\n",
    "28\tValidation loss: 0.057758\tBest loss: 0.033867\tAccuracy: 98.94%\n",
    "29\tValidation loss: 0.087165\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "30\tValidation loss: 0.052274\tBest loss: 0.033867\tAccuracy: 99.10%\n",
    "31\tValidation loss: 0.059831\tBest loss: 0.033867\tAccuracy: 98.79%\n",
    "32\tValidation loss: 0.054240\tBest loss: 0.033867\tAccuracy: 98.91%\n",
    "33\tValidation loss: 0.048165\tBest loss: 0.033867\tAccuracy: 98.94%\n",
    "34\tValidation loss: 0.040565\tBest loss: 0.033867\tAccuracy: 99.18%\n",
    "35\tValidation loss: 0.103207\tBest loss: 0.033867\tAccuracy: 98.28%\n",
    "36\tValidation loss: 400.716797\tBest loss: 0.033867\tAccuracy: 71.46%\n",
    "37\tValidation loss: 11.996887\tBest loss: 0.033867\tAccuracy: 96.09%\n",
    "38\tValidation loss: 2.623182\tBest loss: 0.033867\tAccuracy: 96.56%\n",
    "39\tValidation loss: 1.344962\tBest loss: 0.033867\tAccuracy: 97.69%\n",
    "40\tValidation loss: 1.125381\tBest loss: 0.033867\tAccuracy: 97.42%\n",
    "Early stopping!\n",
    "\n",
    "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e23b8eae8>,\n",
    "       batch_norm_momentum=None, batch_size=500, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=140,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb406b6",
   "metadata": {},
   "source": [
    "#The best loss is reached at epoch 19, but it was already within 10% of that result at epoch 9.\n",
    "\n",
    "#Let's check that we do indeed get 99.32% accuracy on the test set:\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2d5e5",
   "metadata": {},
   "source": [
    "0.99318933644677954"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716bb9d",
   "metadata": {},
   "source": [
    "#now let's use the exact same model, but this time with batch normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ff253",
   "metadata": {},
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aac15c",
   "metadata": {},
   "source": [
    "0\tValidation loss: 1.857111\tBest loss: 1.857111\tAccuracy: 89.64%\n",
    "1\tValidation loss: 1.429800\tBest loss: 1.429800\tAccuracy: 93.94%\n",
    "2\tValidation loss: 0.984367\tBest loss: 0.984367\tAccuracy: 95.86%\n",
    "3\tValidation loss: 1.878914\tBest loss: 0.984367\tAccuracy: 94.68%\n",
    "4\tValidation loss: 4.285415\tBest loss: 0.984367\tAccuracy: 91.87%\n",
    "5\tValidation loss: 3.880259\tBest loss: 0.984367\tAccuracy: 91.91%\n",
    "6\tValidation loss: 7.586149\tBest loss: 0.984367\tAccuracy: 90.38%\n",
    "7\tValidation loss: 7.003541\tBest loss: 0.984367\tAccuracy: 91.05%\n",
    "8\tValidation loss: 3.774627\tBest loss: 0.984367\tAccuracy: 95.00%\n",
    "9\tValidation loss: 9.261741\tBest loss: 0.984367\tAccuracy: 90.66%\n",
    "10\tValidation loss: 4.659614\tBest loss: 0.984367\tAccuracy: 96.36%\n",
    "11\tValidation loss: 7.396369\tBest loss: 0.984367\tAccuracy: 94.21%\n",
    "12\tValidation loss: 10.800961\tBest loss: 0.984367\tAccuracy: 93.28%\n",
    "13\tValidation loss: 10.187258\tBest loss: 0.984367\tAccuracy: 94.68%\n",
    "14\tValidation loss: 12.643840\tBest loss: 0.984367\tAccuracy: 94.53%\n",
    "15\tValidation loss: 7.875628\tBest loss: 0.984367\tAccuracy: 96.13%\n",
    "16\tValidation loss: 30.227692\tBest loss: 0.984367\tAccuracy: 91.01%\n",
    "17\tValidation loss: 13.996559\tBest loss: 0.984367\tAccuracy: 95.62%\n",
    "18\tValidation loss: 14.688783\tBest loss: 0.984367\tAccuracy: 95.39%\n",
    "19\tValidation loss: 17.478910\tBest loss: 0.984367\tAccuracy: 95.66%\n",
    "20\tValidation loss: 20.260157\tBest loss: 0.984367\tAccuracy: 95.50%\n",
    "21\tValidation loss: 26.706875\tBest loss: 0.984367\tAccuracy: 94.18%\n",
    "22\tValidation loss: 28.201635\tBest loss: 0.984367\tAccuracy: 93.78%\n",
    "23\tValidation loss: 33.353935\tBest loss: 0.984367\tAccuracy: 93.98%\n",
    "Early stopping!\n",
    "\n",
    "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e23b8ed90>,\n",
    "       batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c7731",
   "metadata": {},
   "source": [
    "#The best params are reached during epoch 2, that's much faster than earlier. Let's check the accuracy:\n",
    "\n",
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfaee8",
   "metadata": {},
   "source": [
    "0.95504962054874487"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6a33f",
   "metadata": {},
   "source": [
    "Well, batch normalization did not improve accuracy, quite the contrary. Let's see if we can find a good set of hyperparameters that will work well with batch normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6e7ad",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                   fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7df65c",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
    "[CV] learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 6.269045\tBest loss: 6.269045\tAccuracy: 93.24%\n",
    "1\tValidation loss: 25.152470\tBest loss: 6.269045\tAccuracy: 92.18%\n",
    "2\tValidation loss: 64.989517\tBest loss: 6.269045\tAccuracy: 92.46%\n",
    "3\tValidation loss: 60.365185\tBest loss: 6.269045\tAccuracy: 94.88%\n",
    "4\tValidation loss: 72.011459\tBest loss: 6.269045\tAccuracy: 96.79%\n",
    "5\tValidation loss: 428.570953\tBest loss: 6.269045\tAccuracy: 88.04%\n",
    "6\tValidation loss: 457.729675\tBest loss: 6.269045\tAccuracy: 92.81%\n",
    "7\tValidation loss: 555.036926\tBest loss: 6.269045\tAccuracy: 95.70%\n",
    "8\tValidation loss: 853.278564\tBest loss: 6.269045\tAccuracy: 94.45%\n",
    "9\tValidation loss: 1640.808838\tBest loss: 6.269045\tAccuracy: 93.67%\n",
    "10\tValidation loss: 1686.829712\tBest loss: 6.269045\tAccuracy: 95.19%\n",
    "11\tValidation loss: 4588.815430\tBest loss: 6.269045\tAccuracy: 89.95%\n",
    "12\tValidation loss: 3353.238525\tBest loss: 6.269045\tAccuracy: 95.39%\n",
    "13\tValidation loss: 24193.548828\tBest loss: 6.269045\tAccuracy: 85.69%\n",
    "14\tValidation loss: 14681.817383\tBest loss: 6.269045\tAccuracy: 91.48%\n",
    "15\tValidation loss: 10670.790039\tBest loss: 6.269045\tAccuracy: 94.68%\n",
    "16\tValidation loss: 20749.412109\tBest loss: 6.269045\tAccuracy: 88.58%\n",
    "17\tValidation loss: 59897.656250\tBest loss: 6.269045\tAccuracy: 81.98%\n",
    "18\tValidation loss: 18592.509766\tBest loss: 6.269045\tAccuracy: 94.57%\n",
    "19\tValidation loss: 15117.732422\tBest loss: 6.269045\tAccuracy: 96.29%\n",
    "20\tValidation loss: 18595.556641\tBest loss: 6.269045\tAccuracy: 96.05%\n",
    "21\tValidation loss: 43803.089844\tBest loss: 6.269045\tAccuracy: 92.46%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268>, total=  38.7s\n",
    "[CV] learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.1s remaining:    0.0s\n",
    "0\tValidation loss: 10.505345\tBest loss: 10.505345\tAccuracy: 91.05%\n",
    "1\tValidation loss: 27.669804\tBest loss: 10.505345\tAccuracy: 90.30%\n",
    "2\tValidation loss: 70.166283\tBest loss: 10.505345\tAccuracy: 88.90%\n",
    "3\tValidation loss: 61.704323\tBest loss: 10.505345\tAccuracy: 94.92%\n",
    "4\tValidation loss: 175.554169\tBest loss: 10.505345\tAccuracy: 91.36%\n",
    "5\tValidation loss: 303.135834\tBest loss: 10.505345\tAccuracy: 92.65%\n",
    "6\tValidation loss: 589.975830\tBest loss: 10.505345\tAccuracy: 91.09%\n",
    "7\tValidation loss: 890.363159\tBest loss: 10.505345\tAccuracy: 92.77%\n",
    "8\tValidation loss: 1223.559570\tBest loss: 10.505345\tAccuracy: 93.04%\n",
    "9\tValidation loss: 2402.012207\tBest loss: 10.505345\tAccuracy: 91.71%\n",
    "10\tValidation loss: 3208.452393\tBest loss: 10.505345\tAccuracy: 92.53%\n",
    "11\tValidation loss: 3419.600830\tBest loss: 10.505345\tAccuracy: 95.15%\n",
    "12\tValidation loss: 2918.134277\tBest loss: 10.505345\tAccuracy: 94.14%\n",
    "13\tValidation loss: 19868.285156\tBest loss: 10.505345\tAccuracy: 88.12%\n",
    "14\tValidation loss: 24219.689453\tBest loss: 10.505345\tAccuracy: 90.38%\n",
    "15\tValidation loss: 15377.804688\tBest loss: 10.505345\tAccuracy: 91.36%\n",
    "16\tValidation loss: 43667.164062\tBest loss: 10.505345\tAccuracy: 89.41%\n",
    "17\tValidation loss: 66674.023438\tBest loss: 10.505345\tAccuracy: 86.08%\n",
    "18\tValidation loss: 74437.500000\tBest loss: 10.505345\tAccuracy: 87.69%\n",
    "19\tValidation loss: 84437.039062\tBest loss: 10.505345\tAccuracy: 88.12%\n",
    "20\tValidation loss: 56583.324219\tBest loss: 10.505345\tAccuracy: 92.34%\n",
    "21\tValidation loss: 98689.828125\tBest loss: 10.505345\tAccuracy: 89.91%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268>, total=  38.2s\n",
    "[CV] learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 2.343517\tBest loss: 2.343517\tAccuracy: 96.25%\n",
    "1\tValidation loss: 23.652935\tBest loss: 2.343517\tAccuracy: 93.08%\n",
    "2\tValidation loss: 35.758228\tBest loss: 2.343517\tAccuracy: 93.12%\n",
    "3\tValidation loss: 123.383575\tBest loss: 2.343517\tAccuracy: 93.04%\n",
    "4\tValidation loss: 192.505844\tBest loss: 2.343517\tAccuracy: 93.08%\n",
    "5\tValidation loss: 678.043213\tBest loss: 2.343517\tAccuracy: 86.94%\n",
    "6\tValidation loss: 552.234924\tBest loss: 2.343517\tAccuracy: 92.03%\n",
    "7\tValidation loss: 1775.860107\tBest loss: 2.343517\tAccuracy: 89.13%\n",
    "8\tValidation loss: 1147.925293\tBest loss: 2.343517\tAccuracy: 93.20%\n",
    "9\tValidation loss: 2561.473389\tBest loss: 2.343517\tAccuracy: 91.01%\n",
    "10\tValidation loss: 7404.971191\tBest loss: 2.343517\tAccuracy: 85.18%\n",
    "11\tValidation loss: 3112.663330\tBest loss: 2.343517\tAccuracy: 93.35%\n",
    "12\tValidation loss: 8808.947266\tBest loss: 2.343517\tAccuracy: 87.88%\n",
    "13\tValidation loss: 8157.308594\tBest loss: 2.343517\tAccuracy: 90.11%\n",
    "14\tValidation loss: 14201.116211\tBest loss: 2.343517\tAccuracy: 89.37%\n",
    "15\tValidation loss: 43183.285156\tBest loss: 2.343517\tAccuracy: 81.04%\n",
    "16\tValidation loss: 40037.652344\tBest loss: 2.343517\tAccuracy: 84.71%\n",
    "17\tValidation loss: 18612.115234\tBest loss: 2.343517\tAccuracy: 92.46%\n",
    "18\tValidation loss: 50549.546875\tBest loss: 2.343517\tAccuracy: 87.02%\n",
    "19\tValidation loss: 52649.320312\tBest loss: 2.343517\tAccuracy: 90.66%\n",
    "20\tValidation loss: 162720.609375\tBest loss: 2.343517\tAccuracy: 84.36%\n",
    "21\tValidation loss: 78574.101562\tBest loss: 2.343517\tAccuracy: 88.94%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, n_neurons=70, activation=<function relu at 0x7f8e3a410268>, total=  38.0s\n",
    "[CV] learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, n_neurons=90, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e13cb4e18> \n",
    "0\tValidation loss: 60438.898438\tBest loss: 60438.898438\tAccuracy: 81.94%\n",
    "1\tValidation loss: 300987.656250\tBest loss: 60438.898438\tAccuracy: 90.93%\n",
    "2\tValidation loss: 1377872.000000\tBest loss: 60438.898438\tAccuracy: 93.94%\n",
    "3\tValidation loss: 14642961.000000\tBest loss: 60438.898438\tAccuracy: 91.48%\n",
    "4\tValidation loss: 27300228.000000\tBest loss: 60438.898438\tAccuracy: 93.00%\n",
    "5\tValidation loss: 22168134.000000\tBest loss: 60438.898438\tAccuracy: 96.33%\n",
    "6\tValidation loss: 125345760.000000\tBest loss: 60438.898438\tAccuracy: 92.10%\n",
    "7\tValidation loss: 67775336.000000\tBest loss: 60438.898438\tAccuracy: 96.91%\n",
    "8\tValidation loss: 183302928.000000\tBest loss: 60438.898438\tAccuracy: 95.23%\n",
    "9\tValidation loss: 202538928.000000\tBest loss: 60438.898438\tAccuracy: 96.99%\n",
    "10\tValidation loss: 261316368.000000\tBest loss: 60438.898438\tAccuracy: 97.69%\n",
    "11\tValidation loss: 271426848.000000\tBest loss: 60438.898438\tAccuracy: 97.50%\n",
    "12\tValidation loss: 1200596480.000000\tBest loss: 60438.898438\tAccuracy: 95.93%\n",
    "[...and much later...]\n",
    "11\tValidation loss: 169707792.000000\tBest loss: 5764.423340\tAccuracy: 96.60%\n",
    "12\tValidation loss: 729495616.000000\tBest loss: 5764.423340\tAccuracy: 95.04%\n",
    "13\tValidation loss: 1469533312.000000\tBest loss: 5764.423340\tAccuracy: 94.25%\n",
    "14\tValidation loss: 2399959552.000000\tBest loss: 5764.423340\tAccuracy: 96.25%\n",
    "15\tValidation loss: 4667502080.000000\tBest loss: 5764.423340\tAccuracy: 95.74%\n",
    "16\tValidation loss: 4580651520.000000\tBest loss: 5764.423340\tAccuracy: 95.86%\n",
    "17\tValidation loss: 9609373696.000000\tBest loss: 5764.423340\tAccuracy: 95.15%\n",
    "18\tValidation loss: 5375582720.000000\tBest loss: 5764.423340\tAccuracy: 95.93%\n",
    "19\tValidation loss: 88911585280.000000\tBest loss: 5764.423340\tAccuracy: 91.59%\n",
    "20\tValidation loss: 97645641728.000000\tBest loss: 5764.423340\tAccuracy: 92.18%\n",
    "21\tValidation loss: 92258205696.000000\tBest loss: 5764.423340\tAccuracy: 95.04%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, n_neurons=140, activation=<function elu at 0x7f8e3a484268>, total= 1.1min\n",
    "[CV] learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, n_neurons=140, activation=<function elu at 0x7f8e3a484268> \n",
    "0\tValidation loss: 8968.048828\tBest loss: 8968.048828\tAccuracy: 93.86%\n",
    "1\tValidation loss: 39933.992188\tBest loss: 8968.048828\tAccuracy: 92.53%\n",
    "2\tValidation loss: 125085.773438\tBest loss: 8968.048828\tAccuracy: 93.78%\n",
    "3\tValidation loss: 748332.187500\tBest loss: 8968.048828\tAccuracy: 88.43%\n",
    "4\tValidation loss: 2479649.000000\tBest loss: 8968.048828\tAccuracy: 93.51%\n",
    "5\tValidation loss: 4846857.500000\tBest loss: 8968.048828\tAccuracy: 94.18%\n",
    "6\tValidation loss: 29243816.000000\tBest loss: 8968.048828\tAccuracy: 87.72%\n",
    "7\tValidation loss: 130438520.000000\tBest loss: 8968.048828\tAccuracy: 82.92%\n",
    "8\tValidation loss: 163032112.000000\tBest loss: 8968.048828\tAccuracy: 87.06%\n",
    "9\tValidation loss: 295294496.000000\tBest loss: 8968.048828\tAccuracy: 83.54%\n",
    "10\tValidation loss: 1821488640.000000\tBest loss: 8968.048828\tAccuracy: 81.27%\n",
    "11\tValidation loss: 9158688768.000000\tBest loss: 8968.048828\tAccuracy: 76.15%\n",
    "12\tValidation loss: 9291267072.000000\tBest loss: 8968.048828\tAccuracy: 69.55%\n",
    "13\tValidation loss: 7129835520.000000\tBest loss: 8968.048828\tAccuracy: 76.35%\n",
    "14\tValidation loss: 9993897984.000000\tBest loss: 8968.048828\tAccuracy: 83.15%\n",
    "15\tValidation loss: 7655349248.000000\tBest loss: 8968.048828\tAccuracy: 85.81%\n",
    "16\tValidation loss: 27769886720.000000\tBest loss: 8968.048828\tAccuracy: 76.27%\n",
    "17\tValidation loss: 978719735808.000000\tBest loss: 8968.048828\tAccuracy: 49.30%\n",
    "18\tValidation loss: 220747595776.000000\tBest loss: 8968.048828\tAccuracy: 88.15%\n",
    "19\tValidation loss: 320542212096.000000\tBest loss: 8968.048828\tAccuracy: 86.16%\n",
    "20\tValidation loss: 349976592384.000000\tBest loss: 8968.048828\tAccuracy: 82.92%\n",
    "21\tValidation loss: 278264938496.000000\tBest loss: 8968.048828\tAccuracy: 84.68%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, n_neurons=140, activation=<function elu at 0x7f8e3a484268>, total= 1.1min\n",
    "[CV] learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, n_neurons=140, activation=<function elu at 0x7f8e3a484268> \n",
    "0\tValidation loss: 5885.549805\tBest loss: 5885.549805\tAccuracy: 95.11%\n",
    "1\tValidation loss: 80273.859375\tBest loss: 5885.549805\tAccuracy: 90.03%\n",
    "2\tValidation loss: 1050326.375000\tBest loss: 5885.549805\tAccuracy: 69.90%\n",
    "3\tValidation loss: 2969596.500000\tBest loss: 5885.549805\tAccuracy: 77.25%\n",
    "4\tValidation loss: 15399652.000000\tBest loss: 5885.549805\tAccuracy: 71.74%\n",
    "5\tValidation loss: 32421912.000000\tBest loss: 5885.549805\tAccuracy: 78.77%\n",
    "6\tValidation loss: 94322944.000000\tBest loss: 5885.549805\tAccuracy: 72.40%\n",
    "7\tValidation loss: 142864032.000000\tBest loss: 5885.549805\tAccuracy: 79.24%\n",
    "8\tValidation loss: 221855760.000000\tBest loss: 5885.549805\tAccuracy: 89.60%\n",
    "9\tValidation loss: 433116576.000000\tBest loss: 5885.549805\tAccuracy: 85.26%\n",
    "10\tValidation loss: 438715456.000000\tBest loss: 5885.549805\tAccuracy: 91.83%\n",
    "11\tValidation loss: 270049120.000000\tBest loss: 5885.549805\tAccuracy: 96.44%\n",
    "12\tValidation loss: 5670299648.000000\tBest loss: 5885.549805\tAccuracy: 82.10%\n",
    "13\tValidation loss: 4659497472.000000\tBest loss: 5885.549805\tAccuracy: 88.39%\n",
    "14\tValidation loss: 3080572928.000000\tBest loss: 5885.549805\tAccuracy: 90.73%\n",
    "15\tValidation loss: 3663905280.000000\tBest loss: 5885.549805\tAccuracy: 93.90%\n",
    "16\tValidation loss: 8287934976.000000\tBest loss: 5885.549805\tAccuracy: 90.85%\n",
    "17\tValidation loss: 33637832704.000000\tBest loss: 5885.549805\tAccuracy: 85.97%\n",
    "18\tValidation loss: 128785309696.000000\tBest loss: 5885.549805\tAccuracy: 90.07%\n",
    "19\tValidation loss: 42403131392.000000\tBest loss: 5885.549805\tAccuracy: 95.58%\n",
    "20\tValidation loss: 92012290048.000000\tBest loss: 5885.549805\tAccuracy: 92.73%\n",
    "21\tValidation loss: 69790138368.000000\tBest loss: 5885.549805\tAccuracy: 93.35%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, n_neurons=140, activation=<function elu at 0x7f8e3a484268>, total= 1.1min\n",
    "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 135.9min finished\n",
    "0\tValidation loss: 6.603366\tBest loss: 6.603366\tAccuracy: 94.06%\n",
    "1\tValidation loss: 25.367857\tBest loss: 6.603366\tAccuracy: 92.18%\n",
    "2\tValidation loss: 31.812891\tBest loss: 6.603366\tAccuracy: 95.90%\n",
    "3\tValidation loss: 111.553215\tBest loss: 6.603366\tAccuracy: 92.65%\n",
    "4\tValidation loss: 174.170227\tBest loss: 6.603366\tAccuracy: 94.72%\n",
    "5\tValidation loss: 223.264175\tBest loss: 6.603366\tAccuracy: 95.35%\n",
    "6\tValidation loss: 516.788513\tBest loss: 6.603366\tAccuracy: 95.62%\n",
    "7\tValidation loss: 927.823303\tBest loss: 6.603366\tAccuracy: 96.29%\n",
    "8\tValidation loss: 3505.451416\tBest loss: 6.603366\tAccuracy: 89.64%\n",
    "9\tValidation loss: 3493.739746\tBest loss: 6.603366\tAccuracy: 93.86%\n",
    "10\tValidation loss: 3991.661377\tBest loss: 6.603366\tAccuracy: 95.31%\n",
    "11\tValidation loss: 5592.695312\tBest loss: 6.603366\tAccuracy: 94.84%\n",
    "12\tValidation loss: 8464.166992\tBest loss: 6.603366\tAccuracy: 95.54%\n",
    "13\tValidation loss: 14600.711914\tBest loss: 6.603366\tAccuracy: 94.92%\n",
    "14\tValidation loss: 21112.236328\tBest loss: 6.603366\tAccuracy: 95.11%\n",
    "15\tValidation loss: 16540.447266\tBest loss: 6.603366\tAccuracy: 96.95%\n",
    "16\tValidation loss: 16397.814453\tBest loss: 6.603366\tAccuracy: 96.99%\n",
    "17\tValidation loss: 33831.773438\tBest loss: 6.603366\tAccuracy: 96.29%\n",
    "18\tValidation loss: 53113.558594\tBest loss: 6.603366\tAccuracy: 95.39%\n",
    "19\tValidation loss: 172071.625000\tBest loss: 6.603366\tAccuracy: 91.40%\n",
    "20\tValidation loss: 94780.781250\tBest loss: 6.603366\tAccuracy: 95.07%\n",
    "21\tValidation loss: 93082.539062\tBest loss: 6.603366\tAccuracy: 95.43%\n",
    "Early stopping!\n",
    "Out[124]:\n",
    "RandomizedSearchCV(cv=None, error_score='raise',\n",
    "          estimator=DNNClassifier(activation=<function elu at 0x7f8e3a484268>,\n",
    "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42),\n",
    "          fit_params={'n_epochs': 1000, 'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       ...,\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'y_valid': array([0, 4, ..., 1, 2], dtype=uint8)},\n",
    "          iid=True, n_iter=50, n_jobs=1,\n",
    "          param_distributions={'batch_size': [10, 50, 100, 500], 'learning_rate': [0.01, 0.02, 0.05, 0.1], 'batch_norm_momentum': [0.9, 0.95, 0.98, 0.99, 0.999], 'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160], 'activation': [<function relu at 0x7f8e3a410268>, <function elu at 0x7f8e3a484268>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e13cb4e18>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e13cb4f28>]},\n",
    "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
    "          return_train_score=True, scoring=None, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095c3dd",
   "metadata": {},
   "source": [
    "rnd_search_bn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ec22d",
   "metadata": {},
   "source": [
    "{'activation': <function __main__.leaky_relu.<locals>.parametrized_leaky_relu>,\n",
    " 'batch_norm_momentum': 0.999,\n",
    " 'batch_size': 50,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_neurons': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337044b5",
   "metadata": {},
   "source": [
    "y_pred = rnd_search_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e0b30",
   "metadata": {},
   "source": [
    "0.94454173963806187"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625bd9b",
   "metadata": {},
   "source": [
    "Batch normalization did not help in this case. Let's see if dropout can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4f55d",
   "metadata": {},
   "source": [
    "#e.Is the model overfitting the training set? Try adding dropout to every layer and try\n",
    "again. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d386dd",
   "metadata": {},
   "source": [
    "#Since batch normalization did not help, let's go back to the best model we trained earlier and see how it performs on the training set:\n",
    "\n",
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec8634",
   "metadata": {},
   "source": [
    "0.99914401883158566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54db23",
   "metadata": {},
   "source": [
    "#The model performs significantly better on the training set than on the test set (99.91% vs 99.32%), which means it is overfitting the training set. A bit of regularization may help. Let's try adding dropout with a 50% dropout rate:\n",
    "\n",
    "\n",
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75e37e",
   "metadata": {},
   "source": [
    "0\tValidation loss: 0.162759\tBest loss: 0.162759\tAccuracy: 95.15%\n",
    "1\tValidation loss: 0.120510\tBest loss: 0.120510\tAccuracy: 96.64%\n",
    "2\tValidation loss: 0.110715\tBest loss: 0.110715\tAccuracy: 96.91%\n",
    "3\tValidation loss: 0.104193\tBest loss: 0.104193\tAccuracy: 97.22%\n",
    "4\tValidation loss: 0.103560\tBest loss: 0.103560\tAccuracy: 97.81%\n",
    "5\tValidation loss: 0.087045\tBest loss: 0.087045\tAccuracy: 97.89%\n",
    "6\tValidation loss: 0.087227\tBest loss: 0.087045\tAccuracy: 97.65%\n",
    "7\tValidation loss: 0.079840\tBest loss: 0.079840\tAccuracy: 98.16%\n",
    "8\tValidation loss: 0.083102\tBest loss: 0.079840\tAccuracy: 97.50%\n",
    "9\tValidation loss: 0.076794\tBest loss: 0.076794\tAccuracy: 98.01%\n",
    "10\tValidation loss: 0.074914\tBest loss: 0.074914\tAccuracy: 97.93%\n",
    "11\tValidation loss: 0.073794\tBest loss: 0.073794\tAccuracy: 98.12%\n",
    "12\tValidation loss: 0.079777\tBest loss: 0.073794\tAccuracy: 97.89%\n",
    "13\tValidation loss: 0.080277\tBest loss: 0.073794\tAccuracy: 97.54%\n",
    "14\tValidation loss: 0.072409\tBest loss: 0.072409\tAccuracy: 98.08%\n",
    "15\tValidation loss: 0.071988\tBest loss: 0.071988\tAccuracy: 98.12%\n",
    "16\tValidation loss: 0.074609\tBest loss: 0.071988\tAccuracy: 97.93%\n",
    "17\tValidation loss: 0.069488\tBest loss: 0.069488\tAccuracy: 98.28%\n",
    "18\tValidation loss: 0.080863\tBest loss: 0.069488\tAccuracy: 98.40%\n",
    "19\tValidation loss: 0.074966\tBest loss: 0.069488\tAccuracy: 98.20%\n",
    "20\tValidation loss: 0.071082\tBest loss: 0.069488\tAccuracy: 98.12%\n",
    "21\tValidation loss: 0.070138\tBest loss: 0.069488\tAccuracy: 98.20%\n",
    "22\tValidation loss: 0.066032\tBest loss: 0.066032\tAccuracy: 98.28%\n",
    "23\tValidation loss: 0.061130\tBest loss: 0.061130\tAccuracy: 98.36%\n",
    "24\tValidation loss: 0.067107\tBest loss: 0.061130\tAccuracy: 98.16%\n",
    "25\tValidation loss: 0.071372\tBest loss: 0.061130\tAccuracy: 98.16%\n",
    "26\tValidation loss: 0.068535\tBest loss: 0.061130\tAccuracy: 98.36%\n",
    "27\tValidation loss: 0.065336\tBest loss: 0.061130\tAccuracy: 98.48%\n",
    "28\tValidation loss: 0.066783\tBest loss: 0.061130\tAccuracy: 98.40%\n",
    "29\tValidation loss: 0.092769\tBest loss: 0.061130\tAccuracy: 97.77%\n",
    "30\tValidation loss: 0.075746\tBest loss: 0.061130\tAccuracy: 98.01%\n",
    "31\tValidation loss: 0.084024\tBest loss: 0.061130\tAccuracy: 97.81%\n",
    "32\tValidation loss: 0.116428\tBest loss: 0.061130\tAccuracy: 98.44%\n",
    "33\tValidation loss: 0.079498\tBest loss: 0.061130\tAccuracy: 97.89%\n",
    "34\tValidation loss: 0.078189\tBest loss: 0.061130\tAccuracy: 97.97%\n",
    "35\tValidation loss: 0.083723\tBest loss: 0.061130\tAccuracy: 97.81%\n",
    "36\tValidation loss: 0.088210\tBest loss: 0.061130\tAccuracy: 97.19%\n",
    "37\tValidation loss: 0.080040\tBest loss: 0.061130\tAccuracy: 97.93%\n",
    "38\tValidation loss: 0.086932\tBest loss: 0.061130\tAccuracy: 97.89%\n",
    "39\tValidation loss: 0.240580\tBest loss: 0.061130\tAccuracy: 91.67%\n",
    "40\tValidation loss: 0.166662\tBest loss: 0.061130\tAccuracy: 94.29%\n",
    "41\tValidation loss: 0.125562\tBest loss: 0.061130\tAccuracy: 97.15%\n",
    "42\tValidation loss: 0.124890\tBest loss: 0.061130\tAccuracy: 95.82%\n",
    "43\tValidation loss: 0.127020\tBest loss: 0.061130\tAccuracy: 96.76%\n",
    "44\tValidation loss: 0.121540\tBest loss: 0.061130\tAccuracy: 96.05%\n",
    "Early stopping!\n",
    "\n",
    "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e12fd4e18>,\n",
    "       batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd97606",
   "metadata": {},
   "source": [
    "#The best params are reached during epoch 23. Dropout somewhat slowed down convergence.\n",
    "\n",
    "#Let's check the accuracy:\n",
    "\n",
    "\n",
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4a4bd",
   "metadata": {},
   "source": [
    "0.98657326328079398"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772a721",
   "metadata": {},
   "source": [
    "#dropout does not seem to help either. Let's try tuning the hyperparameters, perhaps we can squeeze a bit more performance out of this model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d9d59",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                        fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                        random_state=42, verbose=2)\n",
    "rnd_search_dropout.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16612f",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
    "[CV] learning_rate=0.01, batch_size=100, dropout_rate=0.5, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 0.355079\tBest loss: 0.355079\tAccuracy: 91.44%\n",
    "1\tValidation loss: 0.280624\tBest loss: 0.280624\tAccuracy: 94.10%\n",
    "2\tValidation loss: 0.279819\tBest loss: 0.279819\tAccuracy: 92.77%\n",
    "3\tValidation loss: 0.223614\tBest loss: 0.223614\tAccuracy: 94.10%\n",
    "4\tValidation loss: 0.199802\tBest loss: 0.199802\tAccuracy: 95.11%\n",
    "5\tValidation loss: 0.214481\tBest loss: 0.199802\tAccuracy: 95.47%\n",
    "6\tValidation loss: 0.216195\tBest loss: 0.199802\tAccuracy: 95.78%\n",
    "7\tValidation loss: 0.209172\tBest loss: 0.199802\tAccuracy: 94.80%\n",
    "8\tValidation loss: 0.182841\tBest loss: 0.182841\tAccuracy: 95.70%\n",
    "9\tValidation loss: 0.214252\tBest loss: 0.182841\tAccuracy: 95.82%\n",
    "10\tValidation loss: 0.198762\tBest loss: 0.182841\tAccuracy: 95.62%\n",
    "11\tValidation loss: 0.186415\tBest loss: 0.182841\tAccuracy: 95.82%\n",
    "12\tValidation loss: 0.222924\tBest loss: 0.182841\tAccuracy: 96.05%\n",
    "13\tValidation loss: 0.199636\tBest loss: 0.182841\tAccuracy: 95.97%\n",
    "14\tValidation loss: 0.214436\tBest loss: 0.182841\tAccuracy: 95.97%\n",
    "15\tValidation loss: 0.213507\tBest loss: 0.182841\tAccuracy: 95.47%\n",
    "16\tValidation loss: 0.191497\tBest loss: 0.182841\tAccuracy: 95.78%\n",
    "17\tValidation loss: 0.179503\tBest loss: 0.179503\tAccuracy: 95.93%\n",
    "18\tValidation loss: 0.210343\tBest loss: 0.179503\tAccuracy: 95.74%\n",
    "19\tValidation loss: 0.212626\tBest loss: 0.179503\tAccuracy: 95.27%\n",
    "20\tValidation loss: 0.187110\tBest loss: 0.179503\tAccuracy: 96.09%\n",
    "21\tValidation loss: 0.175171\tBest loss: 0.175171\tAccuracy: 95.78%\n",
    "22\tValidation loss: 0.217172\tBest loss: 0.175171\tAccuracy: 95.66%\n",
    "23\tValidation loss: 0.181060\tBest loss: 0.175171\tAccuracy: 96.44%\n",
    "24\tValidation loss: 0.163630\tBest loss: 0.163630\tAccuracy: 95.93%\n",
    "25\tValidation loss: 0.225873\tBest loss: 0.163630\tAccuracy: 95.58%\n",
    "26\tValidation loss: 0.204975\tBest loss: 0.163630\tAccuracy: 95.66%\n",
    "27\tValidation loss: 0.183588\tBest loss: 0.163630\tAccuracy: 95.97%\n",
    "28\tValidation loss: 0.231080\tBest loss: 0.163630\tAccuracy: 95.11%\n",
    "29\tValidation loss: 0.204342\tBest loss: 0.163630\tAccuracy: 95.74%\n",
    "30\tValidation loss: 0.183963\tBest loss: 0.163630\tAccuracy: 95.93%\n",
    "31\tValidation loss: 0.200975\tBest loss: 0.163630\tAccuracy: 95.23%\n",
    "32\tValidation loss: 0.211165\tBest loss: 0.163630\tAccuracy: 95.23%\n",
    "33\tValidation loss: 0.217777\tBest loss: 0.163630\tAccuracy: 95.07%\n",
    "34\tValidation loss: 0.193184\tBest loss: 0.163630\tAccuracy: 95.39%\n",
    "35\tValidation loss: 0.203809\tBest loss: 0.163630\tAccuracy: 95.58%\n",
    "36\tValidation loss: 0.221673\tBest loss: 0.163630\tAccuracy: 94.57%\n",
    "37\tValidation loss: 0.215750\tBest loss: 0.163630\tAccuracy: 95.39%\n",
    "38\tValidation loss: 0.189653\tBest loss: 0.163630\tAccuracy: 96.09%\n",
    "39\tValidation loss: 0.191333\tBest loss: 0.163630\tAccuracy: 95.19%\n",
    "40\tValidation loss: 0.207714\tBest loss: 0.163630\tAccuracy: 96.01%\n",
    "41\tValidation loss: 0.174490\tBest loss: 0.163630\tAccuracy: 95.39%\n",
    "42\tValidation loss: 0.177445\tBest loss: 0.163630\tAccuracy: 95.82%\n",
    "43\tValidation loss: 0.166708\tBest loss: 0.163630\tAccuracy: 96.09%\n",
    "44\tValidation loss: 0.190829\tBest loss: 0.163630\tAccuracy: 95.70%\n",
    "45\tValidation loss: 0.225985\tBest loss: 0.163630\tAccuracy: 96.25%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.01, batch_size=100, dropout_rate=0.5, n_neurons=70, activation=<function relu at 0x7f8e3a410268>, total=  46.6s\n",
    "[CV] learning_rate=0.01, batch_size=100, dropout_rate=0.5, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.7s remaining:    0.0s\n",
    "0\tValidation loss: 0.748480\tBest loss: 0.748480\tAccuracy: 57.70%\n",
    "1\tValidation loss: 0.516088\tBest loss: 0.516088\tAccuracy: 78.50%\n",
    "2\tValidation loss: 0.448866\tBest loss: 0.448866\tAccuracy: 78.89%\n",
    "3\tValidation loss: 0.435606\tBest loss: 0.435606\tAccuracy: 78.54%\n",
    "4\tValidation loss: 0.435243\tBest loss: 0.435243\tAccuracy: 79.40%\n",
    "5\tValidation loss: 0.450605\tBest loss: 0.435243\tAccuracy: 78.42%\n",
    "6\tValidation loss: 0.430706\tBest loss: 0.430706\tAccuracy: 78.62%\n",
    "7\tValidation loss: 0.449289\tBest loss: 0.430706\tAccuracy: 78.30%\n",
    "8\tValidation loss: 0.413226\tBest loss: 0.413226\tAccuracy: 79.05%\n",
    "9\tValidation loss: 0.436053\tBest loss: 0.413226\tAccuracy: 78.46%\n",
    "10\tValidation loss: 0.459932\tBest loss: 0.413226\tAccuracy: 79.24%\n",
    "11\tValidation loss: 0.424138\tBest loss: 0.413226\tAccuracy: 79.24%\n",
    "12\tValidation loss: 0.409538\tBest loss: 0.409538\tAccuracy: 79.55%\n",
    "13\tValidation loss: 0.416324\tBest loss: 0.409538\tAccuracy: 75.41%\n",
    "14\tValidation loss: 0.440273\tBest loss: 0.409538\tAccuracy: 78.46%\n",
    "15\tValidation loss: 0.435736\tBest loss: 0.409538\tAccuracy: 79.05%\n",
    "16\tValidation loss: 0.428412\tBest loss: 0.409538\tAccuracy: 79.20%\n",
    "17\tValidation loss: 0.450156\tBest loss: 0.409538\tAccuracy: 80.02%\n",
    "18\tValidation loss: 0.421057\tBest loss: 0.409538\tAccuracy: 79.24%\n",
    "19\tValidation loss: 0.442284\tBest loss: 0.409538\tAccuracy: 79.01%\n",
    "20\tValidation loss: 0.426907\tBest loss: 0.409538\tAccuracy: 79.16%\n",
    "21\tValidation loss: 0.439567\tBest loss: 0.409538\tAccuracy: 79.05%\n",
    "22\tValidation loss: 0.452601\tBest loss: 0.409538\tAccuracy: 79.67%\n",
    "23\tValidation loss: 0.424887\tBest loss: 0.409538\tAccuracy: 79.09%\n",
    "24\tValidation loss: 0.441096\tBest loss: 0.409538\tAccuracy: 78.97%\n",
    "25\tValidation loss: 0.417390\tBest loss: 0.409538\tAccuracy: 78.89%\n",
    "26\tValidation loss: 0.418550\tBest loss: 0.409538\tAccuracy: 79.05%\n",
    "27\tValidation loss: 0.426065\tBest loss: 0.409538\tAccuracy: 78.66%\n",
    "28\tValidation loss: 0.413968\tBest loss: 0.409538\tAccuracy: 79.36%\n",
    "29\tValidation loss: 0.425434\tBest loss: 0.409538\tAccuracy: 79.24%\n",
    "30\tValidation loss: 0.455391\tBest loss: 0.409538\tAccuracy: 74.71%\n",
    "31\tValidation loss: 0.429498\tBest loss: 0.409538\tAccuracy: 79.20%\n",
    "32\tValidation loss: 0.427383\tBest loss: 0.409538\tAccuracy: 79.52%\n",
    "33\tValidation loss: 0.422621\tBest loss: 0.409538\tAccuracy: 78.62%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.01, batch_size=100, dropout_rate=0.5, n_neurons=70, activation=<function relu at 0x7f8e3a410268>, total=  30.3s\n",
    "[CV] learning_rate=0.01, batch_size=100, dropout_rate=0.5, n_neurons=70, activation=<function relu at 0x7f8e3a410268> \n",
    "0\tValidation loss: 0.497714\tBest loss: 0.497714\tAccuracy: 86.71%\n",
    "1\tValidation loss: 0.248258\tBest loss: 0.248258\tAccuracy: 93.51%\n",
    "2\tValidation loss: 0.279785\tBest loss: 0.248258\tAccuracy: 93.71%\n",
    "3\tValidation loss: 0.248663\tBest loss: 0.248258\tAccuracy: 94.61%\n",
    "4\tValidation loss: 0.269139\tBest loss: 0.248258\tAccuracy: 94.76%\n",
    "5\tValidation loss: 0.188808\tBest loss: 0.188808\tAccuracy: 95.39%\n",
    "6\tValidation loss: 0.196049\tBest loss: 0.188808\tAccuracy: 95.58%\n",
    "7\tValidation loss: 0.204966\tBest loss: 0.188808\tAccuracy: 95.15%\n",
    "8\tValidation loss: 0.238414\tBest loss: 0.188808\tAccuracy: 94.61%\n",
    "9\tValidation loss: 0.192095\tBest loss: 0.188808\tAccuracy: 95.97%\n",
    "10\tValidation loss: 0.186443\tBest loss: 0.186443\tAccuracy: 95.47%\n",
    "11\tValidation loss: 0.190711\tBest loss: 0.186443\tAccuracy: 95.62%\n",
    "12\tValidation loss: 0.174739\tBest loss: 0.174739\tAccuracy: 95.93%\n",
    "13\tValidation loss: 0.195669\tBest loss: 0.174739\tAccuracy: 95.50%\n",
    "14\tValidation loss: 0.184445\tBest loss: 0.174739\tAccuracy: 95.54%\n",
    "15\tValidation loss: 0.223761\tBest loss: 0.174739\tAccuracy: 95.70%\n",
    "16\tValidation loss: 0.206191\tBest loss: 0.174739\tAccuracy: 95.70%\n",
    "17\tValidation loss: 0.187539\tBest loss: 0.174739\tAccuracy: 96.05%\n",
    "18\tValidation loss: 0.181633\tBest loss: 0.174739\tAccuracy: 96.05%\n",
    "[...and much later...]\n",
    "31\tValidation loss: 1.894134\tBest loss: 1.619874\tAccuracy: 22.01%\n",
    "32\tValidation loss: 1.711688\tBest loss: 1.619874\tAccuracy: 19.08%\n",
    "33\tValidation loss: 1.651240\tBest loss: 1.619874\tAccuracy: 18.73%\n",
    "34\tValidation loss: 1.760639\tBest loss: 1.619874\tAccuracy: 20.91%\n",
    "35\tValidation loss: 1.667938\tBest loss: 1.619874\tAccuracy: 22.01%\n",
    "36\tValidation loss: 1.641116\tBest loss: 1.619874\tAccuracy: 20.91%\n",
    "37\tValidation loss: 1.694960\tBest loss: 1.619874\tAccuracy: 19.08%\n",
    "38\tValidation loss: 1.816517\tBest loss: 1.619874\tAccuracy: 18.73%\n",
    "39\tValidation loss: 1.647246\tBest loss: 1.619874\tAccuracy: 18.73%\n",
    "Early stopping!\n",
    "[CV]  learning_rate=0.05, batch_size=100, dropout_rate=0.5, n_neurons=140, activation=<function elu at 0x7f8e3a484268>, total= 1.0min\n",
    "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 139.0min finished\n",
    "0\tValidation loss: 0.165751\tBest loss: 0.165751\tAccuracy: 95.47%\n",
    "1\tValidation loss: 0.111834\tBest loss: 0.111834\tAccuracy: 96.99%\n",
    "2\tValidation loss: 0.102867\tBest loss: 0.102867\tAccuracy: 96.83%\n",
    "3\tValidation loss: 0.089197\tBest loss: 0.089197\tAccuracy: 97.85%\n",
    "4\tValidation loss: 0.093953\tBest loss: 0.089197\tAccuracy: 97.77%\n",
    "5\tValidation loss: 0.079498\tBest loss: 0.079498\tAccuracy: 98.08%\n",
    "6\tValidation loss: 0.081214\tBest loss: 0.079498\tAccuracy: 98.01%\n",
    "7\tValidation loss: 0.086096\tBest loss: 0.079498\tAccuracy: 97.77%\n",
    "8\tValidation loss: 0.074422\tBest loss: 0.074422\tAccuracy: 97.73%\n",
    "9\tValidation loss: 0.079650\tBest loss: 0.074422\tAccuracy: 97.89%\n",
    "10\tValidation loss: 0.077278\tBest loss: 0.074422\tAccuracy: 97.77%\n",
    "11\tValidation loss: 0.077608\tBest loss: 0.074422\tAccuracy: 98.24%\n",
    "12\tValidation loss: 0.074337\tBest loss: 0.074337\tAccuracy: 98.05%\n",
    "13\tValidation loss: 0.066028\tBest loss: 0.066028\tAccuracy: 98.28%\n",
    "14\tValidation loss: 0.072845\tBest loss: 0.066028\tAccuracy: 98.16%\n",
    "15\tValidation loss: 0.066652\tBest loss: 0.066028\tAccuracy: 98.05%\n",
    "16\tValidation loss: 0.065729\tBest loss: 0.065729\tAccuracy: 98.16%\n",
    "17\tValidation loss: 0.061191\tBest loss: 0.061191\tAccuracy: 98.51%\n",
    "18\tValidation loss: 0.062528\tBest loss: 0.061191\tAccuracy: 98.44%\n",
    "19\tValidation loss: 0.065407\tBest loss: 0.061191\tAccuracy: 98.36%\n",
    "20\tValidation loss: 0.065273\tBest loss: 0.061191\tAccuracy: 98.44%\n",
    "21\tValidation loss: 0.061035\tBest loss: 0.061035\tAccuracy: 98.40%\n",
    "22\tValidation loss: 0.056312\tBest loss: 0.056312\tAccuracy: 98.59%\n",
    "23\tValidation loss: 0.069074\tBest loss: 0.056312\tAccuracy: 98.40%\n",
    "24\tValidation loss: 0.057482\tBest loss: 0.056312\tAccuracy: 98.51%\n",
    "25\tValidation loss: 0.068342\tBest loss: 0.056312\tAccuracy: 98.44%\n",
    "26\tValidation loss: 0.063494\tBest loss: 0.056312\tAccuracy: 98.48%\n",
    "27\tValidation loss: 0.057257\tBest loss: 0.056312\tAccuracy: 98.51%\n",
    "28\tValidation loss: 0.058659\tBest loss: 0.056312\tAccuracy: 98.59%\n",
    "29\tValidation loss: 0.059009\tBest loss: 0.056312\tAccuracy: 98.48%\n",
    "30\tValidation loss: 0.058227\tBest loss: 0.056312\tAccuracy: 98.55%\n",
    "31\tValidation loss: 0.062198\tBest loss: 0.056312\tAccuracy: 98.44%\n",
    "32\tValidation loss: 0.058043\tBest loss: 0.056312\tAccuracy: 98.40%\n",
    "33\tValidation loss: 0.055970\tBest loss: 0.055970\tAccuracy: 98.51%\n",
    "34\tValidation loss: 0.060111\tBest loss: 0.055970\tAccuracy: 98.67%\n",
    "35\tValidation loss: 0.058786\tBest loss: 0.055970\tAccuracy: 98.44%\n",
    "36\tValidation loss: 0.059944\tBest loss: 0.055970\tAccuracy: 98.32%\n",
    "37\tValidation loss: 0.058087\tBest loss: 0.055970\tAccuracy: 98.63%\n",
    "38\tValidation loss: 0.063003\tBest loss: 0.055970\tAccuracy: 98.36%\n",
    "39\tValidation loss: 0.052073\tBest loss: 0.052073\tAccuracy: 98.67%\n",
    "40\tValidation loss: 0.058115\tBest loss: 0.052073\tAccuracy: 98.40%\n",
    "41\tValidation loss: 0.059997\tBest loss: 0.052073\tAccuracy: 98.63%\n",
    "42\tValidation loss: 0.052416\tBest loss: 0.052073\tAccuracy: 98.75%\n",
    "43\tValidation loss: 0.053840\tBest loss: 0.052073\tAccuracy: 98.59%\n",
    "44\tValidation loss: 0.054563\tBest loss: 0.052073\tAccuracy: 98.67%\n",
    "45\tValidation loss: 0.049410\tBest loss: 0.049410\tAccuracy: 98.55%\n",
    "46\tValidation loss: 0.057060\tBest loss: 0.049410\tAccuracy: 98.24%\n",
    "47\tValidation loss: 0.062434\tBest loss: 0.049410\tAccuracy: 98.48%\n",
    "48\tValidation loss: 0.054523\tBest loss: 0.049410\tAccuracy: 98.59%\n",
    "49\tValidation loss: 0.052774\tBest loss: 0.049410\tAccuracy: 98.36%\n",
    "50\tValidation loss: 0.056562\tBest loss: 0.049410\tAccuracy: 98.32%\n",
    "51\tValidation loss: 0.060280\tBest loss: 0.049410\tAccuracy: 98.51%\n",
    "52\tValidation loss: 0.055685\tBest loss: 0.049410\tAccuracy: 98.55%\n",
    "53\tValidation loss: 0.056077\tBest loss: 0.049410\tAccuracy: 98.44%\n",
    "54\tValidation loss: 0.057951\tBest loss: 0.049410\tAccuracy: 98.44%\n",
    "55\tValidation loss: 0.056315\tBest loss: 0.049410\tAccuracy: 98.75%\n",
    "56\tValidation loss: 0.055744\tBest loss: 0.049410\tAccuracy: 98.55%\n",
    "57\tValidation loss: 0.054228\tBest loss: 0.049410\tAccuracy: 98.48%\n",
    "58\tValidation loss: 0.057836\tBest loss: 0.049410\tAccuracy: 98.71%\n",
    "59\tValidation loss: 0.053361\tBest loss: 0.049410\tAccuracy: 98.71%\n",
    "60\tValidation loss: 0.056389\tBest loss: 0.049410\tAccuracy: 98.48%\n",
    "61\tValidation loss: 0.061350\tBest loss: 0.049410\tAccuracy: 98.48%\n",
    "62\tValidation loss: 0.052135\tBest loss: 0.049410\tAccuracy: 98.67%\n",
    "63\tValidation loss: 0.053853\tBest loss: 0.049410\tAccuracy: 98.48%\n",
    "64\tValidation loss: 0.056641\tBest loss: 0.049410\tAccuracy: 98.71%\n",
    "65\tValidation loss: 0.052790\tBest loss: 0.049410\tAccuracy: 98.63%\n",
    "66\tValidation loss: 0.053514\tBest loss: 0.049410\tAccuracy: 98.44%\n",
    "Early stopping!\n",
    "Out[130]:\n",
    "RandomizedSearchCV(cv=None, error_score='raise',\n",
    "          estimator=DNNClassifier(activation=<function elu at 0x7f8e3a484268>,\n",
    "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
    "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7f8e143c3840>,\n",
    "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42),\n",
    "          fit_params={'n_epochs': 1000, 'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       ...,\n",
    "       [ 0.,  0., ...,  0.,  0.],\n",
    "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'y_valid': array([0, 4, ..., 1, 2], dtype=uint8)},\n",
    "          iid=True, n_iter=50, n_jobs=1,\n",
    "          param_distributions={'batch_size': [10, 50, 100, 500], 'learning_rate': [0.01, 0.02, 0.05, 0.1], 'dropout_rate': [0.2, 0.3, 0.4, 0.5, 0.6], 'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160], 'activation': [<function relu at 0x7f8e3a410268>, <function elu at 0x7f8e3a484268>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e12d6f840>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f8e12d6f9d8>]},\n",
    "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
    "          return_train_score=True, scoring=None, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255eb6b2",
   "metadata": {},
   "source": [
    "rnd_search_dropout.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024acd56",
   "metadata": {},
   "source": [
    "{'activation': <function __main__.leaky_relu.<locals>.parametrized_leaky_relu>,\n",
    " 'batch_size': 500,\n",
    " 'dropout_rate': 0.4,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_neurons': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db212d",
   "metadata": {},
   "source": [
    "y_pred = rnd_search_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d0a86",
   "metadata": {},
   "source": [
    "#Neither batch normalization nor dropout improved the model.\n",
    "\n",
    "But that's okay, we have ourselves made a nice DNN that achieves 99.32% accuracy on the test set. Now, let's see if some of its expertise on digits 0 to 4 can be transferred to the task of classifying digits 5 to 9. For the sake of simplicity we will reuse the DNN without BN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b6543",
   "metadata": {},
   "source": [
    "2.Transfer learning.\n",
    "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "model, freezes them, and replaces the softmax output layer with a new one.\n",
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how\n",
    "long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher\n",
    "precision?\n",
    "e. Now unfreeze the top two hidden layers and continue training: can you get the\n",
    "model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a759a50",
   "metadata": {},
   "source": [
    "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "model, freezes them, and replaces the softmax output layer with a new one.\n",
    "\n",
    "Let's load the best model's graph and get a handle on all the important operations we will need. Note that instead of creating a new softmax output layer, we will just reuse the existing one (since it has the same number of outputs as the existing one). We will reinitialize its parameters before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b215d",
   "metadata": {},
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf76f50",
   "metadata": {},
   "source": [
    "#To freeze the lower layers, we will exclude their variables from the optimizer's list of trainable variables, keeping only #the output layer's trainable variables:\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d57a8",
   "metadata": {},
   "source": [
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how\n",
    "long it takes. Despite this small number of examples, can you achieve high precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50415eeb",
   "metadata": {},
   "source": [
    "#Let's create the training, validation and test sets. We need to subtract 5 from the labels because TensorFlow expects #integers from 0 to n_classes-1.\n",
    "\n",
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df91a2d",
   "metadata": {},
   "source": [
    "#Also, for the purpose of this exercise, we want to keep only 100 instances per class in the training set (and let's keep #only 30 instances per class in the validation set). Let's create a small function to do that:\n",
    "\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fec7e9",
   "metadata": {},
   "source": [
    "Now let's train the model. This is the same training code as earlier, using early stopping, except for the initialization: we first initialize all the variables, then we restore the best model trained earlier (on digits 0 to 4), and finally we reinitialize the output layer variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50072d",
   "metadata": {},
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aaefd7",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
    "0\tValidation loss: 1.361167\tBest loss: 1.361167\tAccuracy: 43.33%\n",
    "1\tValidation loss: 1.154602\tBest loss: 1.154602\tAccuracy: 57.33%\n",
    "2\tValidation loss: 1.054218\tBest loss: 1.054218\tAccuracy: 53.33%\n",
    "3\tValidation loss: 0.981128\tBest loss: 0.981128\tAccuracy: 62.67%\n",
    "4\tValidation loss: 0.995353\tBest loss: 0.981128\tAccuracy: 59.33%\n",
    "5\tValidation loss: 0.967000\tBest loss: 0.967000\tAccuracy: 65.33%\n",
    "6\tValidation loss: 0.955700\tBest loss: 0.955700\tAccuracy: 61.33%\n",
    "7\tValidation loss: 1.015331\tBest loss: 0.955700\tAccuracy: 58.67%\n",
    "8\tValidation loss: 0.978280\tBest loss: 0.955700\tAccuracy: 62.00%\n",
    "9\tValidation loss: 0.923389\tBest loss: 0.923389\tAccuracy: 69.33%\n",
    "10\tValidation loss: 0.996236\tBest loss: 0.923389\tAccuracy: 63.33%\n",
    "11\tValidation loss: 0.976757\tBest loss: 0.923389\tAccuracy: 62.67%\n",
    "12\tValidation loss: 0.969096\tBest loss: 0.923389\tAccuracy: 63.33%\n",
    "13\tValidation loss: 1.023069\tBest loss: 0.923389\tAccuracy: 63.33%\n",
    "14\tValidation loss: 1.104664\tBest loss: 0.923389\tAccuracy: 55.33%\n",
    "15\tValidation loss: 0.950175\tBest loss: 0.923389\tAccuracy: 65.33%\n",
    "16\tValidation loss: 1.002944\tBest loss: 0.923389\tAccuracy: 63.33%\n",
    "17\tValidation loss: 0.895543\tBest loss: 0.895543\tAccuracy: 70.67%\n",
    "18\tValidation loss: 0.961151\tBest loss: 0.895543\tAccuracy: 66.67%\n",
    "19\tValidation loss: 0.896372\tBest loss: 0.895543\tAccuracy: 67.33%\n",
    "20\tValidation loss: 0.911938\tBest loss: 0.895543\tAccuracy: 69.33%\n",
    "21\tValidation loss: 0.929007\tBest loss: 0.895543\tAccuracy: 68.00%\n",
    "22\tValidation loss: 0.939231\tBest loss: 0.895543\tAccuracy: 65.33%\n",
    "23\tValidation loss: 0.919057\tBest loss: 0.895543\tAccuracy: 68.67%\n",
    "24\tValidation loss: 0.994529\tBest loss: 0.895543\tAccuracy: 65.33%\n",
    "25\tValidation loss: 0.901279\tBest loss: 0.895543\tAccuracy: 68.67%\n",
    "26\tValidation loss: 0.916238\tBest loss: 0.895543\tAccuracy: 68.67%\n",
    "27\tValidation loss: 1.007434\tBest loss: 0.895543\tAccuracy: 65.33%\n",
    "28\tValidation loss: 0.924729\tBest loss: 0.895543\tAccuracy: 70.00%\n",
    "29\tValidation loss: 0.974399\tBest loss: 0.895543\tAccuracy: 66.00%\n",
    "30\tValidation loss: 0.899418\tBest loss: 0.895543\tAccuracy: 68.00%\n",
    "31\tValidation loss: 0.940563\tBest loss: 0.895543\tAccuracy: 66.00%\n",
    "32\tValidation loss: 0.920235\tBest loss: 0.895543\tAccuracy: 68.00%\n",
    "33\tValidation loss: 0.929848\tBest loss: 0.895543\tAccuracy: 68.67%\n",
    "34\tValidation loss: 0.930288\tBest loss: 0.895543\tAccuracy: 66.67%\n",
    "35\tValidation loss: 0.943884\tBest loss: 0.895543\tAccuracy: 64.67%\n",
    "36\tValidation loss: 0.939372\tBest loss: 0.895543\tAccuracy: 68.00%\n",
    "37\tValidation loss: 0.894239\tBest loss: 0.894239\tAccuracy: 67.33%\n",
    "38\tValidation loss: 0.888806\tBest loss: 0.888806\tAccuracy: 69.33%\n",
    "39\tValidation loss: 0.933829\tBest loss: 0.888806\tAccuracy: 66.00%\n",
    "40\tValidation loss: 0.911836\tBest loss: 0.888806\tAccuracy: 72.67%\n",
    "41\tValidation loss: 0.896729\tBest loss: 0.888806\tAccuracy: 70.00%\n",
    "42\tValidation loss: 0.929394\tBest loss: 0.888806\tAccuracy: 68.00%\n",
    "43\tValidation loss: 0.919418\tBest loss: 0.888806\tAccuracy: 69.33%\n",
    "44\tValidation loss: 0.907830\tBest loss: 0.888806\tAccuracy: 65.33%\n",
    "45\tValidation loss: 1.004304\tBest loss: 0.888806\tAccuracy: 71.33%\n",
    "46\tValidation loss: 0.871899\tBest loss: 0.871899\tAccuracy: 74.00%\n",
    "47\tValidation loss: 0.904889\tBest loss: 0.871899\tAccuracy: 67.33%\n",
    "48\tValidation loss: 0.914138\tBest loss: 0.871899\tAccuracy: 66.00%\n",
    "49\tValidation loss: 0.930001\tBest loss: 0.871899\tAccuracy: 69.33%\n",
    "50\tValidation loss: 0.962153\tBest loss: 0.871899\tAccuracy: 68.67%\n",
    "51\tValidation loss: 0.925021\tBest loss: 0.871899\tAccuracy: 65.33%\n",
    "52\tValidation loss: 0.974412\tBest loss: 0.871899\tAccuracy: 67.33%\n",
    "53\tValidation loss: 0.897499\tBest loss: 0.871899\tAccuracy: 68.67%\n",
    "54\tValidation loss: 0.933581\tBest loss: 0.871899\tAccuracy: 60.67%\n",
    "55\tValidation loss: 0.988574\tBest loss: 0.871899\tAccuracy: 68.67%\n",
    "56\tValidation loss: 0.927290\tBest loss: 0.871899\tAccuracy: 66.67%\n",
    "57\tValidation loss: 1.018713\tBest loss: 0.871899\tAccuracy: 64.00%\n",
    "58\tValidation loss: 0.964709\tBest loss: 0.871899\tAccuracy: 66.00%\n",
    "59\tValidation loss: 1.004696\tBest loss: 0.871899\tAccuracy: 59.33%\n",
    "60\tValidation loss: 1.008746\tBest loss: 0.871899\tAccuracy: 58.67%\n",
    "61\tValidation loss: 0.948558\tBest loss: 0.871899\tAccuracy: 68.00%\n",
    "62\tValidation loss: 0.966037\tBest loss: 0.871899\tAccuracy: 64.00%\n",
    "63\tValidation loss: 0.922541\tBest loss: 0.871899\tAccuracy: 68.00%\n",
    "64\tValidation loss: 0.892541\tBest loss: 0.871899\tAccuracy: 72.00%\n",
    "65\tValidation loss: 0.890340\tBest loss: 0.871899\tAccuracy: 70.67%\n",
    "66\tValidation loss: 0.957904\tBest loss: 0.871899\tAccuracy: 66.00%\n",
    "Early stopping!\n",
    "Total training time: 1.9s\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
    "Final test accuracy: 64.02%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da9e2f",
   "metadata": {},
   "source": [
    "That is not a good accuracy. With such a tiny training set, and with only one layer to tweak, we should not expect a good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868e3ae",
   "metadata": {},
   "source": [
    "c. Try caching the frozen layers, and train the model again: how much faster is it now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e8783",
   "metadata": {},
   "source": [
    "#Let's start by getting a handle on the output of the last frozen layer:\n",
    "\n",
    "import tensorflow as tf\n",
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117199eb",
   "metadata": {},
   "source": [
    "Now let's train the model using roughly the same code as earlier. The difference is that we compute the output of the top frozen layer at the beginning (both for the training set and the validation set), and we cache it. This makes training roughly 1.5 to 3 times faster in this example (this may vary greatly, depending on your system):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3dedaa",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6f71c",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
    "0\tValidation loss: 1.416103\tBest loss: 1.416103\tAccuracy: 44.00%\n",
    "1\tValidation loss: 1.099216\tBest loss: 1.099216\tAccuracy: 53.33%\n",
    "2\tValidation loss: 1.024954\tBest loss: 1.024954\tAccuracy: 59.33%\n",
    "3\tValidation loss: 0.969193\tBest loss: 0.969193\tAccuracy: 60.00%\n",
    "4\tValidation loss: 0.973461\tBest loss: 0.969193\tAccuracy: 64.67%\n",
    "5\tValidation loss: 0.949333\tBest loss: 0.949333\tAccuracy: 64.67%\n",
    "6\tValidation loss: 0.922953\tBest loss: 0.922953\tAccuracy: 66.67%\n",
    "7\tValidation loss: 0.957186\tBest loss: 0.922953\tAccuracy: 62.67%\n",
    "8\tValidation loss: 0.950264\tBest loss: 0.922953\tAccuracy: 68.00%\n",
    "9\tValidation loss: 1.053465\tBest loss: 0.922953\tAccuracy: 59.33%\n",
    "10\tValidation loss: 1.069949\tBest loss: 0.922953\tAccuracy: 54.00%\n",
    "11\tValidation loss: 0.965197\tBest loss: 0.922953\tAccuracy: 62.67%\n",
    "12\tValidation loss: 0.949233\tBest loss: 0.922953\tAccuracy: 63.33%\n",
    "13\tValidation loss: 0.926229\tBest loss: 0.922953\tAccuracy: 63.33%\n",
    "14\tValidation loss: 0.922854\tBest loss: 0.922854\tAccuracy: 67.33%\n",
    "15\tValidation loss: 0.965205\tBest loss: 0.922854\tAccuracy: 66.67%\n",
    "16\tValidation loss: 1.050026\tBest loss: 0.922854\tAccuracy: 59.33%\n",
    "17\tValidation loss: 0.946699\tBest loss: 0.922854\tAccuracy: 64.67%\n",
    "18\tValidation loss: 0.973966\tBest loss: 0.922854\tAccuracy: 64.00%\n",
    "19\tValidation loss: 0.902573\tBest loss: 0.902573\tAccuracy: 66.67%\n",
    "20\tValidation loss: 0.933625\tBest loss: 0.902573\tAccuracy: 65.33%\n",
    "21\tValidation loss: 0.938296\tBest loss: 0.902573\tAccuracy: 64.00%\n",
    "22\tValidation loss: 0.938790\tBest loss: 0.902573\tAccuracy: 66.67%\n",
    "23\tValidation loss: 0.936572\tBest loss: 0.902573\tAccuracy: 68.00%\n",
    "24\tValidation loss: 1.039109\tBest loss: 0.902573\tAccuracy: 65.33%\n",
    "25\tValidation loss: 1.146837\tBest loss: 0.902573\tAccuracy: 59.33%\n",
    "26\tValidation loss: 0.958702\tBest loss: 0.902573\tAccuracy: 68.67%\n",
    "27\tValidation loss: 0.915434\tBest loss: 0.902573\tAccuracy: 70.67%\n",
    "28\tValidation loss: 0.915402\tBest loss: 0.902573\tAccuracy: 66.00%\n",
    "29\tValidation loss: 0.920591\tBest loss: 0.902573\tAccuracy: 70.67%\n",
    "30\tValidation loss: 1.029216\tBest loss: 0.902573\tAccuracy: 64.67%\n",
    "31\tValidation loss: 1.039922\tBest loss: 0.902573\tAccuracy: 55.33%\n",
    "32\tValidation loss: 0.925041\tBest loss: 0.902573\tAccuracy: 64.00%\n",
    "33\tValidation loss: 0.944033\tBest loss: 0.902573\tAccuracy: 67.33%\n",
    "34\tValidation loss: 0.941914\tBest loss: 0.902573\tAccuracy: 66.67%\n",
    "35\tValidation loss: 0.866297\tBest loss: 0.866297\tAccuracy: 69.33%\n",
    "36\tValidation loss: 0.900787\tBest loss: 0.866297\tAccuracy: 70.67%\n",
    "37\tValidation loss: 0.889670\tBest loss: 0.866297\tAccuracy: 66.67%\n",
    "38\tValidation loss: 0.968139\tBest loss: 0.866297\tAccuracy: 62.00%\n",
    "39\tValidation loss: 0.929764\tBest loss: 0.866297\tAccuracy: 66.00%\n",
    "40\tValidation loss: 0.889130\tBest loss: 0.866297\tAccuracy: 68.00%\n",
    "41\tValidation loss: 0.940024\tBest loss: 0.866297\tAccuracy: 70.00%\n",
    "42\tValidation loss: 0.896472\tBest loss: 0.866297\tAccuracy: 69.33%\n",
    "43\tValidation loss: 0.893887\tBest loss: 0.866297\tAccuracy: 67.33%\n",
    "44\tValidation loss: 0.925727\tBest loss: 0.866297\tAccuracy: 68.67%\n",
    "45\tValidation loss: 0.945748\tBest loss: 0.866297\tAccuracy: 66.00%\n",
    "46\tValidation loss: 0.897087\tBest loss: 0.866297\tAccuracy: 70.00%\n",
    "47\tValidation loss: 0.923855\tBest loss: 0.866297\tAccuracy: 68.67%\n",
    "48\tValidation loss: 0.944244\tBest loss: 0.866297\tAccuracy: 66.67%\n",
    "49\tValidation loss: 0.975582\tBest loss: 0.866297\tAccuracy: 66.67%\n",
    "50\tValidation loss: 0.889869\tBest loss: 0.866297\tAccuracy: 68.67%\n",
    "51\tValidation loss: 0.895552\tBest loss: 0.866297\tAccuracy: 69.33%\n",
    "52\tValidation loss: 0.943707\tBest loss: 0.866297\tAccuracy: 66.00%\n",
    "53\tValidation loss: 0.902883\tBest loss: 0.866297\tAccuracy: 70.67%\n",
    "54\tValidation loss: 0.958292\tBest loss: 0.866297\tAccuracy: 68.67%\n",
    "55\tValidation loss: 0.917368\tBest loss: 0.866297\tAccuracy: 67.33%\n",
    "Early stopping!\n",
    "Total training time: 1.1s\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
    "Final test accuracy: 61.16%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be491dd5",
   "metadata": {},
   "source": [
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher\n",
    "precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94271044",
   "metadata": {},
   "source": [
    "#Let's load the best model again, but this time we will create a new softmax output layer on top of the 4th hidden layer:\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_outputs = 5\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80dd64",
   "metadata": {},
   "source": [
    "#And now let's create the training operation. We want to freeze all the layers except for the new output layer:\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd78d7d",
   "metadata": {},
   "source": [
    "And once again we train the model with the same code as earlier. Note: we could of course write a function once and use it multiple times, rather than copying almost the same training code over and over again, but as we keep tweaking the code slightly, the function would need multiple arguments and if statements, and it would have to be at the beginning of the notebook, where it would not make much sense to readers. In short it would be very confusing, so we're better off with copy and paste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16644c78",
   "metadata": {},
   "source": [
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3747b2",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
    "0\tValidation loss: 1.073254\tBest loss: 1.073254\tAccuracy: 51.33%\n",
    "1\tValidation loss: 1.039487\tBest loss: 1.039487\tAccuracy: 64.00%\n",
    "2\tValidation loss: 0.991418\tBest loss: 0.991418\tAccuracy: 59.33%\n",
    "3\tValidation loss: 0.902691\tBest loss: 0.902691\tAccuracy: 64.67%\n",
    "4\tValidation loss: 0.919874\tBest loss: 0.902691\tAccuracy: 63.33%\n",
    "5\tValidation loss: 0.879734\tBest loss: 0.879734\tAccuracy: 72.00%\n",
    "6\tValidation loss: 0.877940\tBest loss: 0.877940\tAccuracy: 70.67%\n",
    "7\tValidation loss: 0.899513\tBest loss: 0.877940\tAccuracy: 71.33%\n",
    "8\tValidation loss: 0.879717\tBest loss: 0.877940\tAccuracy: 67.33%\n",
    "9\tValidation loss: 0.826527\tBest loss: 0.826527\tAccuracy: 75.33%\n",
    "10\tValidation loss: 0.890165\tBest loss: 0.826527\tAccuracy: 67.33%\n",
    "11\tValidation loss: 0.876235\tBest loss: 0.826527\tAccuracy: 68.67%\n",
    "12\tValidation loss: 0.877598\tBest loss: 0.826527\tAccuracy: 71.33%\n",
    "13\tValidation loss: 0.898070\tBest loss: 0.826527\tAccuracy: 74.67%\n",
    "14\tValidation loss: 0.923526\tBest loss: 0.826527\tAccuracy: 68.00%\n",
    "15\tValidation loss: 0.859624\tBest loss: 0.826527\tAccuracy: 70.00%\n",
    "16\tValidation loss: 0.896264\tBest loss: 0.826527\tAccuracy: 67.33%\n",
    "17\tValidation loss: 0.800813\tBest loss: 0.800813\tAccuracy: 73.33%\n",
    "18\tValidation loss: 0.811318\tBest loss: 0.800813\tAccuracy: 74.00%\n",
    "19\tValidation loss: 0.809687\tBest loss: 0.800813\tAccuracy: 75.33%\n",
    "20\tValidation loss: 0.807125\tBest loss: 0.800813\tAccuracy: 72.67%\n",
    "21\tValidation loss: 0.819150\tBest loss: 0.800813\tAccuracy: 71.33%\n",
    "22\tValidation loss: 0.849812\tBest loss: 0.800813\tAccuracy: 76.67%\n",
    "23\tValidation loss: 0.801709\tBest loss: 0.800813\tAccuracy: 74.67%\n",
    "24\tValidation loss: 0.832877\tBest loss: 0.800813\tAccuracy: 74.00%\n",
    "25\tValidation loss: 0.792853\tBest loss: 0.792853\tAccuracy: 72.67%\n",
    "26\tValidation loss: 0.842031\tBest loss: 0.792853\tAccuracy: 76.00%\n",
    "27\tValidation loss: 0.872236\tBest loss: 0.792853\tAccuracy: 71.33%\n",
    "28\tValidation loss: 0.782557\tBest loss: 0.782557\tAccuracy: 78.00%\n",
    "29\tValidation loss: 0.802515\tBest loss: 0.782557\tAccuracy: 73.33%\n",
    "30\tValidation loss: 0.812652\tBest loss: 0.782557\tAccuracy: 72.67%\n",
    "31\tValidation loss: 0.825467\tBest loss: 0.782557\tAccuracy: 76.00%\n",
    "32\tValidation loss: 0.791320\tBest loss: 0.782557\tAccuracy: 76.67%\n",
    "33\tValidation loss: 0.785207\tBest loss: 0.782557\tAccuracy: 77.33%\n",
    "34\tValidation loss: 0.815450\tBest loss: 0.782557\tAccuracy: 76.67%\n",
    "35\tValidation loss: 0.865081\tBest loss: 0.782557\tAccuracy: 71.33%\n",
    "36\tValidation loss: 0.852323\tBest loss: 0.782557\tAccuracy: 74.67%\n",
    "37\tValidation loss: 0.836967\tBest loss: 0.782557\tAccuracy: 72.00%\n",
    "38\tValidation loss: 0.807404\tBest loss: 0.782557\tAccuracy: 77.33%\n",
    "39\tValidation loss: 0.821566\tBest loss: 0.782557\tAccuracy: 75.33%\n",
    "40\tValidation loss: 0.817326\tBest loss: 0.782557\tAccuracy: 76.00%\n",
    "41\tValidation loss: 0.807987\tBest loss: 0.782557\tAccuracy: 70.67%\n",
    "42\tValidation loss: 0.838029\tBest loss: 0.782557\tAccuracy: 74.00%\n",
    "43\tValidation loss: 0.820425\tBest loss: 0.782557\tAccuracy: 76.00%\n",
    "44\tValidation loss: 0.785871\tBest loss: 0.782557\tAccuracy: 76.00%\n",
    "45\tValidation loss: 0.844337\tBest loss: 0.782557\tAccuracy: 78.67%\n",
    "46\tValidation loss: 0.764127\tBest loss: 0.764127\tAccuracy: 78.67%\n",
    "47\tValidation loss: 0.789726\tBest loss: 0.764127\tAccuracy: 77.33%\n",
    "48\tValidation loss: 0.839190\tBest loss: 0.764127\tAccuracy: 72.67%\n",
    "49\tValidation loss: 0.849353\tBest loss: 0.764127\tAccuracy: 75.33%\n",
    "50\tValidation loss: 0.869818\tBest loss: 0.764127\tAccuracy: 74.00%\n",
    "51\tValidation loss: 0.805526\tBest loss: 0.764127\tAccuracy: 76.67%\n",
    "52\tValidation loss: 0.850749\tBest loss: 0.764127\tAccuracy: 72.67%\n",
    "53\tValidation loss: 0.838693\tBest loss: 0.764127\tAccuracy: 71.33%\n",
    "54\tValidation loss: 0.791396\tBest loss: 0.764127\tAccuracy: 75.33%\n",
    "55\tValidation loss: 0.846888\tBest loss: 0.764127\tAccuracy: 76.00%\n",
    "56\tValidation loss: 0.826717\tBest loss: 0.764127\tAccuracy: 74.67%\n",
    "57\tValidation loss: 0.878286\tBest loss: 0.764127\tAccuracy: 70.67%\n",
    "58\tValidation loss: 0.878869\tBest loss: 0.764127\tAccuracy: 72.67%\n",
    "59\tValidation loss: 0.822241\tBest loss: 0.764127\tAccuracy: 72.67%\n",
    "60\tValidation loss: 0.864925\tBest loss: 0.764127\tAccuracy: 73.33%\n",
    "61\tValidation loss: 0.804545\tBest loss: 0.764127\tAccuracy: 73.33%\n",
    "62\tValidation loss: 0.891784\tBest loss: 0.764127\tAccuracy: 72.67%\n",
    "63\tValidation loss: 0.810186\tBest loss: 0.764127\tAccuracy: 74.00%\n",
    "64\tValidation loss: 0.810786\tBest loss: 0.764127\tAccuracy: 74.67%\n",
    "65\tValidation loss: 0.818044\tBest loss: 0.764127\tAccuracy: 74.00%\n",
    "66\tValidation loss: 0.853420\tBest loss: 0.764127\tAccuracy: 74.67%\n",
    "Early stopping!\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
    "Final test accuracy: 69.10%\n",
    "Still not fantastic, but much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cdc11",
   "metadata": {},
   "source": [
    "e. Now unfreeze the top two hidden layers and continue training: can you get the\n",
    "model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa87e5",
   "metadata": {},
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver()\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = two_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242fd02",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
    "0\tValidation loss: 1.054859\tBest loss: 1.054859\tAccuracy: 74.00%\n",
    "1\tValidation loss: 0.812410\tBest loss: 0.812410\tAccuracy: 78.00%\n",
    "2\tValidation loss: 0.750377\tBest loss: 0.750377\tAccuracy: 80.67%\n",
    "3\tValidation loss: 0.570973\tBest loss: 0.570973\tAccuracy: 84.67%\n",
    "4\tValidation loss: 0.805442\tBest loss: 0.570973\tAccuracy: 79.33%\n",
    "5\tValidation loss: 0.920925\tBest loss: 0.570973\tAccuracy: 80.00%\n",
    "6\tValidation loss: 0.817471\tBest loss: 0.570973\tAccuracy: 81.33%\n",
    "7\tValidation loss: 0.777876\tBest loss: 0.570973\tAccuracy: 84.00%\n",
    "8\tValidation loss: 1.030498\tBest loss: 0.570973\tAccuracy: 74.67%\n",
    "9\tValidation loss: 1.074356\tBest loss: 0.570973\tAccuracy: 81.33%\n",
    "10\tValidation loss: 0.912521\tBest loss: 0.570973\tAccuracy: 83.33%\n",
    "11\tValidation loss: 1.356695\tBest loss: 0.570973\tAccuracy: 79.33%\n",
    "12\tValidation loss: 0.918798\tBest loss: 0.570973\tAccuracy: 82.00%\n",
    "13\tValidation loss: 0.971029\tBest loss: 0.570973\tAccuracy: 82.67%\n",
    "14\tValidation loss: 0.860108\tBest loss: 0.570973\tAccuracy: 83.33%\n",
    "15\tValidation loss: 1.074813\tBest loss: 0.570973\tAccuracy: 82.00%\n",
    "16\tValidation loss: 0.867760\tBest loss: 0.570973\tAccuracy: 84.00%\n",
    "17\tValidation loss: 0.858290\tBest loss: 0.570973\tAccuracy: 85.33%\n",
    "18\tValidation loss: 0.996560\tBest loss: 0.570973\tAccuracy: 85.33%\n",
    "19\tValidation loss: 1.304507\tBest loss: 0.570973\tAccuracy: 83.33%\n",
    "20\tValidation loss: 1.134808\tBest loss: 0.570973\tAccuracy: 80.67%\n",
    "21\tValidation loss: 1.189581\tBest loss: 0.570973\tAccuracy: 82.00%\n",
    "22\tValidation loss: 1.131344\tBest loss: 0.570973\tAccuracy: 81.33%\n",
    "23\tValidation loss: 1.240507\tBest loss: 0.570973\tAccuracy: 82.67%\n",
    "Early stopping!\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen\n",
    "Final test accuracy: 78.09%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc06d5f",
   "metadata": {},
   "source": [
    "#Let's check what accuracy we can get by unfreezing all layers:\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam4\")\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "no_frozen_saver = tf.train.Saver()\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = no_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    no_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3971354",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen\n",
    "0\tValidation loss: 0.863416\tBest loss: 0.863416\tAccuracy: 86.00%\n",
    "1\tValidation loss: 0.695079\tBest loss: 0.695079\tAccuracy: 90.00%\n",
    "2\tValidation loss: 0.402921\tBest loss: 0.402921\tAccuracy: 92.00%\n",
    "3\tValidation loss: 0.606936\tBest loss: 0.402921\tAccuracy: 92.00%\n",
    "4\tValidation loss: 0.354645\tBest loss: 0.354645\tAccuracy: 90.67%\n",
    "5\tValidation loss: 0.376935\tBest loss: 0.354645\tAccuracy: 90.67%\n",
    "6\tValidation loss: 0.593208\tBest loss: 0.354645\tAccuracy: 90.00%\n",
    "7\tValidation loss: 0.388302\tBest loss: 0.354645\tAccuracy: 92.67%\n",
    "8\tValidation loss: 0.503276\tBest loss: 0.354645\tAccuracy: 91.33%\n",
    "9\tValidation loss: 1.440716\tBest loss: 0.354645\tAccuracy: 80.00%\n",
    "10\tValidation loss: 0.464323\tBest loss: 0.354645\tAccuracy: 92.00%\n",
    "11\tValidation loss: 0.410302\tBest loss: 0.354645\tAccuracy: 93.33%\n",
    "12\tValidation loss: 1.131754\tBest loss: 0.354645\tAccuracy: 88.00%\n",
    "13\tValidation loss: 0.511544\tBest loss: 0.354645\tAccuracy: 92.00%\n",
    "14\tValidation loss: 0.402083\tBest loss: 0.354645\tAccuracy: 94.00%\n",
    "15\tValidation loss: 1.149943\tBest loss: 0.354645\tAccuracy: 92.00%\n",
    "16\tValidation loss: 0.405171\tBest loss: 0.354645\tAccuracy: 94.00%\n",
    "17\tValidation loss: 0.304346\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "18\tValidation loss: 0.386952\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "19\tValidation loss: 0.387063\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "20\tValidation loss: 0.384417\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "21\tValidation loss: 0.381116\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "22\tValidation loss: 0.379346\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "23\tValidation loss: 0.378128\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "24\tValidation loss: 0.376642\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "25\tValidation loss: 0.375432\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "26\tValidation loss: 0.374804\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "27\tValidation loss: 0.373952\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "28\tValidation loss: 0.373471\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "29\tValidation loss: 0.373027\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "30\tValidation loss: 0.373124\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "31\tValidation loss: 0.373098\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "32\tValidation loss: 0.373206\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "33\tValidation loss: 0.372812\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "34\tValidation loss: 0.373109\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "35\tValidation loss: 0.372616\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "36\tValidation loss: 0.372491\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "37\tValidation loss: 0.372270\tBest loss: 0.304346\tAccuracy: 94.67%\n",
    "Early stopping!\n",
    "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_no_frozen\n",
    "Final test accuracy: 91.34%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76c486",
   "metadata": {},
   "source": [
    "#Let's compare that to a DNN trained from scratch:\n",
    "\n",
    "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\n",
    "dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1c70b",
   "metadata": {},
   "source": [
    "0\tValidation loss: 0.674618\tBest loss: 0.674618\tAccuracy: 80.67%\n",
    "1\tValidation loss: 0.584845\tBest loss: 0.584845\tAccuracy: 88.67%\n",
    "2\tValidation loss: 0.647296\tBest loss: 0.584845\tAccuracy: 84.00%\n",
    "3\tValidation loss: 0.530389\tBest loss: 0.530389\tAccuracy: 87.33%\n",
    "4\tValidation loss: 0.683215\tBest loss: 0.530389\tAccuracy: 90.67%\n",
    "5\tValidation loss: 0.538040\tBest loss: 0.530389\tAccuracy: 89.33%\n",
    "6\tValidation loss: 0.670196\tBest loss: 0.530389\tAccuracy: 90.67%\n",
    "7\tValidation loss: 0.836470\tBest loss: 0.530389\tAccuracy: 85.33%\n",
    "8\tValidation loss: 0.837684\tBest loss: 0.530389\tAccuracy: 92.67%\n",
    "9\tValidation loss: 0.588950\tBest loss: 0.530389\tAccuracy: 88.00%\n",
    "10\tValidation loss: 0.643213\tBest loss: 0.530389\tAccuracy: 90.67%\n",
    "11\tValidation loss: 1.010521\tBest loss: 0.530389\tAccuracy: 88.00%\n",
    "12\tValidation loss: 0.931423\tBest loss: 0.530389\tAccuracy: 90.00%\n",
    "13\tValidation loss: 1.563524\tBest loss: 0.530389\tAccuracy: 88.67%\n",
    "14\tValidation loss: 2.340119\tBest loss: 0.530389\tAccuracy: 89.33%\n",
    "15\tValidation loss: 1.402095\tBest loss: 0.530389\tAccuracy: 88.00%\n",
    "16\tValidation loss: 1.269974\tBest loss: 0.530389\tAccuracy: 86.00%\n",
    "17\tValidation loss: 1.036325\tBest loss: 0.530389\tAccuracy: 89.33%\n",
    "18\tValidation loss: 1.578565\tBest loss: 0.530389\tAccuracy: 88.67%\n",
    "19\tValidation loss: 0.993890\tBest loss: 0.530389\tAccuracy: 93.33%\n",
    "20\tValidation loss: 0.958130\tBest loss: 0.530389\tAccuracy: 87.33%\n",
    "21\tValidation loss: 1.505322\tBest loss: 0.530389\tAccuracy: 88.67%\n",
    "22\tValidation loss: 1.378772\tBest loss: 0.530389\tAccuracy: 89.33%\n",
    "23\tValidation loss: 0.999445\tBest loss: 0.530389\tAccuracy: 88.00%\n",
    "24\tValidation loss: 2.366345\tBest loss: 0.530389\tAccuracy: 90.00%\n",
    "Early stopping!\n",
    "DNNClassifier(activation=<function elu at 0x1243639d8>,\n",
    "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
    "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828>,\n",
    "       learning_rate=0.01, n_hidden_layers=4, n_neurons=100,\n",
    "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32cfb0",
   "metadata": {},
   "source": [
    "y_pred = dnn_clf_5_to_9.predict(X_test2)\n",
    "accuracy_score(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e00f9",
   "metadata": {},
   "source": [
    "0.8481793869574161"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372775e",
   "metadata": {},
   "source": [
    "Transfer learning allowed us to go from 84.8% accuracy to 91.3%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6ad97",
   "metadata": {},
   "source": [
    "3.Pretraining on an auxiliary task.\n",
    "a. In this exercise you will build a DNN that compares two MNIST digit images and\n",
    "predicts whether they represent the same digit or not. Then you will reuse the lower\n",
    "layers of this network to train an MNIST classifier using very little training data. Start\n",
    "by building two DNNs (let’s call them DNN A and B), both similar to the one you built\n",
    "earlier but without the output layer: each DNN should have five hidden layers of 100\n",
    "neurons each, He initialization, and ELU activation. Next, add one more hidden layer\n",
    "with 10 units on top of both DNNs. To do this, you should use\n",
    "TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs\n",
    "for each instance, then feed the result to the hidden layer. Finally, add an output\n",
    "layer with a single neuron using the logistic activation function.\n",
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images,\n",
    "and split #2 should contain contain 5,000 images. Create a function that generates a\n",
    "training batch where each instance is a pair of MNIST images picked from split #1.\n",
    "Half of the training instances should be pairs of images that belong to the same\n",
    "class, while the other half should be images from different classes. For each pair, the\n",
    "\n",
    "training label should be 0 if the images are from the same class, or 1 if they are from\n",
    "different classes.\n",
    "c. Train the DNN on this training set. For each image pair, you can simultaneously feed\n",
    "the first image to DNN A and the second image to DNN B. The whole network will\n",
    "gradually learn to tell whether two images belong to the same class or not.\n",
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and\n",
    "adding a softmax output layer on top with 10 neurons. Train this network on split #2\n",
    "and see if you can achieve high performance despite having only 500 images per\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbbc3a",
   "metadata": {},
   "source": [
    "a. In this exercise you will build a DNN that compares two MNIST digit images and\n",
    "predicts whether they represent the same digit or not. Then you will reuse the lower\n",
    "layers of this network to train an MNIST classifier using very little training data. Start\n",
    "by building two DNNs (let’s call them DNN A and B), both similar to the one you built\n",
    "earlier but without the output layer: each DNN should have five hidden layers of 100\n",
    "neurons each, He initialization, and ELU activation. Next, add one more hidden layer\n",
    "with 10 units on top of both DNNs. To do this, you should use\n",
    "TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs\n",
    "for each instance, then feed the result to the hidden layer. Finally, add an output\n",
    "layer with a single neuron using the logistic activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c474321",
   "metadata": {},
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0e746",
   "metadata": {},
   "source": [
    "#We also need the labels placeholder. Each label will be 0 if the images represent different digits, or 1 if they represent the same digit:\n",
    "\n",
    "y = tf.placeholder(tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa1aaa",
   "metadata": {},
   "source": [
    "#Now let's feed these inputs through two separate DNNs:\n",
    "\n",
    "dnn1 = dnn(X1, name=\"DNN_A\")\n",
    "dnn2 = dnn(X2, name=\"DNN_B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37ab29",
   "metadata": {},
   "source": [
    "#And let's concatenate their outputs:\n",
    "\n",
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7981a",
   "metadata": {},
   "source": [
    "#Each DNN outputs 100 activations (per instance), so the shape is [None, 100]:\n",
    "\n",
    "dnn1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ecb5b",
   "metadata": {},
   "source": [
    "TensorShape([Dimension(None), Dimension(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8738de1",
   "metadata": {},
   "source": [
    "dnn2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482067",
   "metadata": {},
   "source": [
    "TensorShape([Dimension(None), Dimension(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f781c9",
   "metadata": {},
   "source": [
    "#And of course the concatenated outputs have a shape of [None, 200]:\n",
    "\n",
    "dnn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfd65d",
   "metadata": {},
   "source": [
    "TensorShape([Dimension(None), Dimension(200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7bb23",
   "metadata": {},
   "source": [
    "#Now lets add an extra hidden layer with just 10 neurons, and the output layer, with a single neuron:\n",
    "\n",
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc583e40",
   "metadata": {},
   "source": [
    "#The whole network predicts 1 if y_proba >= 0.5 (i.e. the network predicts that the images represent the same digit), or 0 #otherwise. We compute instead logits >= 0, which is equivalent but faster to compute:\n",
    "\n",
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b57dd",
   "metadata": {},
   "source": [
    "#Now let's add the cost function:\n",
    "\n",
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d2f9d",
   "metadata": {},
   "source": [
    "#And we can now create the training operation using an optimizer:\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762d075",
   "metadata": {},
   "source": [
    "#We will want to measure our classifier's accuracy.\n",
    "\n",
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfce7d",
   "metadata": {},
   "source": [
    "#And the usual init and saver:\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c7d1b",
   "metadata": {},
   "source": [
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images,\n",
    "and split #2 should contain contain 5,000 images. Create a function that generates a\n",
    "training batch where each instance is a pair of MNIST images picked from split #1.\n",
    "Half of the training instances should be pairs of images that belong to the same\n",
    "class, while the other half should be images from different classes. For each pair, the\n",
    "\n",
    "training label should be 0 if the images are from the same class, or 1 if they are from\n",
    "different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fcc95",
   "metadata": {},
   "source": [
    "The MNIST dataset returned by TensorFlow's input_data() function is already split into 3 parts: a training set (55,000 instances), a validation set (5,000 instances) and a test set (10,000 instances). Let's use the first set to generate the training set composed image pairs, and we will use the second set for the second phase of the exercise (to train a regular MNIST classifier). We will use the third set as the test set for both phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278bcfe",
   "metadata": {},
   "source": [
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "\n",
    "X_test = X_test\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3876",
   "metadata": {},
   "source": [
    "Let's write a function that generates pairs of images: 50% representing the same digit, and 50% representing different digits. There are many ways to implement this. In this implementation, we first decide how many \"same\" pairs (i.e. pairs of images representing the same digit) we will generate, and how many \"different\" pairs (i.e. pairs of images representing different digits). We could just use batch_size // 2 but we want to handle the case where it is odd (granted, that might be overkill!). Then we generate random pairs and we pick the right number of \"same\" pairs, then we generate the right number of \"different\" pairs. Finally we shuffle the batch and return it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "715643c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fac2a",
   "metadata": {},
   "source": [
    "#Let's test it to generate a small batch of 5 image pairs:\n",
    "\n",
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c706a",
   "metadata": {},
   "source": [
    "#Each row in X_batch contains a pair of images:\n",
    "\n",
    "X_batch.shape, X_batch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681b5c8",
   "metadata": {},
   "source": [
    "((5, 2, 784), dtype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15178f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10164/2224571705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"nearest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m122\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAANSCAYAAABBeWEhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPnklEQVR4nO3ZX4yld13H8c/HXZoo/sHYVXG3NTVZqWsCBsaCF2oNUXd7szHhosVIbEw2NdR4Sa/ggisvTAyhsNmQDeGG3khwJQu9Uy6wplMDpQspGUqk45J0CwaDGJuFrxczmnGY7Zw9c6ZbeL9fySTzPM/vnPPNefc5c/bXzkz0o+3HbvUAOnxGBjAygJEBjAxgZIB9I7e92PaFts/c4HrbfqDtRtun27559WPqIBa5kz+a5PTLXD+T5OT2z7kkHz74WFqlfSPPzGeTfOtllpxN8rHZ8kSS17V9/aoG1MEdXcFzHE/y/I7jze1z39i9sO25bN3tee1rX/uWu+++ewUvz/DUU0+9ODPHlnnsKiJ3j3N77pXOzIUkF5JkbW1t1tfXV/DyDG3/ddnHruLb9WaSO3Ycn0hydQXPqxVZReRLSd61/S37bUm+PTM/8FGtW2ffj+u2H09yb5Lb224meV+S1yTJzJxPcjnJfUk2knw3yYOHNayWs2/kmXlgn+uT5N0rm0gr544XgJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRARaK3PZ022fbbrR9ZI/rP9P279t+oe2Vtg+uflQta9/IbY8keTTJmSSnkjzQ9tSuZe9O8qWZeVOSe5P8ddvbVjyrlrTInXxPko2ZeW5mXkryWJKzu9ZMkp9q2yQ/meRbSa6vdFItbZHIx5M8v+N4c/vcTh9M8mtJrib5YpK/nJnv736itufarrddv3bt2pIj62YtErl7nJtdx3+Y5PNJfinJbyT5YNuf/oEHzVyYmbWZWTt27NhNjqplLRJ5M8kdO45PZOuO3enBJJ+YLRtJvpbk7tWMqINaJPKTSU62vWv7y9T9SS7tWvP1JG9Pkra/kOQNSZ5b5aBa3tH9FszM9bYPJ3k8yZEkF2fmStuHtq+fT/L+JB9t+8Vsfby/Z2ZePMS5dRP2jZwkM3M5yeVd587v+P1qkj9Y7WhaFXe8AIwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDLBQ5Lan2z7bdqPtIzdYc2/bz7e90vYfVzumDuLofgvaHknyaJLfT7KZ5Mm2l2bmSzvWvC7Jh5Kcnpmvt/35Q5pXS1jkTr4nycbMPDczLyV5LMnZXWvemeQTM/P1JJmZF1Y7pg5ikcjHkzy/43hz+9xOv5rkZ9v+Q9un2r5rVQPq4Pb9uE7SPc7NHs/zliRvT/LjSf6p7RMz85X/90TtuSTnkuTOO++8+Wm1lEXu5M0kd+w4PpHk6h5rPjMz/zkzLyb5bJI37X6imbkwM2szs3bs2LFlZ9ZNWiTyk0lOtr2r7W1J7k9yadeav0vy222Ptv2JJG9N8uXVjqpl7ftxPTPX2z6c5PEkR5JcnJkrbR/avn5+Zr7c9jNJnk7y/SQfmZlnDnNwLa4zu/+8vjLW1tZmfX39lrz2D6O2T83M2jKPdccLwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAAtFbnu67bNtN9o+8jLrfrPt99q+Y3Uj6qD2jdz2SJJHk5xJcirJA21P3WDdXyV5fNVD6mAWuZPvSbIxM8/NzEtJHktydo91f5Hkb5O8sML5tAKLRD6e5Pkdx5vb5/5P2+NJ/ijJ+dWNplVZJHL3ODe7jv8myXtm5nsv+0TtubbrbdevXbu24Ig6qKMLrNlMcseO4xNJru5as5bksbZJcnuS+9pen5lP7lw0MxeSXEiStbW13f+h6JAsEvnJJCfb3pXk35Lcn+SdOxfMzF3/+3vbjyb51O7AunX2jTwz19s+nK1vzUeSXJyZK20f2r7u3+FXuUXu5MzM5SSXd53bM+7M/OnBx9IqueMFYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDLBS57em2z7bdaPvIHtf/uO3T2z+fa/um1Y+qZe0bue2RJI8mOZPkVJIH2p7atexrSX53Zt6Y5P1JLqx6UC1vkTv5niQbM/PczLyU5LEkZ3cumJnPzcy/bx8+keTEasfUQSwS+XiS53ccb26fu5E/S/LpvS60Pdd2ve36tWvXFp9SB7JI5O5xbvZc2P5etiK/Z6/rM3NhZtZmZu3YsWOLT6kDObrAms0kd+w4PpHk6u5Fbd+Y5CNJzszMN1cznlZhkTv5ySQn297V9rYk9ye5tHNB2zuTfCLJn8zMV1Y/pg5i3zt5Zq63fTjJ40mOJLk4M1faPrR9/XyS9yb5uSQfapsk12dm7fDG1s3ozJ5/Xg/d2trarK+v35LX/mHU9qllbxx3vACMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAywUOS2p9s+23aj7SN7XG/bD2xff7rtm1c/qpa1b+S2R5I8muRMklNJHmh7ateyM0lObv+cS/LhFc+pA1jkTr4nycbMPDczLyV5LMnZXWvOJvnYbHkiyevavn7Fs2pJRxdYczzJ8zuON5O8dYE1x5N8Y+eitueydacnyX+3feampn1l3J7kxVs9xB7esOwDF4ncPc7NEmsyMxeSXEiStuszs7bA67+iXs1zLfvYRT6uN5PcseP4RJKrS6zRLbJI5CeTnGx7V9vbktyf5NKuNZeSvGv7W/bbknx7Zr6x+4l0a+z7cT0z19s+nOTxJEeSXJyZK20f2r5+PsnlJPcl2Ujy3SQPLvDaF5ae+nD9yM3VmR/406kfMe54ARgZ4NAjv1q3RBeY69623277+e2f974CM11s+8KN9g+Wfq9m5tB+svVF7atJfiXJbUm+kOTUrjX3Jfl0tv6t/bYk/3yYM93EXPcm+dRhz7LrNX8nyZuTPHOD60u9V4d9J79at0QXmesVNzOfTfKtl1my1Ht12JFvtN15s2tuxVxJ8lttv9D2021//ZBnWsRS79Ui25oHsbIt0RVb5DX/Jckvz8x32t6X5JPZ+r9st9JS79Vh38mv1i3RfV9zZv5jZr6z/fvlJK9pe/shz7Wfpd6rw478at0S3Xeutr/Yttu/35Ot9+qbhzzXfpZ6rw714/oQt0RfibnekeTP215P8l9J7p9D3h5s+/Fsfau/ve1mkvclec2OmZZ6r9zWBHDHC8DIAEYGMDKAkQGMDGBkgP8BnbPthE2i0V0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's look at these pairs:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeda15f",
   "metadata": {},
   "source": [
    "#And let's look at the labels (0 means \"different\", 1 means \"same\"):\n",
    "\n",
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b51d8",
   "metadata": {},
   "source": [
    "array([[1],\n",
    "       [0],\n",
    "       [0],\n",
    "       [1],\n",
    "       [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc1bf4",
   "metadata": {},
   "source": [
    "c. Train the DNN on this training set. For each image pair, you can simultaneously feed\n",
    "the first image to DNN A and the second image to DNN B. The whole network will\n",
    "gradually learn to tell whether two images belong to the same class or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fffa3f",
   "metadata": {},
   "source": [
    "#Let's generate a test set composed of many pairs of images pulled from the MNIST test set:\n",
    "\n",
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84552eb0",
   "metadata": {},
   "source": [
    "And now, let's train the model. There's really nothing special about this step, except for the fact that we need a fairly large batch_size, otherwise the model fails to learn anything and ends up with an accuracy of 50%:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3f584",
   "metadata": {},
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cdd0d",
   "metadata": {},
   "source": [
    "0 Train loss: 0.69103277\n",
    "0 Test accuracy: 0.542\n",
    "1 Train loss: 0.6035354\n",
    "2 Train loss: 0.54946035\n",
    "3 Train loss: 0.47047246\n",
    "4 Train loss: 0.4060757\n",
    "5 Train loss: 0.38308156\n",
    "5 Test accuracy: 0.824\n",
    "6 Train loss: 0.39047274\n",
    "7 Train loss: 0.3390794\n",
    "8 Train loss: 0.3210671\n",
    "9 Train loss: 0.31792685\n",
    "10 Train loss: 0.24494292\n",
    "10 Test accuracy: 0.8881\n",
    "11 Train loss: 0.2929235\n",
    "12 Train loss: 0.23225449\n",
    "13 Train loss: 0.23180929\n",
    "14 Train loss: 0.19877923\n",
    "15 Train loss: 0.20065464\n",
    "15 Test accuracy: 0.9203\n",
    "16 Train loss: 0.19700499\n",
    "17 Train loss: 0.18893136\n",
    "18 Train loss: 0.19965452\n",
    "19 Train loss: 0.24071647\n",
    "20 Train loss: 0.18882024\n",
    "20 Test accuracy: 0.9367\n",
    "21 Train loss: 0.12419197\n",
    "22 Train loss: 0.14013417\n",
    "23 Train loss: 0.120789476\n",
    "24 Train loss: 0.15721135\n",
    "25 Train loss: 0.11507861\n",
    "25 Test accuracy: 0.948\n",
    "26 Train loss: 0.13891116\n",
    "27 Train loss: 0.1526081\n",
    "28 Train loss: 0.123436704\n",
    "<<50 more lines>>\n",
    "70 Test accuracy: 0.9743\n",
    "71 Train loss: 0.019732744\n",
    "72 Train loss: 0.039464083\n",
    "73 Train loss: 0.04187814\n",
    "74 Train loss: 0.05303406\n",
    "75 Train loss: 0.052625064\n",
    "75 Test accuracy: 0.9756\n",
    "76 Train loss: 0.038283084\n",
    "77 Train loss: 0.026332883\n",
    "78 Train loss: 0.07060841\n",
    "79 Train loss: 0.03239444\n",
    "80 Train loss: 0.03136283\n",
    "80 Test accuracy: 0.9731\n",
    "81 Train loss: 0.04390848\n",
    "82 Train loss: 0.015268046\n",
    "83 Train loss: 0.04875638\n",
    "84 Train loss: 0.029360933\n",
    "85 Train loss: 0.0418443\n",
    "85 Test accuracy: 0.9759\n",
    "86 Train loss: 0.018274888\n",
    "87 Train loss: 0.038872603\n",
    "88 Train loss: 0.02969683\n",
    "89 Train loss: 0.020990817\n",
    "90 Train loss: 0.045234833\n",
    "90 Test accuracy: 0.9769\n",
    "91 Train loss: 0.039237432\n",
    "92 Train loss: 0.031329047\n",
    "93 Train loss: 0.033414133\n",
    "94 Train loss: 0.025883088\n",
    "95 Train loss: 0.019567214\n",
    "95 Test accuracy: 0.9765\n",
    "96 Train loss: 0.020650322\n",
    "97 Train loss: 0.0339851\n",
    "98 Train loss: 0.047079965\n",
    "99 Train loss: 0.03125228"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff2c85",
   "metadata": {},
   "source": [
    "We have reached 97.6% accuracy on this digit comparison task. That's not bad, this model knows a thing or two about comparing handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7b07d",
   "metadata": {},
   "source": [
    "Let's see if some of that knowledge can be useful for the regular MNIST classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d0466",
   "metadata": {},
   "source": [
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and\n",
    "adding a softmax output layer on top with 10 neurons. Train this network on split #2\n",
    "and see if you can achieve high performance despite having only 500 images per\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff2385",
   "metadata": {},
   "source": [
    "Let's create the model, it is pretty straightforward. There are many ways to freeze the lower layers, as explained in the book. In this example, we chose to use the tf.stop_gradient() function. Note that we need one Saver to restore the pretrained DNN A, and another Saver to save the final model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed7e9b",
   "metadata": {},
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.layers.dense(frozen_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92291ee4",
   "metadata": {},
   "source": [
    "Now on to training! We first initialize all variables (including the variables in the new output layer), then we restore the pretrained DNN A. Next, we just train the model on the small MNIST dataset (containing just 5,000 images):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaff2b1",
   "metadata": {},
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbed1e",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt\n",
    "0 Test accuracy: 0.9455\n",
    "10 Test accuracy: 0.9634\n",
    "20 Test accuracy: 0.9659\n",
    "30 Test accuracy: 0.9656\n",
    "40 Test accuracy: 0.9655\n",
    "50 Test accuracy: 0.9656\n",
    "60 Test accuracy: 0.9655\n",
    "70 Test accuracy: 0.9656\n",
    "80 Test accuracy: 0.9654\n",
    "90 Test accuracy: 0.9654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78928f",
   "metadata": {},
   "source": [
    "Well, 96.5% accuracy, that's not the best MNIST model we have trained so far, but recall that we are only using a small training set (just 500 images per digit). Let's compare this result with the same DNN trained from scratch, without using transfer learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58eee7",
   "metadata": {},
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04318da9",
   "metadata": {},
   "source": [
    "n_epochs = 150\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf927cd",
   "metadata": {},
   "source": [
    "0 Test accuracy: 0.8694\n",
    "10 Test accuracy: 0.9276\n",
    "20 Test accuracy: 0.9299\n",
    "30 Test accuracy: 0.935\n",
    "40 Test accuracy: 0.942\n",
    "50 Test accuracy: 0.9435\n",
    "60 Test accuracy: 0.9442\n",
    "70 Test accuracy: 0.9447\n",
    "80 Test accuracy: 0.9448\n",
    "90 Test accuracy: 0.945\n",
    "100 Test accuracy: 0.945\n",
    "110 Test accuracy: 0.9458\n",
    "120 Test accuracy: 0.9456\n",
    "130 Test accuracy: 0.9458\n",
    "140 Test accuracy: 0.9458"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5f69a",
   "metadata": {},
   "source": [
    "Only 94.6% accuracy... So transfer learning helped us reduce the error rate from 5.4% to 3.5% (that's over 35% error reduction). Moreover, the model using transfer learning reached over 96% accuracy in less than 10 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
