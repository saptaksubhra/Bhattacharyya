{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab842321",
   "metadata": {},
   "source": [
    "1.What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression.\n",
    "\n",
    "Examples: In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91959d3a",
   "metadata": {},
   "source": [
    "2.What are the various circumstances in which feature construction is required?\n",
    "\n",
    "The process of generating new variables (features) based on already existing variables is known as feature construction.\n",
    "The various circumstances in which eature construction is required are Binning, Encoding and other derived variables.\n",
    "\n",
    "Binning : Binning is the opposite of Encoding where new categorical features are constructed from numerical features.This is sometimes done when certain numerical features cannot be directly used in the learning algorithm and are required to be first converted into dummy variables. Thus we first construct a feature from binning and then perform encoding on this newly constructed feature. Binning is of two types : Supervised Binning and Unsupervised Binning.\n",
    "\n",
    "Encoding : Creating features by converting the categorical features into numerical features is known as encoding. Thus in a way, it is the opposite of Binning. There are two main types of encoding- Binary and Target Based. \n",
    "\n",
    "Other Derived Variables : There are multiple random ways of creating new features from pre-existing features. Among such methods is Feature Crossed where new categorical features are created by using two pre-existing categorical features. New features are also constructed by changing the unit of measurement of a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bbe675",
   "metadata": {},
   "source": [
    "3.Describe how nominal variables are encoded.\n",
    "\n",
    "Nominal variables are encoded when we have a feature where variables are just names and there is no order or rank to this variable's feature. For example: City of person lives in, Gender of person, Marital Status. In the above example, We do not have any order or rank, or sequence. All the variables in the respective feature are equal. We can't give them any orders or ranks. Those features are called Nominal features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15667eb1",
   "metadata": {},
   "source": [
    "4.Describe how numeric features are converted to categorical features.\n",
    "\n",
    "Numeric features are converted into categorical features by the method called 'one hot encoding'.\n",
    "\n",
    "Let's see through one example. We have three distinct values of of hair colour in our overall dataset.The colurs are black, grey and brown. What we do is, create three more features/columns with the headers black, grey, and brown respectively and assign the value 0 or 1 (indicating False or True). Suppose, say a person has black hair then we place 1 under the black hair column and the rest will be assigned 0. Similarly, if a person has grey hair then we assign 1 to the grey hair column, and the rest we assign it to 0. Similarly, if a person has brown hair then we assign 1 to the brown hair column, and the rest we assign it to 0.One hot encoding, basically, creates a binary vector of the size of the number of distinct elements (in our case 3). The biggest advantage of this method is that we can get rid of the artificial ordering.\n",
    "\n",
    "The table has been given below to demonstrate one hot encoding.\n",
    "\n",
    "Hair Colour                  Black     Grey    Brown\n",
    "\n",
    "Black                          1        0       0\n",
    "\n",
    "Grey                           0        1       0\n",
    "\n",
    "Brown                          0        0       1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b291657",
   "metadata": {},
   "source": [
    "5.Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?\n",
    "\n",
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion.\n",
    "\n",
    "The advantages and disvantages of the feature selection wrapper approach:\n",
    "\n",
    "Advantages :Model search\n",
    "Sequential Selection Algorithms: Simples interacts with the classifier, small overfitting risk, prone to local optima, less comuptationally, consider the dependence the among features.\n",
    "\n",
    "Disadvantages: Risk of overfitting, more prone than randomized, algorithms for getting stuck in a local optimum, classifier dependent methods, the solution is not optimal.\n",
    "\n",
    "\n",
    "Wrapper : Evolutionary selection algorithms\n",
    "Advantages: less prone to local optima, interacts with the classifier, model feature dependencies, higher performance accuracy than filter.\n",
    "\n",
    "disadvantages : discriminative power, computationally intensive, lower shorter training times, classifier dependent selection, higher risk of over-fitting than deterministic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3aee3",
   "metadata": {},
   "source": [
    "6.When is a feature considered irrelevant? What can be said to quantify it?\n",
    "\n",
    "Strong relevance implies that the feature is essential to the sense that it cannot be removed without loss of prediction accuracy, are irrelevant. Irrelevant features can never contribute to prediction accuracy. So feature is considered irrevalent when it will never be able to contribute to predict accuracy.\n",
    "\n",
    "Irrelevant features can negatively impact model performance.\n",
    "Having irrelevant features in our data can decrease the accuracy of the models and make our model learn based on irrelevant features. These can be said to quantify irrelevant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2da9d",
   "metadata": {},
   "source": [
    "7.When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
    "\n",
    "A function is considered redundant when data redundancy occurs and it occurs when the same piece of data exists in multiple places, whereas data inconsistency is when the same data exists in different formats in multiple tables. Unfortunately, data redundancy can cause data inconsistency, which can provide a company with unreliable and/or meaningless information.\n",
    "\n",
    "The correlation measure is the criteria used to identify features that could be redundant. For instance, if two features {A1, A2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given  pair of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b8014",
   "metadata": {},
   "source": [
    "8.What are the various distance measurements used to determine feature similarity?\n",
    "\n",
    "distance measures or measurements play an important role in machine learning. Perhaps four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "Hamming Distance : calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n",
    "Euclidean Distance : calculates the distance between two real-valued vectors.\n",
    "Manhattan Distance : also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n",
    "Minkowski Distance : calculates the distance between two real-valued vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cce424",
   "metadata": {},
   "source": [
    "9.State difference between Euclidean and Manhattan distances?\n",
    "\n",
    "Euclidean Distance: Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (y).\n",
    "\n",
    "The Euclidean distance or Euclidean metric is the \"ordinary\" (i.e.straight-line) distance between two points in Euclidean space.\n",
    "The Euclidean distance between points p and q is the length of the line segment connecting them.  \n",
    "Manhattan Distance: This is the distance between real vectors using the sum of their absolute difference.\n",
    "\n",
    "It is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b3104",
   "metadata": {},
   "source": [
    "10.Distinguish between feature transformation and feature selection.\n",
    "\n",
    "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space. That is why Feature Extraction is also called Feature Transformation.\n",
    "\n",
    "Feature selection techniques can be used if the requirement is to maintain the original features, unlike the feature extraction techniques which derive useful information from data to construct a new feature subspace. Feature selection techniques are used when model explainability is a key requirement.\n",
    "\n",
    "Feature extraction techniques can be used to improve the predictive performance of the models, especially, in the case of algorithms that don’t support regularization.\n",
    "\n",
    "Unlike feature selection, feature extraction usually needs to transform the original data to features with strong pattern recognition ability, where the original data can be regarded as features with weak recognition ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a3a40",
   "metadata": {},
   "source": [
    "11.Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2.Collection of features using a hybrid approach\n",
    "\n",
    "3.The width of the silhouette: Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a brief or compact graphical representation of how well each object has been classified. The Average Silhouette Width (ASW) is a popular cluster validation index to estimate the number of clusters. Two algorithms (the standard version OSil and a fast version FOSil) are proposed, and they are compared with existing clustering methods in an extensive simulation study covering known and unknown numbers of clusters.\n",
    "\n",
    "4.Receiver operating characteristic curve : A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection.The false-positive rate is also known as probability of false alarm. The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
