{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc26d459",
   "metadata": {},
   "source": [
    "1.What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to select and transform the most relevant variables from raw data when creating a predictive model using machine learning or statistical modeling.\n",
    "\n",
    "Feature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy.That is how it works.\n",
    "\n",
    "The various aspects of feature engineering lies within its various processes.\n",
    "Those are:\n",
    "Feature Creation: Creating features involves creating new variables which will be most helpful for our model.\n",
    "Transformations: Feature transformation is simply a function that transforms features from one representation to another.\n",
    "Feature Extraction: Feature extraction is the process of extracting features from a data set to identify useful information.\n",
    "Exploratory Data Analysis : Exploratory data analysis (EDA) is a powerful and simple tool that can be used to improve our understanding of our data, by exploring its properties. \n",
    "Benchmark : A Benchmark Model is the most user-friendly, dependable, transparent, and interpretable model against which we can measure our own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e969ce",
   "metadata": {},
   "source": [
    "2.What is feature selection, and how does it work? What is the aim of it? What are the various methods of  functions election?\n",
    "\n",
    "Feature selection is the process of isolating the most consistent, non-redundant, and relevant features to use in model construction.\n",
    "The feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. That is how feature selection works.\n",
    "The main aim of feature selection is to improve the performance of a predictive model and reduce the computational cost of modeling.\n",
    "\n",
    "‍The various methods are filter methods, wrapper methods, embedded methods.Feature selection algorithms are categorized as either supervised, which can be used for labeled data; or unsupervised, which can be used for unlabeled data. Unsupervised techniques are classified as filter methods, wrapper methods, embedded methods, or hybrid methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b7e3b",
   "metadata": {},
   "source": [
    "3.Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "\n",
    "In wrapper methods or approaches, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion.\n",
    "\n",
    "Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\n",
    "\n",
    "Feature Selection filter method:\n",
    "Pros : Lower risk of over-fitting, fastest running time, computationally cheaper in comparison with wrapper and embedded method, easily scalable to high-dimensional datasets, ability of good generalization.\n",
    "Cons: No interaction with classification model for feature selection, Mostly ignoring feature dependencies and considering each feature separately in case of univariate techniques which may eventually lead to low computational performance in comaprison with other techniques of feature selection.\n",
    "\n",
    "Wrapper approach or method: \n",
    "Pros: More comprehensive of feature-set space, considers feature dependencies, better generalization than filter approach, interacts with the classifier for feature selection.\n",
    "Cons: Longer running time, high computational cost, there is no gurantee of optimality of the solution if predicted with another classifier, more computationally unfeasible with increased number of features, highre risk of over-fitting in compariosn with embedded and filter method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e5097",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process. : Feature selection is the process of isolating the most consistent, non-redundant, and relevant features to use in model construction. Methodically reducing the size of datasets is important as the size and variety of datasets continue to grow.There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree).\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "\n",
    "Feature Extraction uses an object-based approach to classify imagery, where an object (also called segment) is a group of pixels with similar spectral, spatial, and/or texture attributes. Traditional classification methods are pixel-based, meaning that spectral information in each pixel is used to classify imagery. With high-resolution panchromatic or multispectral imagery, an object-based method offers more flexibility in the types of features to extract.\n",
    "The workflow involves the following steps: \n",
    "Dividing an image into segments\n",
    "Computing various attributes for the segments\n",
    "Creating several new classes\n",
    "Interactively assigning segments (called training samples) to each class\n",
    "Classifying the entire image with a K Nearest Neighbor (KNN), Support Vector Machine (SVM), or Principal Components Analysis (PCA) supervised classification method, based on your training samples.\n",
    "Exporting the classes to a shapefile or classification image.\n",
    "\n",
    "The most widely used function extraction algorithms are PCA( (Principal Component Analysis) and  LDA(Linear Discriminant Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b589f7d",
   "metadata": {},
   "source": [
    "5.Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "Feature engineering is one of the most important steps in machine learning. It is the process of using domain knowledge of the data to create features that make machine learning algorithms work. We can think of machine learning algorithm as a learning child, the more accurate information we provide the more they will be able to interpret the information well. Focusing first on our data will give us better results than focusing only on models. Feature engineering helps us to create better data which helps the model understand it well and provide reasonable results.\n",
    "\n",
    "NLP is a subfield of artificial intelligence where we understand human interaction with machines using natural languages. To understand a natural language, you need to understand how we write a sentence, how we express our thoughts using different words, signs, special characters, etc basically we should understand the context of the sentence to interpret its meaning.\n",
    "\n",
    "If we can use these contexts as features and feed them to our model then the model will be able to understand the sentence better. Some of the common features that we can extract from a sentence are the number of words, number of capital words, number of punctuation, number of unique words, number of stopwords, average sentence length, etc. We can define these features based on our data set we are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb3fff",
   "metadata": {},
   "source": [
    "6.What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance because of the size they could still have a smaller angle between them. Smaller the angle, higher the similarity.\n",
    "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis or text categorization. That is why cosine similarity is a good metric for text categorization.\n",
    "\n",
    "Let's consider two vectors a and b.  \n",
    "The a vector has values, a = (2, 3, 2, 0, 2, 3, 3, 0, 1) and \n",
    "The b vector has values, b = (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
    "\n",
    "the formula for calculating the cosine similarity is : Cos(a,b) = a.b/||a|| * ||b||\n",
    "Now a.b = 2*2 + 3*1 + 2*0 + 0*0 + 2*3 + 3*2 + 3*1 + 0*3 + 1*1 = 4 + 3 + 6 + 6 + 3 + 1 = 23\n",
    "\n",
    "||a|| = √(2)^2 + (3)^2 + (2)^2 + (0)^2 + (2)^2 + (3)^2 + (3)^2 + (0)^2 + (1)^2 = √4 + 9 + 4 + 4 + 9 + 9 + 1 =√ 40 = 6.32\n",
    "||b|| = √(2)^2 + (1)^2 + (0)^2 + (0)^2 + (3)^2 + (2)^2 + (1)^2 + (3)^2 + (1)^2 = √4 + 1 + 9 + 4 + 1 + 9 + 1 =√ 29 = 5.38\n",
    "So, Cos(a,b) = 23/ (6.32 * 5.38) = 23/34.0016 = 0.676 This is the resemblance in cosine or cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd367d",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "Let's consider above two datas as two binary messages.\n",
    "a = 10001011\n",
    "b = 11001111\n",
    "If we compare these two binary messages bitwise mark on one of them where the values of the bit in a given position differ between the messages then the hamming distance is 2. The two messages differ in positions number 2 and number 6.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "The formula to find the Index is:\n",
    "\n",
    "Jaccard Index = (the number in both sets) / (the number in either set) * 100\n",
    "\n",
    "The same formula in notation is:\n",
    "J(X,Y) = |X∩Y| / |X∪Y|\n",
    "In Steps, that’s:\n",
    "\n",
    "1.Count the number of members which are shared between both sets.\n",
    "2.Count the total number of members in both sets (shared and un-shared).\n",
    "3.Divide the number of shared members (1) by the total number of members (2).\n",
    "4.Multiply the number you found in (3) by 100.\n",
    "\n",
    "\n",
    "Two features with values are :\n",
    "X = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "Y = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "\n",
    "Jaccard Index = |(1,1,0,0,1,1)|/ |(1,1,0,0,0,1,0,1,1,1)| = 6 / 10 = 0.6\n",
    "Although it’s customary to leave the answer in decimal form if you’re using set notation, you could multiply by 100 to get a similarity of 60%.\n",
    "\n",
    "Similarity Matching Coefficient or Simple matching coefficient or SMC = Number of matching attributes/ number of attributes\n",
    "\n",
    "= M00 + M11 / M00 + M01 + M10 + M11 \n",
    "Two features with values are :\n",
    "X = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "Y = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "Where 1 means the user liked the product and 0 means that the user didn’t like the product.\n",
    "Where,\n",
    "M11 (Both X and Y liked the product) = 4\n",
    "M00 (Both did not like the product) = 4\n",
    "M10 (X liked the product but Y did not) = 0\n",
    "M01 (X did not like the product but Y did) = 2\n",
    "\n",
    "For features or users X and Y, SMC is 8/10 = 0.8\n",
    "This shows us that the users X and Y have similarity 80% of the time. So if X likes a new product and Y hasn’t seen it, ywe can recommend it to Y as both have good similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521a7d0",
   "metadata": {},
   "source": [
    "8.State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n",
    "\n",
    "High dimensional data refers to a dataset in which the number of features p is larger than the number of observations N, often written as p >> N.\n",
    "For example, a dataset that has p = 16 features and only N = 9 observations would be considered high dimensional data because the number of features is larger than the number of observations.\n",
    "\n",
    "Few real-life examples are :\n",
    "Healthcare Data\n",
    "High dimensional data is common in healthcare datasets where the number of features for a given individual can be massive (i.e. blood pressure, resting heart rate, immune system status, surgery history, height, weight, existing conditions, etc.). In these datasets, it’s common for the number of features to be larger than the number of observations.\n",
    "\n",
    "Financial Data\n",
    "High dimensional data is also common in financial datasets where the number of features for a given stock can be quite large (i.e. PE Ratio, Market Cap, Trading Volume, Dividend Rate, etc.)\n",
    "In these types of dataset, it’s common for the number of features to be much greater than the number of individual stocks.\n",
    "\n",
    "The difficulties in using machine learning techniques on a data set with many dimensions are When the number of features in a dataset exceeds the number of observations, we will never have a deterministic answer.\n",
    "In other words, it becomes impossible to find a model that can describe the relationship between the predictor variables and the response variable because we don’t have enough observations to train the model on.\n",
    "\n",
    "There are two common ways to deal with high dimensional data:\n",
    "\n",
    "i.Choosing to include fewer features: The most obvious way to avoid dealing with high dimensional data is to simply include fewer features in the dataset.\n",
    "ii.Using a regularization method: Another way to handle high dimensional data without dropping features from the dataset is to use a regularization technique such as:\n",
    "Principal Components Analysis\n",
    "Principal Components Regression\n",
    "Ridge Regression\n",
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333446a",
   "metadata": {},
   "source": [
    "9.Make a few quick notes on:\n",
    "\n",
    "1.PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2.Use of vectors : In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis.\n",
    "Feature vectors in machine learning are used to describe the numeric qualifications of an entity in a mathematical way. They are crucial in a variety of machine learning and pattern recognition applications.\n",
    "\n",
    "3.Embedded technique : Embedded methods or techniques combine the qualities' of filter and wrapper methods. It's implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c1913",
   "metadata": {},
   "source": [
    "10.Make a comparison between:\n",
    "\n",
    "1.Sequential backward exclusion vs. sequential forward selection : Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "2.Function selection methods: filter vs. wrapper : The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "3.SMC vs. Jaccard coefficient : The SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index or Jaccard coefficient only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
