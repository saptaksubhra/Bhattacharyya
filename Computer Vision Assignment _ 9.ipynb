{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacfe52f",
   "metadata": {},
   "source": [
    "1.What are the advantages of a CNN for image classification over a completely linked DNN?\n",
    "\n",
    "The main advantage of CNN in comparison with a completely linked DNN is that it automatically detects the important features without any human supervision. For example, given many pictures of cats and dogs it learns distinctive features for each class by itself. CNN is also computationally efficient. It uses special convolution and pooling operations and performs parameter sharing. This enables CNN models to run on any device, making them universally attractive. CNN's layers are only partially connnected and reuses its weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8ffd6",
   "metadata": {},
   "source": [
    "2.Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two,\n",
    "and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the\n",
    "top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does\n",
    "the CNN have in total? How much RAM would this network need when making a single instance\n",
    "prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?\n",
    "\n",
    "The total number criteria the CNN have  is:\n",
    "\n",
    "first convolutional layer kernel-size and RGB channels, plus bias: 3 * 3 * 3 + 1 = 28 output feature maps is 100: 28 * 100 = 2800\n",
    "second convolutional layer kernel-size and last feature maps, plus bias: 3 * 3 * 100 + 1 = 901 output feature maps is 200: 901 * 200 = 180200\n",
    "third convolutional layers kernel-size and last feautre maps, plus bias: 3 * 3 * 200 + 1 =1801 output feautre maps is 400: 1801 * 400 = 720400\n",
    "\n",
    "Total number of criteria the CNN have is = 2800 + 180200 + 720400 = 903400\n",
    "\n",
    "The amount RAM required is :\n",
    "Since 32-bit is 4 bytes.\n",
    "\n",
    "first convolutional layer one feature map size: 100 * 150 = 15000 total output: 15000 * 100 = 1,500,000\n",
    "second convolutional layer one feature map size: 50 * 75 = 3,750 total output: 3750 * 200 = 750,000\n",
    "third convolutional layer one feature map size: 25 * 38 = 950 total ouput: 950 * 400 = 380, 000\n",
    "(1,500,000 + 750,000 + 380,000) * 4 / 1024 /1024 = 10.032 (MB) 903400 * 4 / 1024 / 1024 = 3.44 (MB) 10.032+ 3.44=13.47(MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868989a2",
   "metadata": {},
   "source": [
    "3.What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?\n",
    "\n",
    "The five things we might do to fix the problem if our GPU runs out of memory while training a CNN are :\n",
    "\n",
    "reduce mini-batch size\n",
    "reduce dimensionality using a larger stride in one or more layers\n",
    "remove one or more layers\n",
    "using 16-bits instead of 32-bit floats\n",
    "distributed the cnn across multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266a365",
   "metadata": {},
   "source": [
    "4.Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n",
    "\n",
    "Max-pooling helps in extracting low-level features like edges, points, etc. While Avg-pooling goes for smooth features. If time constraint is not a problem, then one can skip the pooling layer and use a convolutional layer to do the same. A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed419a4",
   "metadata": {},
   "source": [
    "5.When would a local response normalization layer be useful?\n",
    "\n",
    "Local Response Normalization(LRN) type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood. It is a type of regularizer that encourages competition for big activities among nearby groups of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271946d",
   "metadata": {},
   "source": [
    "6.In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and ResNet's core innovations?\n",
    "\n",
    "The main innovation introduced by AlexNet compared to the LeNet-5 is its sheer size. AlexNet main elements are the same: a sequence of convolutional and pooling layers followed by a couple of fully-connected layers.\n",
    "AlexNet is much larger and deeper and stacks convolutional layer directly on top of each convlutional layer.\n",
    "\n",
    "GoogleNet introduces a inception modules, which make it possible to have much deeper net than previous network.\n",
    "ResNet introduces a skip connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a83f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.On MNIST, build your own CNN and strive to achieve the best possible accuracy.\n",
    "\n",
    "#importing all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bdb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the test data and train data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# checking data format\n",
    "\n",
    "img_rows, img_cols=28, 28\n",
    " \n",
    "if k.image_data_format() == 'channels_first':\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    inpx = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    inpx = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c88dc9",
   "metadata": {},
   "source": [
    "# Description of the output classes\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40daecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpx = Input(shape=inpx)\n",
    "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
    "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
    "layer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\n",
    "layer4 = Dropout(0.5)(layer3)\n",
    "layer5 = Flatten()(layer4)\n",
    "layer6 = Dense(250, activation='sigmoid')(layer5)\n",
    "layer7 = Dense(10, activation='softmax')(layer6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f63c78",
   "metadata": {},
   "source": [
    "model = Model([inpx], layer7)\n",
    "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.fit(x_train, y_train, epochs=12, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9ad01",
   "metadata": {},
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6913864",
   "metadata": {},
   "source": [
    "loss=0.0295960184669\n",
    "accuracy=0.991"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b8a5c",
   "metadata": {},
   "source": [
    "8.Using Inception v3 to classify broad images. a.\n",
    "Images of different animals can be downloaded. Load them in Python using the\n",
    "matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop\n",
    "them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.\n",
    "The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to\n",
    "1.0, so make sure yours do as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7555fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opencv-python) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b37ff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-slim in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf-slim) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from absl-py>=0.2.2->tf-slim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# importing data\n",
    "\n",
    "!pip install tf-slim\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tf_slim as slim\n",
    "from tf_slim.nets import inception\n",
    "import tf_slim as slim\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a08871",
   "metadata": {},
   "source": [
    "ckpt_path = \"/kaggle/input/inception_v3.ckpt\"\n",
    "images_path = \"/kaggle/input/animals/*\"\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 16\n",
    "batch_shape = [batch_size, img_height, img_width, 3]\n",
    "num_classes = 1001\n",
    "predict_output = []\n",
    "class_names_path = \"/kaggle/input/imagenet_class_names.txt\"\n",
    "with open(class_names_path) as f:\n",
    "    class_names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e49e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=batch_shape)\n",
    "\n",
    "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "    logits, end_points = inception.inception_v3(\n",
    "        X, num_classes=num_classes, is_training=False\n",
    "    )\n",
    "\n",
    "predictions = end_points[\"Predictions\"]\n",
    "saver = tf.train.Saver(slim.get_model_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c5d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(input_dir):\n",
    "    global batch_shape\n",
    "    images = np.zeros(batch_shape)\n",
    "    filenames = []\n",
    "    idx = 0\n",
    "    batch_size = batch_shape[0]\n",
    "    files = tf.gfile.Glob(input_dir)[:20]\n",
    "    files.sort()\n",
    "    for filepath in files:\n",
    "        with tf.gfile.Open(filepath, \"rb\") as f:\n",
    "            imgRaw = np.array(Image.fromarray(imread(f, as_gray=False, pilmode=\"RGB\")).resize((299, 299))).astype(np.float) / 255.0\n",
    "        # Images for inception classifier are normalized to be in [-1, 1] interval.\n",
    "        images[idx, :, :, :] = imgRaw * 2.0 - 1.0\n",
    "        filenames.append(os.path.basename(filepath))\n",
    "        idx += 1\n",
    "        if idx == batch_size:\n",
    "            yield filenames, images\n",
    "            filenames = []\n",
    "            images = np.zeros(batch_shape)\n",
    "            idx = 0\n",
    "    if idx > 0:\n",
    "        yield filenames, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e382561",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_creator = tf.train.ChiefSessionCreator(\n",
    "        scaffold=tf.train.Scaffold(saver=saver),\n",
    "        checkpoint_filename_with_path=ckpt_path,\n",
    "        master='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145f3ba",
   "metadata": {},
   "source": [
    "with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n",
    "    for filenames, images in load_images(images_path):\n",
    "        labels = sess.run(predictions, feed_dict={X: images})\n",
    "        for filename, label, image in zip(filenames, labels, images):\n",
    "            predict_output.append([filename, label, image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae950fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in predict_output:\n",
    "    out_list = list(x[1])\n",
    "    topPredict = sorted(range(len(out_list)), key=lambda i: out_list[i], reverse=True)[:5]\n",
    "    plt.imshow((((x[2]+1)/2)*255).astype(int))\n",
    "    plt.show()\n",
    "    print(\"Filename:\",x[0])\n",
    "    print(\"Displaying the top 5 Predictions for above image:\")\n",
    "    for p in topPredict:\n",
    "        print(class_names[p-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968b795",
   "metadata": {},
   "source": [
    "9.Large-scale image recognition using transfer learning.\n",
    "a. Make a training set of at least 100 images for each class. You might, for example, identify your\n",
    "own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as\n",
    "the flowers dataset or MITs places dataset (requires registration, and it is huge).\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also\n",
    "adding some randomness for data augmentation.\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the\n",
    "last layer before output layer) and replace output layer with appropriate number of outputs for\n",
    "your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the\n",
    "output layer must have five neurons and use softmax activation function).\n",
    "d. Separate the data into two sets: a training and a test set. The training set is used to train the\n",
    "model, and the test set is used to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce9660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d930f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import zipfile \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2151844",
   "metadata": {},
   "source": [
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23580046",
   "metadata": {},
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "import matplotlib.image as mpimg\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "pic_index = 100\n",
    "train_cat_fnames = os.listdir( train_cats_dir )\n",
    "train_dog_fnames = os.listdir( train_dogs_dir )\n",
    "\n",
    "\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "    \n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "896b1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78dbe8",
   "metadata": {},
   "source": [
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dir,  batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763ed8b",
   "metadata": {},
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
    "include_top = False, # Leave out the last fully connected layer\n",
    "weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225473b",
   "metadata": {},
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    # Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32b284",
   "metadata": {},
   "source": [
    "vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
