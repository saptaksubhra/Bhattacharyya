{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5226baf",
   "metadata": {},
   "source": [
    "1.Why would you want to use the Data API?\n",
    "\n",
    "An API (Application Programming Interface) is a set of functions that allows applications to access data and interact with external software components, operating systems, or microservices. To simplify, an API delivers a user response to a system and sends the system's response back to a user.\n",
    "There are lot of examples in everday's life where we need to use APIs. Such as : going to a bank to take out or deposit money, seraching for hotels in the website, seraching for a facebook profile, finding an authentic restaurant from the website.\n",
    "It can also be used to make a defined action such as updating or deleting data. There are four basic request methods that can be made with API:\n",
    "\n",
    "GET – Gathers information (Pulling all Coupon Codes)\n",
    "PUT –  Updates pieces of data (Updating Product pricing)\n",
    "POST – Creates (Creating a new Product Category)\n",
    "DELETE – (Deleting a blog post)\n",
    "There are other reasons or benefits of why API is so important to ecommerce sites today.\n",
    "i. Security : Security is enhanced when sites use APIs. \n",
    "ii. Speed : Without APIs, you would have to call a store and ask them to look at their inventory from all their suppliers, which they would eventually get back to you. This, instead of having an API where you could easily see what a product was, the price, or it’s stock level.\n",
    "iii.Scalability: APIs allow scalability and flexibility when expanding your store’s catalog, security, or data needs. Your store can grow at a faster rate when you don’t have to factor in new code for every single product or user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa691781",
   "metadata": {},
   "source": [
    "2.What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "The benefits of splitting a large dataset into multiple files are :\n",
    "\n",
    "Multiple Users can Access Data Simultaneously. When the data in a database is split, in frontend and backend, it can be easily supplied to multiple users sharing a network. \n",
    "Provides Better Protection.\n",
    "Allows for Future Planning. \n",
    "Easy to Modify User Interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251da6e",
   "metadata": {},
   "source": [
    "3.During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "During training, we can you tell that our input pipeline is the bottleneck when a CPU bottleneck occurs. A  CPU bottleneck occurs when the GPU resource is under utilized as a result of one, or more of the CPUs, having reached maximum utilization. In this situation, the GPU will be partially idle while it waits for the CPU to pass in training data. This is an undesired state.\n",
    "\n",
    "There are a number of different tools and techniques for evaluating the runtime performance of a training session, and identifying and studying an input pipeline bottleneck.\n",
    "\n",
    "i.System Metrics: The first thing to check is the system resource utilization. There are a number of different ways to do this.\n",
    "ii. Performance Profilers: To dive into the next level of detail of how the training is performing, we can use a performance profiler.\n",
    "TensorFlow Profiler: The built in TensorFlow profiler includes a wealth of performance analytics, and in particular tools for analyzing the performance of the input pipeline. \n",
    "Amazon SageMaker Debugger: If we are training in the Amazon SageMaker environment, we can take advantage of the profiling features that are built into Amazon SageMaker Debugger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bb9dd",
   "metadata": {},
   "source": [
    "4.Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    " A TFRecord file is composed of a sequence of arbitrary binary records: you can store absolutely any binary data you want in each record. However, in practice most TFRecord files contain sequences of serialized protocol buffers. This makes it possible to benefit from the advantages of protocol buffers, such as the fact that they can be read easily across multiple platforms and languages and their definition can be updated later in a backward-compatible way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda8cc4",
   "metadata": {},
   "source": [
    "5.Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "Protocol buffers or protobuf are ideal for any situation in which we need to serialize structured, record-like, typed data in a language-neutral, platform-neutral, extensible manner. They are most often used for defining communications protocols (together with gRPC) and for data storage.\n",
    "\n",
    "Some of the advantages of using protocol buffers include:\n",
    "\n",
    "Compact data storage\n",
    "Fast parsing\n",
    "Availability in many programming languages\n",
    "Optimized functionality through automatically-generated classes\n",
    "\n",
    "We can use our protbuf definition when we need fast serialisation/deserialisation. Type safety is important. Schema adherence is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc123398",
   "metadata": {},
   "source": [
    "6.When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "\n",
    "When using TFRecords, we will generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thus reduce download time. But if the files are located on the same machine as the training script, it’s usually preferable to leave compression off, to avoid wasting CPU for decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d7a65",
   "metadata": {},
   "source": [
    "7.Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "\n",
    "If you preprocess the data when creating the data files, the training script will run faster, since it will not have to perform preprocessing on the fly. In some cases, the preprocessed data will also be much smaller than the original data, so you can save some space and speed up downloads. It may also be helpful to materialize the preprocessed data, for example to inspect it or archive it. However, this approach has a few cons. First, it’s not easy to experiment with various preprocessing logics if you need to generate a preprocessed dataset for each variant. Second, if you want to perform data augmentation, you have to materialize many variants of your dataset, which will use a large amount of disk space and take a lot of time to generate. Lastly, the trained model will expect preprocessed data, so you will have to add preprocessing code in your application before it calls the model.\n",
    "\n",
    "If the data is preprocessed with the tf.data pipeline, it’s much easier to tweak the preprocessing logic and apply data augmentation. Also, tf.data makes it easy to build highly efficient preprocessing pipelines (e.g., with multithreading and prefetching). However, preprocessing the data this way will slow down training. Moreover, each training instance will be preprocessed once per epoch rather than just once if the data was preprocessed when creating the data files. Lastly, the trained model will still expect preprocessed data.\n",
    " \n",
    "If you add preprocessing layers to your model, you will only have to write the preprocessing code once for both training and inference. If your model needs to be deployed to many different platforms, you will not need to write the preprocessing code multiple times. Plus, you will not run the risk of using the wrong preprocessing logic for your model, since it will be part of the model. On the downside, preprocessing the data will slow down training, and each training instance will be preprocessed once per epoch. Moreover, by default the preprocessing operations will run on the GPU for the current batch (you will not benefit from parallel preprocessing on the CPU, and prefetching). Fortunately, the upcoming Keras preprocessing layers should be able to lift the preprocessing operations from the preprocessing layers and run them as part of the tf.data pipeline, so you will benefit from multithreaded execution on the CPU and prefetching.\n",
    "\n",
    "Lastly, using TF Transform for preprocessing gives you many of the benefits from the previous options: the preprocessed data is materialized, each instance is preprocessed just once (speeding up training), and preprocessing layers get generated automatically so you only need to write the preprocessing code once. The main drawback is the fact that you need to learn how to use this tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
