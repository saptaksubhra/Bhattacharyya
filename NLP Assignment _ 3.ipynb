{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0612d7fe",
   "metadata": {},
   "source": [
    "1.Explain the basic architecture of RNN cell.\n",
    "\n",
    "Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer. The main and most important feature of RNN is Hidden state, which remembers some information about a sequence.\n",
    "RNN have a “memory” which remembers all information about what has been calculated. It uses the same parameters for each input as it performs the same task on all the inputs or hidden layers to produce the output. This reduces the complexity of parameters, unlike other neural networks.\n",
    "The working of a RNN can be understood with the help of below example:\n",
    "\n",
    "Example:\n",
    "Suppose there is a deeper network with one input layer, three hidden layers and one output layer. Then like other neural networks, each hidden layer will have its own set of weights and biases, let’s say, for hidden layer 1 the weights and biases are (w1, b1), (w2, b2) for second hidden layer and (w3, b3) for third hidden layer. This means that each of these layers are independent of each other, i.e. they do not memorize the previous outputs.\n",
    "\n",
    "Now the RNN will do the following:\n",
    "\n",
    "RNN converts the independent activations into dependent activations by providing the same weights and biases to all the layers, thus reducing the complexity of increasing parameters and memorizing each previous outputs by giving each output as input to the next hidden layer.\n",
    "Hence these three layers can be joined together such that the weights and bias of all the hidden layers is the same, into a single recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a8e2e",
   "metadata": {},
   "source": [
    "2.Explain Backpropagation through time (BPTT)\n",
    "\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9950459",
   "metadata": {},
   "source": [
    "3.Explain Vanishing and exploding gradients\n",
    "\n",
    "Vanishing –\n",
    "As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n",
    "\n",
    "Exploding –\n",
    "On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3b86f",
   "metadata": {},
   "source": [
    "4.Explain Long short-term memory (LSTM)\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84583491",
   "metadata": {},
   "source": [
    "5.Explain Gated recurrent unit (GRU)\n",
    "\n",
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ab747",
   "metadata": {},
   "source": [
    "6.Explain Peephole LSTM\n",
    "\n",
    "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. In this peephole connection we can see that all the gates are having an input along with the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1b799",
   "metadata": {},
   "source": [
    "7.Bidirectional RNNs\n",
    "\n",
    "Bidirectional RNN ( BRNN ) duplicates the RNN processing chain so that inputs are processed in both forward and reverse time order. This allows a BRNN to look at future context as well. Two common variants of RNN include GRU and LSTM . LSTM does better than RNN in capturing long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741758c",
   "metadata": {},
   "source": [
    "8.Explain the gates of LSTM with equations.\n",
    "\n",
    "LSTM is made up of Gates:\n",
    "In LSTM we will have 3 gates:\n",
    "i) Input Gate.\n",
    "ii) Forget Gate.\n",
    "iii) Output Gate.\n",
    "Gates in LSTM are the sigmoid activation functions i.e they output a value between 0 or 1 and in most of the cases it is either 0 or 1.\n",
    "we use sigmoid function for gates because, we want a gate to give only positive values and should be able to give us a clear cut answer whether, we need to keep a particular feature or we need to discard that feature.\n",
    "“0” means the gates are blocking everything.\n",
    "“1” means gates are allowing everything to pass through it.\n",
    "The equations for the gates in LSTM are:\n",
    "\n",
    "it = σ(wi[ h t-1 , xt ] + bi)\n",
    "ft = σ(wf[ h t-1 , xt ] + bf)\n",
    "ot = σ(wo[ h t-1 , xt ] + bo)\n",
    "\n",
    "where, \n",
    "it = input gate\n",
    "ft = forget gate\n",
    "ot = output gate\n",
    "σ = sigmoid function\n",
    "wx = weight of the repective gate (x) neurons\n",
    "ht-1 = ouput of the previous lstm block ( at timestamp t -1)\n",
    "xt = input at current timestamp\n",
    "bx = biases for the respective gates (x)\n",
    "\n",
    "First equation is for Input Gate which tells us that what new information we’re going to store in the cell state.\n",
    "Second is for the forget gate which tells the information to throw away from the cell state.\n",
    "Third one is for the output gate which is used to provide the activation to the final output of the lstm block at timestamp ‘t’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f745f",
   "metadata": {},
   "source": [
    "9.Explain BiLSTM\n",
    "\n",
    "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other one taking the input in a backward direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843ec01",
   "metadata": {},
   "source": [
    "10.Explain BiGRU\n",
    "\n",
    "The Bidirectional Gated Recurrent Unit (BGRU) can process the data with time series, so that it can have better performance for the data with time series attributes. In the experiment, we used single CNN, LSTM and GRU models and mixed models CNN-LSTM, CNN-GRU and FS-CNN-BGRU (the model used in this manuscript)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
