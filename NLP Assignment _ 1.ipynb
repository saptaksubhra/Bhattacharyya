{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f634d7",
   "metadata": {},
   "source": [
    "1.Explain One-Hot Encoding\n",
    "\n",
    "One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature of each sample that corresponds to its original category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a3a14",
   "metadata": {},
   "source": [
    "2.Explain Bag of Words\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc3d94",
   "metadata": {},
   "source": [
    "3.Explain Bag of N-Grams\n",
    "\n",
    "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ad57d",
   "metadata": {},
   "source": [
    "4.Explain TF-IDF\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81c12f",
   "metadata": {},
   "source": [
    "5.What is OOV problem?\n",
    "\n",
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a6ade",
   "metadata": {},
   "source": [
    "6.What are word embeddings?\n",
    "\n",
    "In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d4bfd",
   "metadata": {},
   "source": [
    "7.Explain Continuous bag of words (CBOW)\n",
    "\n",
    "The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. Let us consider an example for understanding this. Consider the sentence: ‘It is a pleasant day’ and the word ‘pleasant’ goes as input to the neural network. We are trying to predict the word ‘day’ here. We will use the one-hot encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us predict the output based on the word with least error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ceca36",
   "metadata": {},
   "source": [
    "8.Explain SkipGram\n",
    "\n",
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3b54a",
   "metadata": {},
   "source": [
    "9.Explain Glove Embeddings.\n",
    "\n",
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space.\n",
    "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics. Unlike the occurrence matrix, the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the co-occurrence matrix represents a pair of words occurring together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
