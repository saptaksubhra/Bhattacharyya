{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663efe97",
   "metadata": {},
   "source": [
    "1.What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "CNN's layers are only partially connnected and reuses its weights.\n",
    "learned a kernel which can detect a particular features.\n",
    "A CNN's architecture embeds this prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a89fa8",
   "metadata": {},
   "source": [
    "2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of\n",
    "2, and &quot;same&quot; padding. The lowest layer outputs 100 feature maps, the middle one outputs\n",
    "200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much\n",
    "RAM will this network require when making a prediction for a single instance? What about when\n",
    "training on a mini-batch of 50 images?\n",
    "\n",
    "parameters\n",
    "\n",
    "first convolutional layer kernel-size and RGB channels, plus bias: 3 * 3 * 3 + 1 = 28 output feature maps is 100: 28 * 100 = 2800\n",
    "second convolutional layer kernel-size and last feature maps, plus bias: 3 * 3 * 100 + 1 = 901 output feature maps is 200: 901 * 200 = 180200\n",
    "third convolutional layers kernel-size and last feautre maps, plus bias: 3 * 3 * 200 + 1 =1801 output feautre maps is 400: 1801 * 400 = 720400\n",
    "Total parameters is 2800 + 180200 + 720400 = 903400\n",
    "\n",
    "memories since 32-bit is 4 bytes\n",
    "\n",
    "first convolutional layer one feature map size: 100 * 150 = 15000 total output: 15000 * 100 = 1,500,000\n",
    "second convolutional layer one feature map size: 50 * 75 = 3,750 total output: 3750 * 200 = 750,000\n",
    "third convolutional layer one feature map size: 25 * 38 = 950 total ouput: 950 * 400 = 380, 000\n",
    "(1,500,000 + 750,000 + 380,000) * 4 / 1024 /1024 = 10.032 (MB) 903400 * 4 / 1024 / 1024 = 3.44 (MB) 10.032+ 3.44=13.47(MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58591e2",
   "metadata": {},
   "source": [
    "3.If your GPU runs out of memory while training a CNN, what are five things you could try to\n",
    "solve the problem?\n",
    "\n",
    "reduce mini-batch size\n",
    "reduce dimensionality using a larger stride in one or more layers\n",
    "remove one or more layers\n",
    "using 16-bits instead of 32-bit floats\n",
    "distributed the cnn across multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419a95c",
   "metadata": {},
   "source": [
    "4.Why would you want to add a max pooling layer rather than a convolutional layer with the\n",
    "same stride?\n",
    "\n",
    "A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21b95c",
   "metadata": {},
   "source": [
    "5.When would you want to add a local response normalization layer?\n",
    "\n",
    "Normalization layers in the model often helps to speed up and stabilize the learning process. If training with large batches isn't an issue and if the network doesn't have any recurrent connections, Batch Normalization could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ce661",
   "metadata": {},
   "source": [
    "6.Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main\n",
    "innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "AlexNet:\n",
    "\n",
    "it is much larger and deeper\n",
    "stacks convolutional layer directly on top of each convlutional layer\n",
    "\n",
    "GooLeNet:\n",
    "\n",
    "introduce a inception modules, which make it possible to have much deeper net than previous network\n",
    "\n",
    "ResNet:\n",
    "\n",
    "introduce a skip connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3c695",
   "metadata": {},
   "source": [
    "7.What is a fully convolutional network?\n",
    "\n",
    "FCNs, or Fully Convolutional Networks, are a form of architecture that is primarily used for semantic segmentation. Convolution, pooling, and upsampling are the only locally linked layers they use. Since dense layers aren't used, there are fewer parameters (making the networks faster to train).\n",
    "\n",
    "How can you convert a dense layer into a\n",
    "convolutional layer?\n",
    "\n",
    "A fully convolution network can be built by simply replacing the FC layers with there equivalent Conv layers. One way to do so is to pop layers from the model. In the model stack, each popping will remove the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c619d3",
   "metadata": {},
   "source": [
    "8.What is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "One of the most non-trivial tasks in image processing is segmentation. Segmentation is the process defining an image in such a manner that different objects can be extracted from it. In it's simplest form, segmentation exists as a thresholding problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e4f45",
   "metadata": {},
   "source": [
    "9.Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da53fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all importing all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81ed55",
   "metadata": {},
   "source": [
    "Creating the train data and test data\n",
    "Test data: Used for testing the model that how our model has been trained. \n",
    "Train data: Used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf82068",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2276699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data format\n",
    "\n",
    "img_rows, img_cols=28, 28\n",
    " \n",
    "if k.image_data_format() == 'channels_first':\n",
    "   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "   inpx = (1, img_rows, img_cols)\n",
    " \n",
    "else:\n",
    "   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "   inpx = (img_rows, img_cols, 1)\n",
    " \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a4a9f",
   "metadata": {},
   "source": [
    "#description of the output classes\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3ce146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the dataset is ready so let’s move towards the CNN model \n",
    "\n",
    "inpx = Input(shape=inpx)\n",
    "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
    "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
    "layer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\n",
    "layer4 = Dropout(0.5)(layer3)\n",
    "layer5 = Flatten()(layer4)\n",
    "layer6 = Dense(250, activation='sigmoid')(layer5)\n",
    "layer7 = Dense(10, activation='softmax')(layer6)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d2868",
   "metadata": {},
   "source": [
    "#Calling compile and fit function\n",
    "model = Model([inpx], layer7)\n",
    "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.fit(x_train, y_train, epochs=12, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f390c",
   "metadata": {},
   "source": [
    "#Evaluating function\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b170d2",
   "metadata": {},
   "source": [
    "loss = 0.0295960184669\n",
    "accuracy = 0.991"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76a0c0",
   "metadata": {},
   "source": [
    "10.Use transfer learning for large image classification, going through these steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c16a5",
   "metadata": {},
   "source": [
    "a. Create a training set containing at least 100 images per class. For example, you could\n",
    "classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
    "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7e536",
   "metadata": {},
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tp\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec67e0",
   "metadata": {},
   "source": [
    "dataset, info = tfds.load(\"food101\",\n",
    "                                             shuffle_files=False,\n",
    "                                             with_info=True,\n",
    "                                             as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b1db5",
   "metadata": {},
   "source": [
    "Downloading and preparing dataset 4.65 GiB (download: 4.65 GiB, generated: Unknown size, total: 4.65 GiB) to /root/tensorflow_datasets/food101/2.0.0...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataset food101 downloaded and prepared to /root/tensorflow_datasets/food101/2.0.0. Subsequent calls will reuse this data.\n",
    "2022-05-22 01:42:56.525982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:56.648735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:56.649537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:56.653453: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
    "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "2022-05-22 01:42:56.655431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:56.656437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:56.657378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:58.789808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:58.790612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:58.791290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2022-05-22 01:42:58.793145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df592c",
   "metadata": {},
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6e531",
   "metadata": {},
   "source": [
    "['apple_pie',\n",
    " 'baby_back_ribs',\n",
    " 'baklava',\n",
    " 'beef_carpaccio',\n",
    " 'beef_tartare',\n",
    " 'beet_salad',\n",
    " 'beignets',\n",
    " 'bibimbap',\n",
    " 'bread_pudding',\n",
    " 'breakfast_burrito',\n",
    " 'bruschetta',\n",
    " 'caesar_salad',\n",
    " 'cannoli',\n",
    " 'caprese_salad',\n",
    " 'carrot_cake',\n",
    " 'ceviche',\n",
    " 'cheesecake',\n",
    " 'cheese_plate',\n",
    " 'chicken_curry',\n",
    " 'chicken_quesadilla',\n",
    " 'chicken_wings',\n",
    " 'chocolate_cake',\n",
    " 'chocolate_mousse',\n",
    " 'churros',\n",
    " 'clam_chowder',\n",
    " 'club_sandwich',\n",
    " 'crab_cakes',\n",
    " 'creme_brulee',\n",
    " 'croque_madame',\n",
    " 'cup_cakes',\n",
    " 'deviled_eggs',\n",
    " 'donuts',\n",
    " 'dumplings',\n",
    " 'edamame',\n",
    "  'eggs_benedict',\n",
    " 'escargots',\n",
    " 'falafel',\n",
    " 'filet_mignon',\n",
    " 'fish_and_chips',\n",
    " 'foie_gras',\n",
    " 'french_fries',\n",
    " 'french_onion_soup',\n",
    " 'french_toast',\n",
    " 'fried_calamari',\n",
    " 'fried_rice',\n",
    " 'frozen_yogurt',\n",
    " 'garlic_bread',\n",
    " 'gnocchi',\n",
    "  'greek_salad',\n",
    " 'grilled_cheese_sandwich',\n",
    " 'grilled_salmon',\n",
    " 'guacamole',\n",
    " 'gyoza',\n",
    " 'hamburger',\n",
    " 'hot_and_sour_soup',\n",
    " 'hot_dog',\n",
    " 'huevos_rancheros',\n",
    " 'hummus',\n",
    " 'ice_cream',\n",
    " 'lasagna',\n",
    " 'lobster_bisque',\n",
    " 'lobster_roll_sandwich',\n",
    " 'macaroni_and_cheese',\n",
    " 'macarons',\n",
    " 'miso_soup',\n",
    " 'mussels',\n",
    " 'nachos',\n",
    " 'omelette',\n",
    " 'onion_rings',\n",
    " 'oysters',\n",
    " 'pad_thai',\n",
    " 'paella',\n",
    " 'pancakes',\n",
    " 'panna_cotta',\n",
    " 'peking_duck',\n",
    " 'pho',\n",
    " 'pizza',\n",
    " 'pork_chop',\n",
    " 'poutine',\n",
    " 'prime_rib',\n",
    " 'pulled_pork_sandwich',\n",
    " 'ramen',\n",
    " 'ravioli',\n",
    " 'red_velvet_cake',\n",
    " 'risotto',\n",
    " 'samosa',\n",
    " 'sashimi',\n",
    " 'scallops',\n",
    " 'seaweed_salad',\n",
    " 'shrimp_and_grits',\n",
    " 'spaghetti_bolognese',\n",
    " 'spaghetti_carbonara',\n",
    " 'spring_rolls',\n",
    " 'steak',\n",
    " 'strawberry_shortcake',\n",
    " 'sushi',\n",
    " 'tacos',\n",
    " 'takoyaki',\n",
    " 'tiramisu',\n",
    " 'tuna_tartare',\n",
    " 'waffles']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a4f05",
   "metadata": {},
   "source": [
    "n_classes = info.features[\"label\"].num_classes\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0375019",
   "metadata": {},
   "source": [
    "101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a6e63",
   "metadata": {},
   "source": [
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772fcd4",
   "metadata": {},
   "source": [
    "75750"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12261f4e",
   "metadata": {},
   "source": [
    "In food101, train dataset contains 75750 images(750 images per class), while test dataset contains 2520 images(250 images per class) over 101 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5886b4",
   "metadata": {},
   "source": [
    "#b. Split it into a training set, a validation set, and a test set.\n",
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"food101\",\n",
    "    split=[\"train[:5%]\", \"train[5%:15%]\", \"train[15%:40%]\"],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016ae43",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12, 10))\n",
    "index = 0\n",
    "for image, label in train_set_raw.take(9):\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea553e4",
   "metadata": {},
   "source": [
    "c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
    "optionally add data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea186456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the image and standardize the shape to 224*224*3\n",
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bffb9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24e21b",
   "metadata": {},
   "source": [
    "batch_size = 32\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442f854",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed620ba",
   "metadata": {},
   "source": [
    "#d. Fine-tune a pretrained model on this dataset.\n",
    "\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027e257",
   "metadata": {},
   "source": [
    "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "83689472/83683744 [==============================] - 2s 0us/step\n",
    "83697664/83683744 [==============================] - 2s 0us/step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c5f79",
   "metadata": {},
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa400b5",
   "metadata": {},
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f94cd",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "1775/1775 [==============================] - 181s 100ms/step - loss: 1.9680 - accuracy: 0.5095 - val_loss: 1.9272 - val_accuracy: 0.5145\n",
    "Epoch 2/10\n",
    "1775/1775 [==============================] - 154s 87ms/step - loss: 1.6239 - accuracy: 0.5899\n",
    "Epoch 3/10\n",
    "1775/1775 [==============================] - 152s 85ms/step - loss: 1.5669 - accuracy: 0.6034\n",
    "Epoch 4/10\n",
    "1775/1775 [==============================] - 151s 85ms/step - loss: 1.5321 - accuracy: 0.6136\n",
    "Epoch 5/10\n",
    "1775/1775 [==============================] - 151s 85ms/step - loss: 1.5069 - accuracy: 0.6203\n",
    "Epoch 6/10\n",
    "1775/1775 [==============================] - 152s 86ms/step - loss: 1.4944 - accuracy: 0.6243\n",
    "Epoch 7/10\n",
    "1775/1775 [==============================] - 153s 86ms/step - loss: 1.4795 - accuracy: 0.6263\n",
    "Epoch 8/10\n",
    "1775/1775 [==============================] - 153s 86ms/step - loss: 1.4738 - accuracy: 0.6269\n",
    "Epoch 9/10\n",
    "1775/1775 [==============================] - 154s 87ms/step - loss: 1.4633 - accuracy: 0.6297\n",
    "Epoch 10/10\n",
    "1775/1775 [==============================] - 155s 87ms/step - loss: 1.4541 - accuracy: 0.6318"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
