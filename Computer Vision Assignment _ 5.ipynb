{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35d938e",
   "metadata": {},
   "source": [
    "1.How can each of these parameters be fine-tuned? • Number of hidden layers\n",
    "\n",
    "Here are some guidelines to know the number of hidden layers and neurons per each hidden layer in a classification problem:\n",
    "\n",
    "Based on the data, draw an expected decision boundary to separate the classes.\n",
    "Express the decision boundary as a set of lines. Note that the combination of such lines must yield to the decision boundary.\n",
    "The number of selected lines represents the number of hidden neurons in the first hidden layer.\n",
    "To connect the lines created by the previous layer, a new hidden layer is added. Note that a new hidden layer is added each time you need to create connections among the lines in the previous hidden layer.\n",
    "The number of hidden neurons in each new hidden layer equals the number of connections to be made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867dfde",
   "metadata": {},
   "source": [
    "• Network architecture (network depth)\n",
    "\n",
    "Finetuning means taking weights of a trained neural network and use it as initialization for a new model being trained on data from the same domain (often e.g. images). It is used to speed up the training, overcome small dataset size.\n",
    "There are various strategies, such as training the whole initialized network or 'freezing' some of the pre-trained weights (usually whole layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67f4bf",
   "metadata": {},
   "source": [
    "• Each layer's number of neurons (layer width)\n",
    "\n",
    "Deciding the number of hidden neuron layers is only a small part of the problem. We must also determine how many neurons will be in each of these hidden layers. \n",
    "\n",
    "There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:\n",
    "\n",
    "The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b240b79",
   "metadata": {},
   "source": [
    "• Form of activation\n",
    "\n",
    "In simplest of terms, the activation function is a function that takes an input signal and converts it to an output signal. In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. Activation functions are simply mathematical functions that act as a gateway for some gradients (the input values). For an artificial neural network to work and do complicated computations, it requires more than a linear representation. Activation functions provide the network with the said non-linearity. Without activation functions, our neural networks would cease to be anything more than a simple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3b314",
   "metadata": {},
   "source": [
    "• Optimization and learning\n",
    "\n",
    "To fine-tune optimization and leraning, here are the number steps given below:\n",
    "\n",
    "Step 1: Understand what tuning machine learning model is.\n",
    "Step 2: Cover The Basics. \n",
    "Step 3: Find Your Score Metric. \n",
    "Obtain Accurate Forecasting Score. \n",
    "Step 5: Diagnose Best Parameter Value Using Validation Curves. \n",
    "Step 6: Use Grid Search To Optimise Hyperparameter Combination.\n",
    "Continuously Tune The Parameters To Further Improve Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e130287",
   "metadata": {},
   "source": [
    "• Learning rate and decay schedule\n",
    "\n",
    "The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure.This has the effect of quickly learning good weights early and fine tuning them later.\n",
    "Two popular and easy to use learning rate schedules are as follows:\n",
    "\n",
    "Decrease the learning rate gradually based on the epoch.\n",
    "Decrease the learning rate using punctuated large drops at specific epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df4d7",
   "metadata": {},
   "source": [
    "• Mini batch size\n",
    "\n",
    "Mini-batch sizes, commonly called “batch sizes” for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed. Such as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on.\n",
    "\n",
    "Batch size is a slider on the learning process.\n",
    "\n",
    "Small values give a learning process that converges quickly at the cost of noise in the training process.\n",
    "Large values give a learning process that converges slowly with accurate estimates of the error gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ee17a",
   "metadata": {},
   "source": [
    "• Algorithms for optimization\n",
    "\n",
    "A tuning problem consists of an optimization algorithm to be tuned, and an application problem that is to be solved by this optimization algorithm. The goal is to optimize the performance of the optimization algorithm on (unseen instances of) the application problem. A tuning algorithm for addressing the tuning problem usually consists of a sampling algorithm that generates candidate parameter configurations, and a stochasticity handling technique that deals with the stochasticity during the evaluation of the parameter configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c301a",
   "metadata": {},
   "source": [
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "Early stopping requires that we configure our network to be under constrained, meaning that it has more capacity than is required for the problem.\n",
    "\n",
    "When training the network, a larger number of training epochs is used than may normally be required, to give the network plenty of opportunity to fit, then begin to overfit the training dataset.\n",
    "\n",
    "There are three elements to using early stopping; they are:\n",
    "\n",
    "Monitoring model performance.\n",
    "Trigger to stop training.\n",
    "The choice of model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7dcffe",
   "metadata": {},
   "source": [
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase. And Linear regression model will try to optimize the coefficient in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d1124",
   "metadata": {},
   "source": [
    "• L2 normalization\n",
    "\n",
    "Ridge regression refers to a type of linear regression where in order to get better predictions in the long term, we introduce a small amount of bias. It is also known as L2 regularization. Regularization comes into play and shrinks the learned estimates towards zero. In other words, it tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d6f7a",
   "metadata": {},
   "source": [
    "• Drop out layers\n",
    "\n",
    "The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer. A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8. To tune a dropout rate is a good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d384aa5",
   "metadata": {},
   "source": [
    "• Data augmentation\n",
    "\n",
    "The number of training samples generated by data augmentation technique is very high when compared with training samples without the data augmentation when the number of epochs is high. In case of data augmentation, different series of training samples are generated during every epoch  whereas, without the data augmentation, the same set of training samples are used during every epoch.\n",
    "When we don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
