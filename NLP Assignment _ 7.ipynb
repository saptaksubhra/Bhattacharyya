{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f47d206",
   "metadata": {},
   "source": [
    "1.Explain the architecture of BERT\n",
    "\n",
    "BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b47c64",
   "metadata": {},
   "source": [
    "2.Explain Masked Language Modeling (MLM)\n",
    "\n",
    "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af454ca6",
   "metadata": {},
   "source": [
    "3.Explain Next Sentence Prediction (NSP)\n",
    "\n",
    "Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling — MLM). Where MLM teaches BERT to understand relationships between words — NSP teaches BERT to understand longer-term dependencies across sentences.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58110a",
   "metadata": {},
   "source": [
    "4.What is Matthews evaluation?\n",
    "\n",
    "Matthew’s correlation coefficient formula :\n",
    "MCC is a best single-value classification metric which helps to summarize the confusion matrix or an error matrix. A confusion matrix has four entities:\n",
    "\n",
    "True positives (TP)\n",
    "True negatives (TN)\n",
    "False positives (FP)\n",
    "False negatives (FN)\n",
    "And is calculated by the following formula:\n",
    "\n",
    "MCC = TN X TP - FN X FP / {(TP + FP)(TP + FN)(TN + FP)(TN + FN)}^1 /2\n",
    "\n",
    "If the prediction returns good rates for all four of these entities, it is said to be a reliable measure producing high scores. And to suit most correlation coefficients, MCC also ranges between +1 and -1 as:\n",
    "\n",
    "+1 is the best agreement between the predicted and actual values.\n",
    "0 is no agreement. Meaning, prediction is random according to the actuals.\n",
    "Example of MCC:\n",
    "Confusion matrix with entries: TP = 90, FP = 4; TN = 1, FN = 5. When we substitute these values in the formula we get 0.14. \n",
    "\n",
    "0.14 means the classifier is very close to a random guess classifier (0). \n",
    "\n",
    "Hence, it seems that the MCC helps us to identify the ineffectiveness of the classifier in classifying especially the negative class samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa5e61",
   "metadata": {},
   "source": [
    "5.What is Matthews Correlation Coefficient (MCC)?\n",
    "\n",
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39060a46",
   "metadata": {},
   "source": [
    "6.Explain Semantic Role Labeling\n",
    "\n",
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. It serves to find the meaning of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c95a6",
   "metadata": {},
   "source": [
    "7.Why Fine-tuning a BERT model takes less time than pretraining\n",
    "\n",
    "Fine-tuning a BERT model takes less time than pretraining because the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f47723",
   "metadata": {},
   "source": [
    "8.Recognizing Textual Entailment (RTE)\n",
    "\n",
    "Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. Textual entailment recognition is the task of deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61947a54",
   "metadata": {},
   "source": [
    "9.Explain the decoder stack of GPT models.\n",
    "\n",
    "GPT model was based on Transformer architecture. It was made of decoders stacked on top of each other (12 decoders). These models were same as BERT as they were also based on Transformer architecture. The difference in architecture with BERT is that it used stacked encoder layers. GPT model works on a principle called autoregressive which is similar to one used in RNN. It is a technique where the previous output becomes current input.\n",
    "GPT-2 consists of solely stacked decoder blocks from the transformer architecture. In the standard transformer architecture, the decoder is fed a word embedding concatenated with a context vector, both generated by the encoder. In GPT-2 the context vector is zero-initialized for the first word embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
