{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd8b7ee",
   "metadata": {},
   "source": [
    "1.What are Sequence-to-sequence models?\n",
    "\n",
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization.\n",
    "Use Cases of the Sequence to Sequence Models are machine translation , speech recognition.\n",
    "The most common architecture used to build Seq2Seq models is Encoder-Decoder architecture.\n",
    "Both encoder and the decoder are LSTM models (or sometimes GRU models)\n",
    "Encoder reads the input sequence and summarizes the information in something called the internal state vectors or context vector.The decoder is an LSTM whose initial states are initialized to the final states of the Encoder LSTM.\n",
    "Drawbacks of Encoder-Decoder Models :\n",
    "As with human beings, this architecture has very limited memory.\n",
    "As a general rule of thumb, the deeper a neural network is, the harder it is to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3213e",
   "metadata": {},
   "source": [
    "2.What are the Problem with Vanilla RNNs?\n",
    "\n",
    "The problems with Vanilla RNNs are :\n",
    "Training RNNs\n",
    "The vanishing or exploding gradient problem\n",
    "RNNs cannot be stacked up\n",
    "Slow and Complex training procedures\n",
    "Difficult to process longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4e6c7",
   "metadata": {},
   "source": [
    "3.What is Gradient clipping?\n",
    "\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a50791",
   "metadata": {},
   "source": [
    "4.Explain Attention mechanism\n",
    "\n",
    "The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acb651",
   "metadata": {},
   "source": [
    "5.Explain Conditional random fields (CRFs)\n",
    "\n",
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\n",
    "\n",
    "Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa637826",
   "metadata": {},
   "source": [
    "6.Explain self-attention\n",
    "\n",
    "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input.\n",
    "\n",
    "The first step is multiplying each of the encoder input vectors with three weights matrices (W(Q), W(K), W(V)) that we trained during the training process. This matrix multiplication will give us three vectors for each of the input vector: the key vector, the query vector, and the value vector.\n",
    "The second step in calculating self-attention is to multiply the Query vector of the current input with the key vectors from other inputs.\n",
    "In the third step, we will divide the score by square root of dimensions of the key vector (dk). In the paper the dimension of the key vector is 64, so that will be 8. The reason behind that is if the dot products become large, this causes some self-attention scores to be very small after we apply softmax function in the future.\n",
    "In the fourth step, we will apply the softmax function on all self-attention scores we calculated wrt the query word (here first word).\n",
    "In the fifth step, we multiply the value vector on the vector we calculated in the previous step.\n",
    "In the final step, we sum up the weighted value vectors that we got in the previous step, this will give us the self-attention output for the given word.\n",
    "The above procedure is applied to all the input sequences. Mathematically, the self-attention matrix for input matrices (Q, K, V) is calculated as:\n",
    "\n",
    "Attention ( Q, K, V ) = softmax (QK^T/dk^1/2)V\n",
    "where Q, K, V are the concatenation of query, key, and value vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a00df",
   "metadata": {},
   "source": [
    "7.What is Bahdanau Attention?\n",
    "\n",
    "The Bahdanau attention mechanism has inherited its name from the first author of the paper in which it was published. \n",
    "It follows the work of Cho et al. (2014) and Sutskever et al. (2014), who had also employed an RNN encoder-decoder framework for neural machine translation, specifically by encoding a variable-length source sentence into a fixed-length vector. The latter would then be decoded into a variable-length target sentence. \n",
    "Bahdanau et al. (2014) argue that this encoding of a variable-length input into a fixed-length vector squashes the information of the source sentence, irrespective of its length, causing the performance of a basic encoder-decoder model to deteriorate rapidly with an increasing length of the input sentence. The approach they propose, on the other hand, replaces the fixed-length vector with a variable-length one, to improve the translation performance of the basic encoder-decoder model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442769dd",
   "metadata": {},
   "source": [
    "8.What is a Language Model?\n",
    "\n",
    "A language model in NLP is a probabilistic statistical model that determines the probability of a given sequence of words occurring in a sentence based on the previous words. It helps to predict which word is more likely to appear next in the sentence. Hence it is widely used in predictive text input systems, speech recognition, machine translation, spelling correction etc. The input to a language model is usually a training set of example sentences. The output is a probability distribution over sequences of words. We can use the last one word (unigram), last two words (bigram), last three words (trigram) or last n words (n-gram) to predict the next word as per our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3119a3",
   "metadata": {},
   "source": [
    "9.What is Multi-Head Attention?\n",
    "\n",
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).\n",
    "Mathematically, MultiHead(Q, K, V) = [head1 , head2, ...., headh]W0\n",
    "where, headi = Attention(QWi ^Q, KWi ^K, VWi ^V)\n",
    "Above W are all learnable parameter matrices.\n",
    "It should be noted that scaled dot-product attention is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ef24c",
   "metadata": {},
   "source": [
    "10.What is Bilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n",
    "The Python Natural Language Toolkit library, or NLTK, provides an implementation of the BLEU score that you can use to evaluate your generated text against a reference.\n",
    "NLTK provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences.\n",
    "\n",
    "The reference sentences must be provided as a list of sentences where each reference is a list of tokens. The candidate sentence is provided as a list of tokens. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc413b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    reference = [['this', 'is', 'an', 'acid', 'test'], ['this', 'is' 'test']]\n",
    "    candidate = ['this', 'is', 'an', 'acid', 'test']\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print(score)\n",
    "except:\n",
    "    print('calculating BLEU score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe12bb0",
   "metadata": {},
   "source": [
    "Running this example prints a perfect score as the candidate matches one of the references exactly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
