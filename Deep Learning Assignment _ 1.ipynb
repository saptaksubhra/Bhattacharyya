{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9544e3",
   "metadata": {},
   "source": [
    "1.What is the function of a summation junction of a neuron? What is threshold activation function?\n",
    "\n",
    "A summation junction for the input signals is weighted by the respective synaptic weight. The function of it is to combine linear combiner of the weighted input signals.\n",
    "\n",
    "A threshold activation function (or simply the activation function, also known as squashing function) results in an output signal only when an input signal exceeding a specific threshold value comes as an input. It is similar in behaviour to the biological neuron which transmits the signal only when the total input signal meets the firing threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505205f1",
   "metadata": {},
   "source": [
    "2.What is a step function? What is the difference of step function with threshold function?\n",
    "\n",
    "A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron are A1 = 1 and A0 = 0.\n",
    "\n",
    "Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated whereas A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b008f8",
   "metadata": {},
   "source": [
    "3.Explain the McCulloch–Pitts model of neuron.\n",
    "\n",
    "This is simplified model of real neurons, known as Threshold Logic Unit. A set of synapsesc (i.e connections) brings the activations from the other neurons. A processing unit sums the inputs, the applies the non-linear activation funcation (i.e threshold / transfer function). A output line transmit the result to other neurons. The mcculloch pitts neuron model is extremely simplifield model of real biological neurons. some of its missing features includes: non-binary inputs-outputs, non-linear summation, smoothe thresholding , non-deterministic and tempory information processing.\n",
    "\n",
    "It is inspired by Biological Neural network.\n",
    "It allows only Binary Value(0,1).\n",
    "It has threshold function as activation function.\n",
    "It is first mathematical model of Biological Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488602b",
   "metadata": {},
   "source": [
    "4.Explain the ADALINE network model.\n",
    "\n",
    "MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852ff4c",
   "metadata": {},
   "source": [
    "5.What is the constraint of a simple perceptron? Why it may fail with a real-world data set?\n",
    "\n",
    "A simple perceptron have the following constraints: First, the output values of a perceptron can take on only one of two values (0 or 1) because of the hard-limit transfer function. Second, perceptrons can only classify linearly separable sets of vectors. \n",
    "\n",
    "Perceptrons only represent linearly separable problems. Perceptron may fail with real-world dataset as they fail to converge if the training examples are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07340940",
   "metadata": {},
   "source": [
    "6.What is linearly inseparable problem? What is the role of the hidden layer?\n",
    "\n",
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable. For example, XOR is a lineraly inseparable problem.\n",
    "\n",
    "A hidden layer in an artificial neural network is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75993146",
   "metadata": {},
   "source": [
    "7.Explain XOR problem in case of a simple perceptron.\n",
    "\n",
    "A \"single-layer\" perceptron can't apply XOR. The reason is because the classes in XOR are not linearly separable. We cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0). This led to invention of multi-layer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c17ebc",
   "metadata": {},
   "source": [
    "8.Design a multi-layer perceptron to implement A XOR B.\n",
    "\n",
    "The truth table for a two-input XOR-Gate is given below:\n",
    "\n",
    "x1\tx2\ty\n",
    "0\t0\t0\n",
    "0\t1\t1\n",
    "1\t0\t1\n",
    "1\t1\t0\n",
    "\n",
    "Let's desiogn  a multi-layer perceptron to implement A XOR B by using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb26b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 0) = 0\n",
      "XOR(0, 1) = 1\n",
      "XOR(1, 0) = 1\n",
      "XOR(1, 1) = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "  \n",
    "# define Unit Step Function\n",
    "def unitStep(v):\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#design Perceptron Model\n",
    "def perceptronModel(x, w, b):\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unitStep(v)\n",
    "    return y\n",
    "  \n",
    "# NOT Logic Function\n",
    "# wNOT = -1, bNOT = 0.5\n",
    "def NOT_logicFunction(x):\n",
    "    wNOT = -1\n",
    "    bNOT = 0.5\n",
    "    return perceptronModel(x, wNOT, bNOT)\n",
    "  \n",
    "# AND Logic Function\n",
    "# here w1 = wAND1 = 1, \n",
    "# w2 = wAND2 = 1, bAND = -1.5\n",
    "def AND_logicFunction(x):\n",
    "    w = np.array([1, 1])\n",
    "    bAND = -1.5\n",
    "    return perceptronModel(x, w, bAND)\n",
    "  \n",
    "# OR Logic Function\n",
    "# w1 = 1, w2 = 1, bOR = -0.5\n",
    "def OR_logicFunction(x):\n",
    "    w = np.array([1, 1])\n",
    "    bOR = -0.5\n",
    "    return perceptronModel(x, w, bOR)\n",
    "  \n",
    "# XOR Logic Function\n",
    "# with AND, OR and NOT  \n",
    "# function calls in sequence\n",
    "def XOR_logicFunction(x):\n",
    "    y1 = AND_logicFunction(x)\n",
    "    y2 = OR_logicFunction(x)\n",
    "    y3 = NOT_logicFunction(y1)\n",
    "    final_x = np.array([y2, y3])\n",
    "    finalOutput = AND_logicFunction(final_x)\n",
    "    return finalOutput\n",
    "  \n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 0])\n",
    "test2 = np.array([0, 1])\n",
    "test3 = np.array([1, 0])\n",
    "test4 = np.array([1, 1])\n",
    "  \n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test1)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test2)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test3)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31766",
   "metadata": {},
   "source": [
    "9.Explain the single-layer feed forward architecture of ANN.\n",
    "\n",
    "In this type of network, we have only two layers input layer and output layer but the input layer does not count because no computation is performed in this layer. The output layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16644347",
   "metadata": {},
   "source": [
    "10.Explain the competitive network architecture of ANN.\n",
    "\n",
    "An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the brain. ANNs, like people, learn by examples. An ANN is configured for a specific application, such as pattern recognition or data classification, through a learning process. Learning largely involves adjustments to the synaptic connections that exist between the neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e66d1b",
   "metadata": {},
   "source": [
    "11.Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.\n",
    "\n",
    "We can consider a Neural Network: It contains the following:\n",
    "\n",
    "two inputs\n",
    "two hidden neurons\n",
    "two output neurons\n",
    "two biases\n",
    "\n",
    "The steps are mentioned below in Backpropagation.\n",
    "Step — 1: Forward Propagation : We will start by propagating forward. We will repeat this process for the output layer neurons, using the output from the hidden layer neurons as inputs. \n",
    "Step — 2: Backward Propagation : Now, we will propagate backwards. This way we will try to reduce the error by changing the values of weights and biases.\n",
    "Step — 3: Putting all the values together and calculating the updated weight value : Now, let’s put all the values together.\n",
    "Similarly, we can calculate the other weight values as well.\n",
    "After that we will again propagate forward and calculate the output. Again, we will calculate the error.\n",
    "If the error is minimum we will stop right there, else we will again propagate backwards and update the weight values.\n",
    "This process will keep on repeating until error becomes minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf0676",
   "metadata": {},
   "source": [
    "12.What are the advantages and disadvantages of neural networks?\n",
    "\n",
    "Advantages : \n",
    "Having fault tolerance , Having a distributed memory, Gradual Corruption, ability to make machine learning, Storing up information on the entire network, ability to work with the incomplete knowledge, parallel processing capability.\n",
    "\n",
    "Disadvantages: \n",
    " Hardware dependence, Determination of proper network structure,  Unexplained behavior of the network, Difficulty of showing the problem to the network, The unkwon duration of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6f028",
   "metadata": {},
   "source": [
    "13.Write short notes on any two of the following:\n",
    "\n",
    "1.Biological neuron : In the biological systems, a neuron is a cell just like any other cell of the body, which has a DNA code and is generated in the same way as the other cells. Though it might have different DNA, the function is similar in all the organisms. Biological neuron models, also known as a spiking neuron models, are mathematical descriptions of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, called action potentials or spikes.\n",
    "\n",
    "2.ReLU function : ReLU stands for rectified linear activation unit and is considered one of the few milestones in the deep learning revolution. It is simple yet really better than its predecessor activation functions such as sigmoid or tanh. ReLU activation function formula : f(x)=max(0,x)\n",
    "ReLU function is its derivative both are monotonic. The function returns 0 if it receives any negative input, but for any positive value x, it returns that value back. Thus it gives an output that has a range from 0 to infinity. Now, let's see how Relu function works by giving some inputs  to the ReLU activation function and see how it transforms them and then we will plot them also. ReLU is used as a default activation function and nowadays and it is the most commonly used activation function in neural networks, especially in CNNs.\n",
    "\n",
    "\n",
    "3.Single-layer feed forward ANN\n",
    "4.Gradient descent\n",
    "5.Recurrent networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c0a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation of ReLU function\n",
    "\n",
    "try:\n",
    "    def ReLU(x):\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "except:\n",
    "    print('ReLU function operation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc9395a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAe0ElEQVR4nO3de3RU1dkG8GefTLiGhJDhYggUg6EaiyBCoVQFYdRqraLVCCgIyE1Q5I4LW/26qG0UYhArRQQRsBbQclEqtQ4otKIlgiglFQFBkUvC5A6EJJPzfn/EZkEJJJmZk33OzPNby7XIMJn9vIzzcNg5M0eJiICIiBzH0B2AiIgCwwInInIoFjgRkUOxwImIHIoFTkTkUCxwIiKHcjX0gseOHbP08d1uN3w+n6VrWI0z2ANnsIdwmAEIbo7ExMQab+cROBGRQ7HAiYgcigVORORQLHAiIodigRMRORQLnIjIoVjgREQOxQInIrKQlJfBXPUKzKKCkD82C5yIyEKyeglk8zuoOPRVyB+bBU5EZBFzxzbItvegfvZLNO7eO+SPzwInIrKA5B6DrHwJ6Hwl1F0PWLIGC5yIKMSkohzmy88BRhSMMTOgXNZ87BQLnIgoxOTNV4Fvv4YxajJUQmvL1mGBExGFkOzcDvngXaib74Lq9mNL12KBExGFiJw8AXP5i8DlXaDuGW75eixwIqIQEH8FzMVzAQDGmOlQrmjL12SBExGFgPxlOXB4P4wRk6Bat2uQNVngRERBkt3/gnjfhhpwB1SPnzTYurUW+MKFCzF69GhMmzbtgt97++23kZaWhuLiYkvCERHZneTlwlz2AtCxM9S9Ixt07VoLvH///pg9e/YFt/t8PuzZswdut9uSYEREdid+f9W+t1kJY9wMqGjr973PVWuBp6amIiYm5oLbly9fjgceeABKKUuCERHZnaxfCXy9D2r4Y1Btar7wsJUCenvQp59+ilatWqFTp0613tfr9cLr9QIA0tPTLT9id7lcjv9XAWewB85gD3adoWzndhS+tw5NbxmE2NsG1Xp/K+aod4GXlZVh7dq1+NWvflWn+3s8Hng8nuqvfT5ffZesF7fbbfkaVuMM9sAZ7MGOM0i+D+b83wBJnVB21wN1yhfMHImJNR/d1/sslJycHOTm5mLGjBmYOHEi8vLyMGvWLBQWFgYUjIjISaSyEuYr84CKChjjZkI1aqwtS72PwDt27IglS5ZUfz1x4kT8/ve/R2xsbEiDERHZkbz9BnAgG+rhqVDtkrRmqbXA58+fj+zsbJSUlGD8+PFIS0vDgAEDGiIbEZGtyN7PIJvegrr+Zhh9+uuOU3uBT548+ZK//9JLL4UqCxGRbUlhPsylzwOXdYAaPFZ3HAB8JyYRUa3ErIS5JAMoO1u1791Y3773uVjgRES1kI2rgX17oIaOh0rsqDtONRY4EdElyH8+h2xcDfWTm2D8dKDuOOdhgRMRXYQUF1Tte7dtDzV0vO44F2CBExHVQEwT5tJM4Mzpqn3vJk11R7oAC5yIqAay6S0gezfU4DFQSZ10x6kRC5yI6H/IV3shG96A+vGNUDfcojvORbHAiYjOISXFVW+Vb90OatgEW3/iKguciOh7YpowX80EThVVfb53k2a6I10SC5yI6Hvy/nrg3zuh0kZDdeysO06tWOBERADk4JeQtSuA6/pC9b9Nd5w6YYETUcST0yVVl0Zr1RrG8Mdsve99LhY4EUU0Eam6KHFRQdX53s2a645UZyxwIoposvlt4PMdUPeOgOqUojtOvbDAiShiyaH9kLeWA917Qw38he449cYCJ6KIJGdOwXz5WSAuHsaISY7Z9z4XC5yIIo6IwFz+B6AwD8bYGVDNW+iOFBAWOBFFHPnwXWDXdqi7h0N1vlJ3nIDVekm1hQsXYteuXYiLi0NGRgYAYOXKldi5cydcLhfatm2LCRMmoHlz5/zklogil3xzELJmKdC1J9TNd+mOE5Raj8D79++P2bNnn3fbNddcg4yMDMybNw+XXXYZ1q1bZ1lAIqJQkdIzMBc/B8TEwRg5Gcpw9iZErelTU1MRExNz3m3dunVDVFQUAKBLly7Iz8+3Jh0RUYiICGTlS4Avp2rfu0Ws7khBq3ULpTZbtmxB3759L/r7Xq8XXq8XAJCeng632x3skpfkcrksX8NqnMEeOIM9hGqGM39fj5KsfyDmgXFo/pMbQ5Csfqx4LoIq8LVr1yIqKgo33HDDRe/j8Xjg8Xiqv/b5fMEsWSu32235GlbjDPbAGewhFDPId4dgLpkPpF6LMzfehlINfybBzJGYmFjj7QFvAH344YfYuXMnJk1y5vmTRBQZ5GwpzJefA5rFwHh4iuP3vc8V0CS7d+/Ghg0bMGvWLDRu3DjUmYiIQkJEIH9aBOQchzFmGlRsS92RQqrWLZT58+cjOzsbJSUlGD9+PNLS0rBu3Tr4/X7MmTMHAJCSkoKxY8daHpaIqD5k+2bIJx9A/WII1A+76o4TcrUW+OTJky+4bcCAAVZkISIKGTn6LeSNRcCV10DdkaY7jiXCZzOIiOh7Una26nNOGjeFMXoalBGlO5IlWOBEFHbkz4uBE99VlXdcvO44lmGBE1FYMT/5APKRF+q2+6BSu+uOYykWOBGFDTn+HeT1PwIpqVB3DtEdx3IscCIKC1JeVrXvHd0IxpgZUFHhue99LhY4EYUFWb0EOPoNjFFToOITdMdpECxwInI8c8c2yLb3oG69B6rrdbrjNBgWOBE5muQeq/qUwc5XQg16UHecBsUCJyLHkoryqs85MaKq9r1dQX/AqqOwwInIseTNZcC3X8MY+ThUQmvdcRocC5yIHEl2bod88Fcoz11Q3XvrjqMFC5yIHEdOnoC5/EXg8i5QvxyuO442LHAichTxV8BcPBcAYIyZDuWK1pxIHxY4ETmK/GUFcHg/jBGPQbVupzuOVixwInIM2f0viHcD1E0/h+px8WvxRgoWOBE5guSdhLnsBaBjZ6j7RumOYwsscCKyPfH7Yb4yFzArYYybARUdufve56r1rPeFCxdi165diIuLQ0ZGBgDg1KlTyMzMxMmTJ9G6dWtMmTIFMTExloclosgk618HDn4JNXYGVJuar9AeiWo9Au/fvz9mz5593m3r169H165dsWDBAnTt2hXr16+3Kh8RRbiynR9D3lsLdePPYPS6QXccW6m1wFNTUy84us7KykK/fv0AAP369UNWVpY16YgooklBHooWzAGSOkHd/7DuOLYT0AcHFBUVIT6+6jJF8fHxKC4uvuh9vV4vvF4vACA9PR1utzuQJevM5XJZvobVOIM9cAa9pNKPgsxfw19ehoRZv4Mrsb3uSEGx4rmw/JNfPB4PPB5P9dc+n8/S9dxut+VrWI0z2ANn0Mtc9zok+3PEPv4UCpvEAA6d47+CeS4SE2ve9w/oLJS4uDgUFBQAAAoKChAbGxtQKCKimkj2Z5BNb0L91IOm/X+mO45tBVTgPXv2xNatWwEAW7duRa9evUIaiogilxTmw1zyPNAuCWrION1xbK3WLZT58+cjOzsbJSUlGD9+PNLS0jBo0CBkZmZiy5YtcLvdmDp1akNkJaIwJ2YlzCUZQFkpjGnPQDVurDuSrdVa4JMnT67x9qeeeirUWYgowsnGNcC+PVAjJkG176g7ju3xnZhEZAvy5ReQjaug+twE1Xeg7jiOwAInIu2kuKBq66RtItQD46GU0h3JEVjgRKSVmCbMpZnAmdMwxs2EatJUdyTHYIETkVay6S0gezfU4NFQSZfrjuMoLHAi0ka+2gvZ8AZUrxugbrhVdxzHYYETkRZSUgzzlXlA67ZQwyZy3zsALHAianBimjCXzQdOFVXtezdtpjuSI7HAiajByfvrgT2fQqU9DNWxs+44jsUCJ6IGJQe/hKxdAVzXF6r/7brjOBoLnIgajJwugbl4LtCqNYzhj3LfO0gscCJqECJSdVHiogIYY2dCNeNlGIPFAieiBiGb3wY+3wF170NQl6fojhMWWOBEZDk5tB/y1nKge2+ogXfqjhM2WOBEZCk5cwrm4ueAuHgYIyZx3zuEWOBEZBkRgbn8D0CBD8bYGVDNW+iOFFZY4ERkGfnwXWDXdqi7h0F1vlJ3nLDDAiciS8i3ByFrlgJde0LdPEh3nLAU1FXpN27ciC1btkAphQ4dOmDChAlo1KhRqLIRkUNJ6RmYLz8HxMTBGDkZyuCxohUC/lPNz8/Hpk2bkJ6ejoyMDJimie3bt4cyGxE5kIhAVr4E+HJgjJkO1SJWd6SwFdRfi6Zpory8HJWVlSgvL0d8fHyochGRQ8k//g7J+gfUnUOhulytO05YUyIigX7zu+++iz//+c9o1KgRunXrhkmTJl1wH6/XC6/XCwBIT09HeXl54GnrwOVywe/3W7qG1TiDPXCG+qs4fAD5s0aj0VXd0PKpzJBsnYTD8wAEN8fFtqYDLvBTp04hIyMDU6ZMQbNmzfD888+jT58+uPHGGy/5fceOHQtkuTpzu93w+XyWrmE1zmAPnKF+5GwpzGemAqVnYDw1Hyo2NP8iD4fnAQhujsTExBpvD/ivxz179qBNmzaIjY2Fy+VC79698dVXXwX6cETkcPLGIiDnOIzR00JW3nRpARe42+3G/v37UVZWBhHBnj170L59+1BmIyKHMD/aDPn4A6g77oe68hrdcSJGwKcRpqSkoE+fPpg1axaioqLQqVMneDyeUGYjIgeQY99WHX3/sCvUHWm640SUoM4DT0tLQ1oanzCiSCVlZVXnezduUrV1YkTpjhRReHY9EQVMVi0Gjh+BMXoqVMtWuuNEHBY4EQXE/ORDyD/fh7rtPqjUa3XHiUgscCKqNzlxFPL6QiAlFerOIbrjRCwWOBHVi1SUV+17R0fDGD0dKor73rqwwImoXmT1EuC7QzBGTYVq5dYdJ6KxwImozsysf0K2/g3q1nugul6nO07EY4ETUZ1I7nHIiheBzldCDXpQdxwCC5yI6kAqKmAungsYUTDGzIByBfUWEgoRFjgR1UreWgZ8cwDGyMehElrrjkPfY4ET0SXJro8hWzZCee6C6t5bdxw6BwuciC5KfDkwly8AOqVA/XK47jj0P1jgRFQj8X+/7y2AMXYGlCtadyT6HyxwIqqRrF0BHPoKxojHoFq30x2HasACJ6ILyOc7IO9vgLrp51A9+uqOQxfBAiei80jeSZivzgc6JkPdN1J3HLoEFjgRVRO/H+YrcwGzEsa4mVDRNV9Ml+yBBU5E1WTDn4CDX0INmwjVpuYL6ZJ9sMCJCAAg/94J+dtfoG68FcaPb9Qdh+ogqPfDnj59GosWLcKRI0eglMIjjzyCLl26hCobETUQKciDuTQTSOoEdf9o3XGojoIq8GXLlqF79+6YNm0a/H4/ysrKQpWLiBqIVFbCXDIPqCiv2vdu1Fh3JKqjgLdQzpw5g//85z8YMGAAAMDlcqF58+YhC0ZEDUPe+TPw1V6oBx+BapekOw7VgxIRCeQbDx8+jJdffhlJSUn45ptvkJycjBEjRqBJkybn3c/r9cLr9QIA0tPTUV5eHnzqS3C5XPD7/ZauYTXOYA+RMEPZ51ko/M1kNLnpdsQ99mQDJqu7cHgegODmaNSo5rOBAi7wgwcP4sknn8ScOXOQkpKCZcuWoWnTphg8ePAlv+/YsWOBLFdnbrcbPp/P0jWsxhnsIdxnkKICmL+ZBMTEwngyA6pxkxrvp1s4PA9AcHMkJtZ8RlDAWygJCQlISEhASkoKAKBPnz44dOhQoA9HRA1IzEqYSzKAslIY42bZtrzp0gIu8JYtWyIhIaH6iHrPnj1ISuL+GZETyF/fBL78AmrIOKj2HXXHoQAFdRbKqFGjsGDBAvj9frRp0wYTJkwIVS4isojs2wN5ZxVUn/5QP/XojkNBCKrAO3XqhPT09FBlISKLSXEhzFcygDaXQT3wCJRSuiNREPhOTKIIIaZZ9Wad0yVV53s3aao7EgWJBU4UIeS9tUD2Z1CDx0B1uFx3HAoBFjhRBJD92ZD1r0P1ugHqxlt1x6EQYYEThTk5VQzzlXlAQpuqTxnkvnfYYIEThTExzaqLM5QUVp3v3bSZ7kgUQixwojB25p1VwJ5Poe4bBfWDzrrjUIixwInClHy9D6dW/hHo8ROom36uOw5ZIKjzwInInuT0KZiL58JIaAM89Bj3vcMUj8CJwoyIwHztBaAwHy2nzYFqFqM7ElmEBU4UZmTzO8Duf0H98iFEd0nVHYcsxAInCiNyeD/krdeAbj+G8typOw5ZjAVOFCbkzGmYi+cCcS1hjHyc+94RgAVOFAZEBOaKF4G8XBhjZkA1b6E7EjUAFjhRGJAPNwE7t0PdPQzqiqt0x6EGwgIncjj59mvImiXAj66DuuVu3XGoAbHAiRxMzp6B+fJzVde1HDUZyuBLOpLw2SZyKBGBrFwInDwBY8x0qBZxuiNRAwu6wE3TxMyZM3llHqIGJv/4O2THNqi7hkJ1+ZHuOKRB0AX+7rvvon379qHIQkR1JN8dhqx6BbiqG9Rtv9QdhzQJqsDz8vKwa9cuDBw4MFR5iKgWcra0at+7WXMYo6dCGVG6I5EmQX2Y1WuvvYYHH3wQpaWlF72P1+uF1+sFAKSnp8PtdgezZK1cLpfla1iNM9iDXWcoemEOzuYcRfz/vYBGySmXvK9dZ6iPcJgBsGaOgAt8586diIuLQ3JyMvbu3XvR+3k8Hng8nuqvfT5foEvWidvttnwNq3EGe7DjDOb2zZAPN0HdMRjFiZ2AWvLZcYb6CocZgODmSExMrPH2gAt83759+PTTT/HZZ5+hvLwcpaWlWLBgASZNmhToQxLRJcjxI5A/LQJ+2BXqF/frjkM2EHCBDx06FEOHDgUA7N27F++88w7Lm8giUlYGc9GzQOMm3PemajwPnMgBZNVi4Ni3MB6eCtUyQXccsomQXJHn6quvxtVXXx2KhyKi/2F+8iHkn+9D3XYv1NXX6o5DNsIjcCIbkxNHIa//EbgiFequB3THIZthgRPZlFSUV53vHe2qeqt8FPe96XwscCKbktVLgO8OwRg5GaqV88+DptBjgRPZkJn1T8jWv0HdcjfUNb10xyGbYoET2YzkHoeseBFI/iHU3cN0xyEbY4ET2YhUVFTtexsGjLEzoFwhOVGMwhQLnMhG5C+vAd8ehDHicaiENrrjkM2xwIlsQnZ9DNn8DtTAX0Bd20d3HHIAFjiRDYgvB+byBcAProC6d4TuOOQQLHAizcRfAXPxXEAExriZUK5o3ZHIIVjgRJrJupXAoa9gPPQYVOt2uuOQg7DAiTSSz7Mgf18P1f92qOt+qjsOOQwLnEgTyT8Jc9l8oMPlUGmjdMchB2KBE2kgfn/VvrffD2PcLKjoRrojkQOxwIk0kLf/BBz8EmrYBKi2NV8ui6g2LHCiBib/3gnZ9BeoG26B0buf7jjkYCxwogYkBXkwl2YC7X8ANXiM7jjkcAF/0ILP58NLL72EwsJCKKXg8Xhw++23hzIbUViRykqYS+YB5WVV+96NGuuORA4XcIFHRUVh2LBhSE5ORmlpKZ544glcc801SEpKCmU+orAhG1cBX+2FGjkZ6jK+Tih4AW+hxMfHIzk5GQDQtGlTtG/fHvn5+SELRhROJHs35K9roPoOhNF3gO44FCZCsgeem5uLQ4cO4YorrgjFwxGFFSkqgLn0eaBdEtTQcbrjUBhRIiLBPMDZs2fx9NNP45577kHv3r0v+H2v1wuv1wsASE9PR3l5eTDL1crlcsHv91u6htU4gz2EYgaprEThbyajfN+/kTB3KVwdk0OUrm74PNhHMHM0alTz+wSCKnC/349nn30W3bp1wx133FGn7zl27Figy9WJ2+2Gz+ezdA2rcQZ7CMUM5sZVkA1vQD30GIzrbw5Rsrrj82AfwcyRmFjzewUC3kIRESxatAjt27evc3kTRRLZ92/I26ugeveD+qlHdxwKQwGfhbJv3z5s27YNHTt2xIwZMwAAQ4YMQY8ePUIWjsippLgQ5ivzgDaXQT34CJRSuiNRGAq4wK+88kqsWbMmlFmIwoKYJsxXM4HTJTAefxqqSTPdkShM8Z2YRCEm760F9n4Gdf9oqA6X645DYYwFThRCsj8bsv51qJ7XQ/X7me44FOZY4EQhIqeKq/a9E9pADX+U+95kORY4UQiICMxlLwAlhVXXtWzKfW+yHgucKATk/fXAF1lQ946C+gHfkUwNgwVOFCQ5+CVk7Qrg2j5QA36uOw5FEBY4URDk9Kmqfe+WCTAemsR9b2pQLHCiAIkIzOULgMI8GGNnQDWP0R2JIgwLnChAsmUj8NknUPc8BJX8Q91xKAKxwIkCIIf3Q95cBlzTC+rmu3THoQjFAieqJzlzGubiuUBcSxgjH+e+N2nDAieqBxGBrPgDkJcLY8wMqJhY3ZEogrHAiepBtm6C7PwIatAwqCuu0h2HIhwLnKiO5NuvIauXAj+6DurWu3XHIWKBE9WFnD0D8+XngJgWMEZNhjL40iH9+H8hUS1EBLLyj8DJEzDGTIdqEac7EhEAFjhRreSf70N2bIW6cwhUlx/pjkNUjQVOdAly9BvIqsXAVd2gbr9Xdxyi8wR8STUA2L17N5YtWwbTNDFw4EAMGjQoRLGI9JOzpVX73k2bwxg9FcqI0h2J6DwBH4GbpomlS5di9uzZyMzMxEcffYTvvvsulNmItCp+JQM48R2Mh6dCxcbrjkN0gYCPwA8cOIB27dqhbdu2AIC+ffsiKysLSUlJIQv3X+bG1ZAd2+p0X5/LhUq/P+QZGhJnsAExUXniKNQdg6Gu6qY7DVGNAi7w/Px8JCQkVH+dkJCA/fv3X3A/r9cLr9cLAEhPT4fb7a73Wmfad0B5p7p9SL4yFFym1HsNO+EM9hB9w81odv/DUFHO3TpxuVwBvebsJBxmAKyZI+ACF7nwxVnTZ0J4PB54PJ7qr30+X/0Xu7Zv1X914Ha7A1vDRjiDPbQMgxnC4XkIhxmA4OZITEys8faA98ATEhKQl5dX/XVeXh7i47lPSETUUAIu8M6dO+P48ePIzc2F3+/H9u3b0bNnz1BmIyKiSwh4CyUqKgqjRo3CM888A9M0cdNNN6FDhw6hzEZERJcQ1HngPXr0QI8ePUKVhYiI6oHvxCQicigWOBGRQ7HAiYgcigVORORQSmp6Rw4REdle2B2BP/HEE7ojBI0z2ANnsIdwmAGwZo6wK3AiokjBAicicqiwK/BzPzjLqTiDPXAGewiHGQBr5uAPMYmIHCrsjsCJiCIFC5yIyKGC+jArO/n444/x5ptv4ujRo/jd736Hzp07AwByc3MxZcqU6g9ET0lJwdixY3VGvaiLzQAA69atw5YtW2AYBkaOHInu3bvrC1pHa9aswebNmxEbGwsAGDJkiGM+/CwcLtg9ceJENGnSBIZhICoqCunp6boj1WrhwoXYtWsX4uLikJGRAQA4deoUMjMzcfLkSbRu3RpTpkxBTEyM5qQXV9MMlr0WJEwcOXJEjh49Kk8//bQcOHCg+vacnByZOnWqxmR1d7EZjhw5ItOnT5fy8nLJycmRRx99VCorKzUmrZvVq1fLhg0bdMeot8rKSnn00UflxIkTUlFRIdOnT5cjR47ojlVvEyZMkKKiIt0x6mXv3r1y8ODB816zK1eulHXr1omIyLp162TlypWa0tVNTTNY9VoImy2UpKSki152yCkuNkNWVhb69u2L6OhotGnTBu3atcOBAwc0JIwM516w2+VyVV+wm6yXmpp6wdF1VlYW+vXrBwDo16+f7Z+LmmawSthsoVxKbm4uZs6ciaZNm2Lw4MG46qqrdEeql/z8fKSkpFR/3apVK+Tn52tMVHfvvfcetm3bhuTkZAwfPtzW//T9r7pesNsJnnnmGQDAzTff7NjT8YqKiqov1xgfH4/i4mLNiQJjxWvBUQU+Z84cFBYWXnD74MGD0atXrxq/Jz4+HgsXLkSLFi3w9ddfY+7cucjIyECzZs0sTluzQGYQG5/peal5brnlFtx7770AgNWrV2PFihWYMGFCAyesv5r+vGu6YLfdzZkzB61atUJRURF++9vfIjExEampqbpjRSSrXguOKvBf//rX9f6e6OhoREdHAwCSk5PRtm1bHD9+/LwfEDakQGb43wtI5+fno1WrVqGMFbC6zjNw4EA8++yzFqcJjXC5YPd//x+Ji4tDr169cODAAUcWeFxcHAoKChAfH4+CgoLqHwQ6ScuWLat/HcrXQtjsgV9McXExTNMEAOTk5OD48eNo27at5lT107NnT2zfvh0VFRXIzc3F8ePHccUVV+iOVauCgoLqX+/YscMx10wNhwt2nz17FqWlpdW//uKLL9CxY0fNqQLTs2dPbN26FQCwdevWi/5L1c6sei2EzTsxd+zYgVdffRXFxcVo3rw5OnXqhCeffBKffPIJ1qxZg6ioKBiGgfvuu8+2L8aLzQAAa9euxQcffADDMDBixAhce+21mtPW7sUXX8Thw4ehlELr1q0xduxYxxzJ7tq1C8uXL6++YPc999yjO1K95OTkYN68eQCAyspKXH/99Y6YYf78+cjOzkZJSQni4uKQlpaGXr16ITMzEz6fD263G1OnTrX1z1JqmmHv3r2WvBbCpsCJiCJN2G+hEBGFKxY4EZFDscCJiByKBU5E5FAscCIih2KBExE5FAuciMih/h++ZY8vlOM8lgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, we store numbers from -16 to 16 in a list called input_series and next we apply ReLU to all these numbers and plot them\n",
    "\n",
    "try:\n",
    "    from matplotlib import pyplot\n",
    "    pyplot.style.use('ggplot')\n",
    "    input_series = [x for x in range(-16,16)]\n",
    "    output_series = [ReLU(x) for x in input_series]\n",
    "    pyplot.plot(input_series, output_series)\n",
    "    pyplot.show()\n",
    "except:\n",
    "    print('actual implementaion of ReLU function')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
