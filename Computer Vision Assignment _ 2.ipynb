{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a52402f",
   "metadata": {},
   "source": [
    "1.Explain convolutional neural network, and how does it work?\n",
    "\n",
    "A Convolutional Neural Network, also known as CNN or ConvNet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image. A digital image is a binary representation of visual data.\n",
    "\n",
    "It is a type of feed-forward artificial network where the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex. The visual cortex has a small region of cells that are sensitive to specific regions of the visual field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bfcc5",
   "metadata": {},
   "source": [
    "2.How does refactoring parts of your neural network definition favor you?\n",
    "\n",
    "Deep neural networks (DNN) are growing in capability and applicability. Their effectiveness has led to their use in safety critical and autonomous systems, yet there is a dearth of cost-effective methods available for reasoning about the behavior of a DNN. The applicability and scalability of existing DNN verification techniques are done through DNN refactoring. A DNN refactoring defines (a) the transformation of the DNN's architecture, i.e., the number and size of its layers, and (b) the distillation of the learned relationships between the input features and function outputs of the original to train the transformed network. Unlike with traditional code refactoring, DNN refactoring does not guarantee functional equivalence of the two networks, but rather it aims to preserve the accuracy of the original network while producing a simpler network that is amenable to more efficient property verification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8415e",
   "metadata": {},
   "source": [
    "3.What does it mean to flatten? Is it necessary to include it in the MNIST CNN? What is the reason for this?\n",
    "\n",
    "Flatten or Flattening means converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector.\n",
    "\n",
    "It is not necessary to include flattening in the MNIST CNN. However if we are using Dense or Linear layer of weights  then it is necessar to include flatten as we want to couple information that exists vertically as well as horizontally.\n",
    "\n",
    "The reason is that Flatten is usually considered as an operation, like an activation function, that is applied to an output. They provide “flatten” as a layer so that it’s easier to use; we don’t have to use some backend function or deal with the gradient calculation on our own. If we are counting the number of layers, then flatten should not be counted. But if we’re drawing a diagram of what is in our neural network, we should include it because it’s an operation and will help people understand our network’s architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ddf600",
   "metadata": {},
   "source": [
    "4.What exactly does NCHW stand for?\n",
    "\n",
    "NCHW stands for: batch N, channels C, depth D, height H, width W. It is a way to store multidimensional arrays / data frames / matrix into memory, which can be considered as a 1-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1b86a",
   "metadata": {},
   "source": [
    "5.Why are there 7*7*(1168-16) multiplications in the MNIST CNN's third layer?\n",
    "\n",
    "There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let’s ignore the batch axis to keep things simple. So for each of 14 * 14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that’s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications.\n",
    "\n",
    "What happened here is that our stride-2 convolution halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e446fc",
   "metadata": {},
   "source": [
    "6.Explain definition of receptive field?\n",
    "\n",
    "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c2a66",
   "metadata": {},
   "source": [
    "7.What is the scale of an activation's receptive field after two stride-2 convolutions? What is the reason for this?\n",
    "\n",
    "The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by). A receptive field of a feature can be described by its center location and its size. However, not all pixels in a receptive field is equally important to its corresponding CNN’s feature. Within a receptive field, the closer a pixel to the center of the field, the more it contributes to the calculation of the output feature. Which means that a feature does not only look at a particular region (i.e. its receptive field) in the input image, but also focus exponentially more to the middle of that region.\n",
    "\n",
    "The formula to get the scale of an activation's recptive field after two stride-2 covvolutions is :\n",
    "\n",
    "n out = [n in + 2p -k] / s + 1\n",
    "\n",
    "where, \n",
    "nin = number of input features\n",
    "nout = number of output features\n",
    "k = convolution kernel size\n",
    "p = convolution padding size\n",
    "s = convolution stride size\n",
    "\n",
    "Now, in this scenario, nin = 5x5, k = 3x3, p = 1x1, s = 2x2.\n",
    "Putting all these values in the above formula we will get\n",
    "\n",
    "n out = [5 + 2x1 -3] / 2 + 1 = [7 -3] / 2 + 1 = 4 / 2 + 1 = 2 + 1 = 3\n",
    "Therefore the output feature map is 3x3 ( green map).Applying the same convolution on top of the 3x3 feature map, we will get a 2x2 feature map (orange map)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508785f",
   "metadata": {},
   "source": [
    "8.What is the tensor representation of a color image?\n",
    "\n",
    "Colour images are three band monochrome images in which, each band contains a different color and the actual information is stored in the digital image. The color images contain gray level information in each spectral band.\n",
    "The images are represented as red, green and blue (RGB images). And each color image has 24 bits/pixel means 8 bits for each of the three color band(RGB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48369474",
   "metadata": {},
   "source": [
    "9.How does a color input interact with a convolution?\n",
    "\n",
    "The invariance of the CNN to an artifact is derived from the data. The CNN only has the data to learn if color is a decisive factor for recognizing an object or not. If you only present it with red 'A's, it will learn that red is a decisive factor for recognizing the 'A'. By presenting it with a large number of different 'A's that are colored differently. The CNN will learn that color has little influence in recognizing an 'A'. The weight of the red channels or red features will not be dominant. You might even find that the CNN will learn grayscale filters instead of color sensitive filters. The color distribution for some objects, especially natural objects like those in ImageNet is not uniform. This results in the CNN learning color sensitive filters. After training the filters will be weighted according to the distribution with which it can recognize the object with least amount of error.\n",
    "For instances of objects that may be appear in different colors, where these colors are arbitrary (e.g. letters or digits on a sign/poster) we need to present sufficient examples for the CNN to untangle color information from recognizing those letters and digits. If it happens to only recognize red 'A's, it's because we never showed it otherwise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
