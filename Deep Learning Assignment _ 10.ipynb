{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55196672",
   "metadata": {},
   "source": [
    "1.What does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf. Variable s) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, TensorFlow. js, TensorFlow Serving, or TensorFlow Hub.\n",
    "\n",
    "We can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. For example, we can use the CLI to inspect the model's SignatureDef s. The CLI enables us to quickly confirm that the input Tensor dtype and shape match the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ed10",
   "metadata": {},
   "source": [
    "2.When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "\n",
    "Tf Serving should be used in the following scenarios.\n",
    "i. When we need to deploy new algorithms and experiments in the production environments while keeping the same server architecture and APIs.\n",
    "ii. When we need to have a flexible and high-performing serving system for machine learning models.\n",
    "ii. When we need to provide out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\n",
    "\n",
    "Main Features of TF Serving :\n",
    "\n",
    "Can serve multiple models, or multiple versions of the same model simultaneously\n",
    "Exposes both gRPC as well as HTTP inference endpoints\n",
    "Allows deployment of new model versions without changing any client code\n",
    "Supports canarying new versions and A/B testing experimental models\n",
    "Adds minimal latency to inference time due to efficient, low-overhead implementation\n",
    "Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls\n",
    "Supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models\n",
    "\n",
    "Some tools we can use to deply it are :\n",
    "Flask, Azure, Kubernetes, TensorFlow Serving, Cortex, TorchServe, Triton Inference Server, KFServing , ForestFlow, Multi Model Server, DeepDetect , BentoML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc60775",
   "metadata": {},
   "source": [
    "3.How do you deploy a model across multiple TF Serving instances?\n",
    "\n",
    "To deploy a model across multiple TF Serving instances the steps are given below for Windows 10.\n",
    "\n",
    "Step 1: Install the Docker App.\n",
    "Step 2: Pull the TensorFlow Serving Image. docker pull tensorflow/serving. \n",
    "Step 3: Create and Train the Model. \n",
    "Step 4: Save the Model. \n",
    "Step 5: Serving the model using Tensorflow Serving. \n",
    "Step 6: Make a REST request the model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22561f",
   "metadata": {},
   "source": [
    "4.When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "\n",
    "gRPC API should be used in the following circumstances.\n",
    "\n",
    "Microservices connections: gRPC’s low-latency and high-speed throughput communication make it particularly useful for connecting architectures that consist of lightweight microservices where the efficiency of message transmission is paramount.\n",
    "Multi-language systems: With its native code generation support for a wide range of development languages, gRPC is excellent when managing connections within a polyglot environment. \n",
    "Real-time streaming: When real-time communication is a requirement, gRPC’s ability to manage bidirectional streaming allows your system to send and receive messages in real-time without waiting for Unary client-response communication. \n",
    "Low-power low-bandwidth networks: gRPC’s use of serialized Protobuf messages offers light-weight messaging, greater efficiency, and speed for bandwidth-constrained, low-power networks (especially when compared to JSON). IoT would be an example of this kind of network that could benefit from gRPC APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf4fb3",
   "metadata": {},
   "source": [
    "5.What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
    "\n",
    "While we deploy a Machine Learning Model, we need to reduce the model size, TFLite’s model converter can take a saved model and compress it to a much lighter format based on FlatBuffers. This is a dynamic, cross-platform serialization library initially created by Google without any preprocessing: this reduces the loading time and memory footprint.\n",
    "\n",
    "Another way we can reduce the model size while we deploy a TensorFlow model to a mobile or embedded device(other than only using smaller neural network architectures) is by using smaller bit-widths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f7819",
   "metadata": {},
   "source": [
    "6.What is quantization-aware training, and why would you need it?\n",
    "\n",
    "Quantization aware training emulates inference-time quantization, creating a model that downstream tools will use to produce actually quantized models. The quantized models use lower-precision (e.g. 8-bit instead of 32-bit float), leading to benefits during deployment.\n",
    "In QAT, as opposed to computing scale factors to activation tensors after the DNN is trained, the quantization error is considered when training the model. The training graph is modified to simulate the lower precision behavior in the forward pass of the training process. This introduces the quantization errors as part of the training loss, which the optimizer tries to minimize during the training. Thus, QAT helps in modeling the quantization errors during training and mitigates its effects on the accuracy of the model at deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162af6d",
   "metadata": {},
   "source": [
    "7.What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "\n",
    "Data parallelism is when we use the same model for every thread, but feed it with different parts of the data and  model parallelism is when we use the same data for every thread, but split the model among threads.\n",
    "\n",
    "Data Parallelism is generally recommended for the following reasons.\n",
    "\n",
    "Data parallelism can be easily implemented and it is thus the most widely used implementation strategy on multi-GPUs.\n",
    "Data parallelism means that each GPU uses the same model to trains on different data subset. In data parallel, there is no synchronization between GPUs in forward computing, because each GPU has a fully copy of the model, including the deep net structure and parameters.\n",
    "\n",
    "The performance of model parallelism is often worse than data parallelism, because the communication expenses from model parallelism are much more than that of data parallelism.\n",
    "\n",
    "Data parallelism is a way of performing parallel execution of an application on multiple processors. It focuses on distributing data across different nodes in the parallel execution environment and enabling simultaneous sub-computations on these distributed data across the different compute nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042d02d",
   "metadata": {},
   "source": [
    "8.When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
    "\n",
    "There are two distribution strategies we can use when training a model across multiple servers. Those are data parallelism and model parallelism. But, in general, Data Parallelism is recommended as it is is easy to apply.\n",
    "\n",
    "To choose in between these two parallelisms we have some following reasons.\n",
    "\n",
    "Data parallelism is a way of performing parallel execution of an application on multiple processors. It focuses on distributing data across different nodes in the parallel execution environment and enabling simultaneous sub-computations on these distributed data across the different compute nodes. \n",
    "\n",
    "Data parallelism can be easily implemented and it is thus the most widely used implementation strategy on multi-GPUs.\n",
    "Data parallelism means that each GPU uses the same model to trains on different data subset. In data parallel, there is no synchronization between GPUs in forward computing, because each GPU has a fully copy of the model, including the deep net structure and parameters.\n",
    "\n",
    "The performance of model parallelism is often worse than data parallelism, because the communication expenses from model parallelism are much more than that of data parallelism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
