{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380c64d9",
   "metadata": {},
   "source": [
    "1.What is the concept of cyclical momentum?\n",
    "\n",
    "Momentum and learning rate are closely related. It can be seen in the weight update equation for SGD that the momentum has similar impact as learning rate on weight updates. It has been found from the experiment that reducing the momentum when learning rate is increasing gives better results. This supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a better minima, so the new gradients need to be given more weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c6b2d",
   "metadata": {},
   "source": [
    "2.What callback keeps track of hyperparameter values (along with other data) during training?\n",
    "\n",
    "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.\n",
    "\n",
    "A callback function we are already familiar with is keras.callbacks.History(). This is automatically included in .fit().\n",
    "Another very useful one is keras.callbacks.ModelCheckpoint which saves the model with its weights at a certain point in the training. This can prove useful if your model is running for a long time and a system failure happens. Not all is lost then. It's a good practice to save the model weights only when an improvement is observed as measured by the acc, for example.\n",
    "keras.callbacks.EarlyStopping stops the training when a monitored quantity has stopped improving.\n",
    "keras.callbacks.LearningRateScheduler will change the learning rate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c6187",
   "metadata": {},
   "source": [
    "3.In the color dim plot, what does one column of pixels represent?\n",
    "\n",
    "The way digital remembers the colors is by digitizing (convert to be numbers) the analog color into three numeric (digital) values representing the color. This organization of numeric data is called pixels. A pixel contains the digital numeric RGB color data (numbers) of one tiny surface area. It creates a row and column array of pixels,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4314f7",
   "metadata": {},
   "source": [
    "4.In color dim, what does \"poor teaching\" look like? What is the reason for this?\n",
    "\n",
    "Each pixel in an image has a color which has been produced by some combination of the primary colors red, green, and blue (RGB). Each of these colors can have a brightness value ranging from 0 to 255 for a digital image with a bit depth of 8-bits. A RGB histogram results when the computer scans through each of these RGB brightness values and counts how many are at each level from 0 through 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ddc1d",
   "metadata": {},
   "source": [
    "5.Does a batch normalization layer have any trainable parameters?\n",
    "\n",
    "The end result is batch normalization adds two additional trainable parameters to a layer: The normalized output that's multiplied by a gamma (standard deviation) parameter, and the additional beta (mean) parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d3485",
   "metadata": {},
   "source": [
    "6.In batch normalization during preparation, what statistics are used to normalize? What about during the validation process?\n",
    "\n",
    "Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but allows the relationships between units and the nonlinear statistics of a single unit to change.\n",
    "\n",
    "Typically, the population statistics are taken from the training set. If we include the test set, at test time, we will have information that technically, we shouldn't have access to (information about the whole dataset). For the same reason, the validation set shouldn't be used to compute those statistics.\n",
    "\n",
    "We should keep in mind that due to the fact that batch-normalization isn't only at the input layer, the population's statistics will vary from epoch to epoch, as the network learns and changes its parameters (and therefore, its outputs at each layer).\n",
    "\n",
    "Therefore the common way to compute these statistics is to keep a (exponentially decaying or moving) averages during training. This will smoothen out the stochastic variations due to mini-batch training, and stay up to date to the current status of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9866022",
   "metadata": {},
   "source": [
    "7.Why do batch normalization layers help models generalize better?\n",
    "\n",
    "Batch normalization layers help models generalise better because its advantages.\n",
    "And the advantages are:\n",
    "\n",
    "Speed Up the Training\n",
    "By Normalizing the hidden layer activation the Batch normalization speeds up the training process.\n",
    "\n",
    "Handles internal covariate shift\n",
    "It solves the problem of internal covariate shift. Through this, we ensure that the input for every layer is distributed around the same mean and standard deviation.\n",
    "\n",
    "Internal covariate shift\n",
    "\n",
    "Smoothens the Loss Function\n",
    "Batch normalization smoothens the loss function that in turn by optimizing the model parameters improves the training speed of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d3251",
   "metadata": {},
   "source": [
    "8.Explain between MAX POOLING and AVERAGE POOLING.\n",
    "\n",
    "Max pooling: The maximum pixel value of the batch is selected.\n",
    "Average pooling: The average value of all the pixels in the batch is selected.\n",
    "The batch here means a group of pixels of size equal to the filter size which is decided based on the size of the image.\n",
    "Average pooling method smooths out the image and hence the sharp features may not be identified when this pooling method is used.\n",
    "Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image. For example: in MNIST dataset, the digits are represented in white color and the background is black."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600ba68",
   "metadata": {},
   "source": [
    "9.What is the purpose of the POOLING LAYER?\n",
    "\n",
    "Pooling layers provide an approach to down sampling feature maps by summarizing the presence of features in patches of the feature map. Two common pooling methods are average pooling and max pooling that summarize the average presence of a feature and the most activated presence of a feature respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d7f77",
   "metadata": {},
   "source": [
    "10.Why do we end up with Completely CONNECTED LAYERS?\n",
    "\n",
    "Completely connected layers are global (they can introduce any kind of dependence). This is also why convolutions work so well in domains like image analysis - due to their local nature they are much easier to train, even though mathematically they are just a subset of what fully connected layers can represent.\n",
    "For example, completely connected layers are an essential component of Convolutional Neural Networks (CNNs), which have been proven very successful in recognizing and classifying images for computer vision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734081a9",
   "metadata": {},
   "source": [
    "11.What do you mean by PARAMETERS?\n",
    "\n",
    "Parameters are integral parts of batch normalization. Likewise any network layer has the parameters(eg. bias, weights). Batch normalization also has parameters of its own. It is mainly of two types. One is learnable parameter and another is non-learnable parameter. Two learnable parameters called beta and gamma.\n",
    "Two non-learnable parameters (Mean Moving Average and Variance Moving Average) are saved as part of the ‘state’ of the Batch Norm layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac697c0c",
   "metadata": {},
   "source": [
    "12.What formulas are used to measure these PARAMETERS?\n",
    "\n",
    "These parameters are per Batch Norm layer. So if we have, say, three hidden layers and three Batch Norm layers in the network, we would have three learnable beta and gamma parameters for the three layers. Similarly for the Moving Average parameters. So, number of hiden or batch norm layers = number of learnable or non-learnable parameters.\n",
    "\n",
    "Mean and variance formulas are used to measure these parameters.\n",
    "For each activation vector separately, we need to calculate the mean and variance of all the values in the mini-batch.\n",
    "We need to calculate the normalized values for each activation feature vector using the corresponding mean and variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
