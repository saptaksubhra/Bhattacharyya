{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79f634f",
   "metadata": {},
   "source": [
    "1.Write the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a7a0370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights at the start of the training\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New weights after the training\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Testing network on new examples : \n",
      "[-0.16444904]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot, tanh\n",
    " \n",
    "# Class to create a neural\n",
    "# network with single neuron\n",
    "class SingleNeuron():\n",
    "     \n",
    "    def __init__(self):\n",
    "         \n",
    "        # Using seed to make sure it'll \n",
    "        # generate same weights in every run\n",
    "        random.seed(1)\n",
    "         \n",
    "        # 3x1 Weight matrix\n",
    "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    " \n",
    "    # tanh as activation function\n",
    "    def tanh(self, x):\n",
    "        return tanh(x)\n",
    " \n",
    "    # derivative of tanh function.\n",
    "    # Needed to calculate the gradients.\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1.0 - tanh(x) ** 2\n",
    " \n",
    "    # forward propagation\n",
    "    def forward_propagation(self, inputs):\n",
    "        return self.tanh(dot(inputs, self.weight_matrix))\n",
    "     \n",
    "    # training the neural network.\n",
    "    def train(self, train_inputs, train_outputs,\n",
    "                            num_train_iterations):\n",
    "                                 \n",
    "        # Number of iterations we want to\n",
    "        # perform for this set of input.\n",
    "        for iteration in range(num_train_iterations):\n",
    "            output = self.forward_propagation(train_inputs)\n",
    " \n",
    "            # Calculate the error in the output.\n",
    "            error = train_outputs - output\n",
    " \n",
    "            # multiply the error by input and then\n",
    "            # by gradient of tanh function to calculate\n",
    "            # the adjustment needs to be made in weights\n",
    "            adjustment = dot(train_inputs.T, error *\n",
    "                             self.tanh_derivative(output))\n",
    "                              \n",
    "            # Adjust the weight matrix\n",
    "            self.weight_matrix += adjustment\n",
    " \n",
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    single_neuron = SingleNeuron()\n",
    "     \n",
    "    print ('Random weights at the start of the training')\n",
    "    print (single_neuron.weight_matrix)\n",
    " \n",
    "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    train_outputs = array([[0, 1, 1, 0]]).T\n",
    " \n",
    "    neural_network.train(train_inputs, train_outputs, 10000)\n",
    " \n",
    "    print ('New weights after the training')\n",
    "    print (single_neuron.weight_matrix)\n",
    " \n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Testing network on new examples : \")\n",
    "    print (single_neuron.forward_propagation(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52d9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing Relu on (1.0) gives 1.0\n",
      "Implementing Relu on (0.0) gives 0.0\n",
      "Implementing Relu on (-36.0) gives 0.0\n",
      "Implementing Relu on (43.0) gives 43.0\n",
      "Implementing Relu on (-33.3) gives 0.0\n"
     ]
    }
   ],
   "source": [
    "#2.Write the Python code to implement ReLU.\n",
    "\n",
    "try:\n",
    "    def relu(a):\n",
    "        return max(0.0,a)\n",
    "    a = 1.0\n",
    "    print('Implementing Relu on (%.1f) gives %.1f' % (a, relu(a)))\n",
    "    a = 0.0\n",
    "    print('Implementing Relu on (%.1f) gives %.1f' % (a, relu(a)))\n",
    "    a = -36.0\n",
    "    print('Implementing Relu on (%.1f) gives %.1f' % (a, relu(a)))\n",
    "    a = 43.0\n",
    "    print('Implementing Relu on (%.1f) gives %.1f' % (a, relu(a)))\n",
    "    a = -33.3\n",
    "    print('Implementing Relu on (%.1f) gives %.1f' % (a, relu(a)))\n",
    "except:\n",
    "    print('Implementation of ReLU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ef13d",
   "metadata": {},
   "source": [
    "3.Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "Let's write a function that computes the matrix product of two tensors. We'll need three nested for loops: one for the row indices, one for the column indices, and one for the inner sum. ac and ar stand for number of columns of a and number of rows of a, respectively (the same convention is followed for b), and we make sure calculating the matrix product is possible by checking that a has as many columns as b has rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3494c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import tensor\n",
    "\n",
    "def matmul(a,b):\n",
    "    ar,ac = a.shape # n_rows * n_cols\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e19b15",
   "metadata": {},
   "source": [
    "4.Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a86322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e293d8f",
   "metadata": {},
   "source": [
    "5.What is the “hidden size” of a layer?\n",
    "\n",
    "The size of the hidden layer is normally between the size of the input and output. It should be 2/3 the size of the input layerplus the size of the o/p layer. The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4502703",
   "metadata": {},
   "source": [
    "6.What does the t method do in PyTorch?\n",
    "\n",
    "torch.t(input) → Tensor\n",
    "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).\n",
    "\n",
    "Parameters\n",
    "input (Tensor) – the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16ca6851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5827)\n",
      "tensor(1.5827)\n",
      "tensor([ 1.5105,  0.0037, -0.6219, -2.1718,  0.3947,  1.1170, -0.3889, -0.5528,\n",
      "         0.5944, -0.0591,  0.9541,  0.5999, -0.1821,  1.2880, -0.7170,  1.0794])\n",
      "tensor([[ 0.7269,  1.3612, -0.4919, -1.1367,  0.3969],\n",
      "        [ 0.8873, -0.7430, -0.2489, -0.7647,  0.2062],\n",
      "        [ 0.7858,  0.0079, -0.2266, -1.4388,  0.0361],\n",
      "        [ 0.2394,  0.1977,  0.9185, -0.6987, -0.4942]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7269,  0.8873,  0.7858,  0.2394],\n",
       "        [ 1.3612, -0.7430,  0.0079,  0.1977],\n",
       "        [-0.4919, -0.2489, -0.2266,  0.9185],\n",
       "        [-1.1367, -0.7647, -1.4388, -0.6987],\n",
       "        [ 0.3969,  0.2062,  0.0361, -0.4942]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of t method in Pytorch\n",
    "import torch\n",
    "a = torch.randn(())\n",
    "print(a)\n",
    "torch.t(a)\n",
    "print(torch.t(a))\n",
    "a = torch.randn(16)\n",
    "print(a)\n",
    "a = torch.randn(4,5)\n",
    "print(a)\n",
    "torch.t(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e590d",
   "metadata": {},
   "source": [
    "7.Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Matrix multiplication is executed in plain Python with respect to nested for loops. And that is not a very good idea as Python is a slow language and that is not going to be efficient. That is why matrix multiplication written in plain Python very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b715675",
   "metadata": {},
   "source": [
    "8.In matmul, why is ac==br?\n",
    "\n",
    "In matmul or matrix multiplication, ac == br because number of columns in left matrix must be equal to number of rows in right matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17ba83",
   "metadata": {},
   "source": [
    "9.In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "Magic Commands are succinct solutions to common obstacles. They can be identified by their prefix % or %%. There are two kinds of magic commands: Line Magics (% prefix) and Cell Magics (%% prefix). Line magics operate only on their line when Cell Magics operate on their full cell. We can measure the time taken for a single cell to execute in Jupyter Notebook by the magic command or cell magic %%time. Let's have a look through the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540bb0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5):\n",
    "    sqrts = [n ** (1/2) for n in range(10**i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33988e",
   "metadata": {},
   "source": [
    "10.What is elementwise arithmetic?\n",
    "\n",
    "All the basic operators (+, -, *,  /, >, <, == ) can be applied elementwise. That means if we write a+b for two tensors a and b that have the same shape, we will get a tensor composed of the sums the elements of a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "399af0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 45.4300,  40.5400, 103.2900])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "a = tensor([2.0,4.5,-5.6])\n",
    "b = tensor([43.43,36.04,108.89])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f43e1b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Booleans operators will return an array of Booleans:\n",
    "\n",
    "a > b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff404fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a < b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd545e2",
   "metadata": {},
   "source": [
    "If we want to know if every element of a is less than the corresponding element in b, or if two tensors are equal, we need to combine those elementwise operations with torch.all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be186494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(False))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a < b).all(), (a == b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ddd1a",
   "metadata": {},
   "source": [
    "11.Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f29978c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a > b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f7860",
   "metadata": {},
   "source": [
    "12.What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "A tensor with rank 0 is a zero-dimensional array. The element of a zero-dimensional array is a point. This is represented as a Scalar in Math and has magnitude. Eg: s = 48.3. Shape - []\n",
    "Reduction operations like all(), sum() and mean() return tensors with only one element, called rank-0 tensors. If you want to convert this to a plain Python Boolean or number, you need to call .item():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30af8371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.086669921875"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of rank-0 tensor into a Python data type\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "a = tensor([2.0,4.5,-5.6])\n",
    "b = tensor([43.43,36.04,108.89])\n",
    "(a + b).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bb2e4",
   "metadata": {},
   "source": [
    "13.How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i-th row of a and the j-th column of b before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed. That means that our matrix multiplication will be speeded up eliminating loops and replacing them with PyTorch functionalities. This will give us C speed (underneath PyTorch) instead of Python speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3196b",
   "metadata": {},
   "source": [
    "14.What are the broadcasting rules?\n",
    "\n",
    "Broadcasting Rules: \n",
    "Broadcasting two arrays together follow these rules:\n",
    " \n",
    "If the arrays don’t have the same rank then prepend the shape of the lower rank array with 1s until both shapes have the same length.\n",
    "The two arrays are compatible in a dimension if they have the same size in the dimension or if one of the arrays has size 1 in that dimension.\n",
    "The arrays can be broadcast together if they are compatible with all dimensions.\n",
    "After broadcasting, each array behaves as if it had shape equal to the element-wise maximum of shapes of the two input arrays.\n",
    "In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac50f4",
   "metadata": {},
   "source": [
    "15.What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "Tensor.expand_as(other) → Tensor\n",
    "Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91a0ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[0.0772, 0.4048, 0.4750],\n",
      "        [0.7310, 0.8615, 0.9692]])\n",
      "b: tensor([[[0.0082, 0.2850, 0.5373],\n",
      "         [0.7049, 0.9183, 0.9978]],\n",
      "\n",
      "        [[0.6508, 0.4721, 0.2031],\n",
      "         [0.7776, 0.5318, 0.2108]]])\n",
      "c: tensor([[[0.0772, 0.4048, 0.4750],\n",
      "         [0.7310, 0.8615, 0.9692]],\n",
      "\n",
      "        [[0.0772, 0.4048, 0.4750],\n",
      "         [0.7310, 0.8615, 0.9692]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2,2, 3)\n",
    "print('a:',a)\n",
    "print('b:',b)\n",
    "c = a.expand_as(b)\n",
    "print('c:',c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
