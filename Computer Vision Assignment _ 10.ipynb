{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149ca827",
   "metadata": {},
   "source": [
    "1.Why don't we start all of the weights with zeros?\n",
    "\n",
    "If we initialize all the weights with 0, then the derivative with respcet to loss function is the same for every weight in W[l], thus all weights have the same value in subsequent iterations. This problem is known as network failing to break symmetry.This makes hidden layers symmetric and this process continues for all the n iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b08057",
   "metadata": {},
   "source": [
    "2.Why is it beneficial to start weights with a mean zero distribution?\n",
    "\n",
    "There are two scenarios to start weights. One is too-large initialization and the other is too-small initialization. A too-large initialization leads to exploding gradients. A too-small initialization leads to vanishing gradients.\n",
    "To prevent the gradients of the network’s activations from vanishing or exploding, we will stick to the following rules of thumb:\n",
    "\n",
    "The mean of the activations should be zero.\n",
    "The variance of the activations should stay the same across every layer.\n",
    "Under these two assumptions, the backpropagated gradient signal should not be multiplied by values too small or too large in any layer. It should travel to the input layer without exploding or vanishing.\n",
    "Therefore the aim  of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. And that is the benefit to start weights with a mean zero distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5b123",
   "metadata": {},
   "source": [
    "3.What is dilated convolution, and how does it work?\n",
    "\n",
    "It is a technique that expands the kernel (input) by inserting holes between its consecutive elements. In simpler terms, it is the same as convolution but it involves pixel skipping, so as to cover a larger area of the input.\n",
    "Dilated convolution helps expand the area of the input image covered without pooling. The objective is to cover more information from the output obtained with every convolution operation. This method offers a wider field of view at the same computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d73d8",
   "metadata": {},
   "source": [
    "4.What is TRANSPOSED CONVOLUTION, and how does it work?\n",
    "\n",
    "Transposed convolutions are standard convolutions but with a modified input feature map. The stride and padding do not correspond to the number of zeros added around the image and the amount of shift in the kernel when sliding it across the input, as they would in a standard convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ef908",
   "metadata": {},
   "source": [
    "5.Explain Separable convolution.\n",
    "\n",
    "A Separable Convolution is a process in which a single convolution can be divided into two or more convolutions to produce the same output. A single process is divided into two or more sub-processes to achieve the same effect.\n",
    "Mainly there are two types of Separable Convolutions :\n",
    "\n",
    "Spatially Separable Convolutions.\n",
    "Depth-wise Separable Convolutions.\n",
    "\n",
    "Spatially Separable Convolutions: In images height and width are called spatial axes. The kernel that can be separated across spatial axes is called the spatially separable kernel. The kernel is broken into two smaller kernels and those kernels are multiplied sequentially with the input image to get the same effect of the full kernel.\n",
    "\n",
    "Depthwise Separable Convolutions: When we call tf.keras.layers.SeparableConv2D we would be calling a Depthwise separable convolution layer itself. Here we can use even those kernels which can not be spatially separable. Similar to spatial convolution, here also a regular convolution is divided into two convolutions namely :\n",
    "\n",
    "Depthwise convolution\n",
    "Pointwise convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b666c5",
   "metadata": {},
   "source": [
    "6.What is depthwise convolution, and how does it work?\n",
    "\n",
    "Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output.\n",
    "In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:\n",
    "\n",
    "Split the input and filter into channels.\n",
    "We convolve each input with the respective filter.\n",
    "We stack the convolved outputs together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791baa01",
   "metadata": {},
   "source": [
    "7.What is Depthwise separable convolution, and how does it work?\n",
    "\n",
    "The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension, the number of channels as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. We can image each channel as a particular interpretation of that image; in for example, the 'red' channel interprets the 'redness' of each pixel, the 'blue' channel interprets the 'blueness' of each pixel, and the 'green' channel interprets the 'greenness' of each pixel. An image with 64 channels has 64 different interpretations of that image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b7912",
   "metadata": {},
   "source": [
    "8.Capsule networks are what they sound like.\n",
    "\n",
    "A Capsule Neural Network (CapsNet) is a machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f65e4",
   "metadata": {},
   "source": [
    "9.Why is POOLING such an important operation in CNNs?\n",
    "\n",
    "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf881aa8",
   "metadata": {},
   "source": [
    "10.What are receptive fields and how do they work ?\n",
    "\n",
    "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network. It gives us an idea of where we’re getting our results from as data flows through the layers of the network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
