{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fbc297",
   "metadata": {},
   "source": [
    "1.What are the main tasks that autoencoders are used for?\n",
    "\n",
    "An autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations (encoding) by training the network to ignore signal 'noise'.\n",
    "\n",
    "The main tasks that autoencoders are used for : \n",
    "Dimensionality Reduction\n",
    "Image Compression\n",
    "Image Denoising\n",
    "Feature Extraction\n",
    "Image generation\n",
    "Sequence to sequence prediction\n",
    "Recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d3ddb",
   "metadata": {},
   "source": [
    "2.Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?\n",
    "\n",
    "Let's say we have a large dataset but most of it is unlabeled dataset, then we can first train a stacked autoencoder using all the data , then we can reuse the lower layers to generate neural network for our actual task and train it by using the labeled dataset. That means first and foremost we need to train the deep autoencoder on the full dataset ( labeled and unlabeled) and then reuse its lower half for the classifier and train the classifier by using the labeled dataset.\n",
    "If we are coming across a complex supervised task but we don't have a lot of labeled training dataset , the the solution is to find a neural network which performs a similar task and reuse its lower layers. And this makes it possible to train a high-performance model using little training dataseta or instances because our neural network would not have to learn all the low-level features. It will reuse the feature detectors learnt by the existing network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288040f7",
   "metadata": {},
   "source": [
    "3.If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?\n",
    "\n",
    "If an autoencoder perfectly reconstructs the inputs, does not necessarily mean that it is a good autoencoder. Probably it is simply an overcomplete autoencoder that learnt to copy its inmputs to the codings layer and then to the ouputs. Actually, even if the codings layer contained in a single neuron, iot would be possible for a very deep autoencoder to learn to map each training instance to a different coding.\n",
    "\n",
    "To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss( for example, compute the MSE, the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good  sign that the autoencoder is bad , but a low reconstruction loss is not a guarantee and that is good. We should also evaluate the performance of an autoencoder in accordance with its applications. Fort example, if we are using the autoencoder for unsupervised pretraining  of a classifier, then we should also evaluate the performace of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc0734",
   "metadata": {},
   "source": [
    "4.What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?\n",
    "\n",
    "Undercomplete Autoencoder is a type of Autoencoder. An undercomplete autoencoder is one whose codings layer is smaller than input and output layers. Its goal is to capture the important features present in the data. It has a small hidden layer hen compared to Input Layer. This Autoencoder do not need any regularization as they maximize the probability of data rather copying the input to output.\n",
    "\n",
    "Overcomplete autoencoders : An overcomplete autoencoder is one whose codings layer is larger than input and output layers. This is when our encoding output's dimension is larger than our input's dimension\n",
    "Essentially we increase the dimension of our data with an overcomplete autoencoder.\n",
    "\n",
    "The main risk of an excessively undercomplete autoencoder is that it might fail to reconstruct the inputs.\n",
    "\n",
    "The main risk of an overcomplete autoencoder is that it might just copy the inputs to the outputs without learning any useful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c8c30",
   "metadata": {},
   "source": [
    "5.How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
    "\n",
    "An autoencoder with tied weights has decoder weights that are the transpose of the encoder weights; this is a form of parameter sharing, which reduces the number of parameters of the model.\n",
    "To apply tying weights in a stacked encoder, we need to create a custom layer to tie weights between the layer using keras. This custom layer acts as a regular dense layer, but it uses the transposed weights of the encoder’s dense layer, however having its own bias vector. To tie the weights of an encoder layer and its corresponding layer we simply make the decoder weights equal to the transpose of the encoder weights. The point of tying weights in a stacked autoencoder is to reduce the number of weights of the model almost to half of the original, thus reducing the risk of over-fitting and speeding up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e33ad",
   "metadata": {},
   "source": [
    "6.What is a generative model? Can you name a type of generative autoencoder?\n",
    "\n",
    "A Generative Model is a powerful way of learning any kind of data distribution using unsupervised learning and it has achieved tremendous success in just few years. All types of generative models aim at learning the true data distribution of the training set so as to generate new data points with some variations. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits.The output distribution is similar to training data. \n",
    "\n",
    "Variational autoencoder is a type of generative encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6842545",
   "metadata": {},
   "source": [
    "7.What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "\n",
    "A generative adversarial network (GAN) is a machine learning (ML) model in which two neural networks compete with each other to become more accurate in their predictions. GANs typically run unsupervised and use a cooperative zero-sum game framework to learn.\n",
    "\n",
    "A few tasks in real life applications where GANs can shine  are : \n",
    "\n",
    "Generate Examples for Image Datasets\n",
    "Generating examples is very handy in medicine or material science, where there’s very little data to work with. \n",
    "Generate Photographs of Human Faces\n",
    "Video game designers can use this to generate realistic human faces. \n",
    "Generate Realistic Photographs\n",
    "Very useful for photographers and videographers. \n",
    "Generate Cartoon Characters\n",
    "Artists can use this to create a new character design, or scenes in a cartoon, or even in a video game. \n",
    "Image-to-Image Translation\n",
    "Photographers can use these algorithms to convert day into night, summer into winter, etc.\n",
    "GANs can be used to simulate a worst-case scenario to optimize risk management in a business. \n",
    "\n",
    "Other use cases of GAN could be:\n",
    "Text-to-Image Translation\n",
    "Face Frontal View Generation\n",
    "Generate New Human Poses\n",
    "Photos to Emojis\n",
    "Face Aging\n",
    "Super Resolution\n",
    "Photo Inpainting\n",
    "Clothing Translation\n",
    "Video Prediction\n",
    "3D Object Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfefcf4",
   "metadata": {},
   "source": [
    "8.What are the main difficulties when training GANs?\n",
    "\n",
    "The main difficulties when training GANs are mode collapse, non-convergence and instability, due to inappropriate design of network architecture, use of objective function and selection of optimization algorithm. Also, it offers high cost due to higher material cost and costly processes involved in its manufacturing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
