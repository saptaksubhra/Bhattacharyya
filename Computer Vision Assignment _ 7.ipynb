{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cfb637",
   "metadata": {},
   "source": [
    "1.What is the COVARIATE SHIFT Issue, and how does it affect you?\n",
    "\n",
    "Covariate shift occurs when the distribution of variables in the training data is different to real-world or testing data. This means that the model may make the wrong predictions once it is deployed, and its accuracy will be significantly lower. It is the most common type of shift and it is now gaining more attention as nearly every real-world dataset suffers from this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536b7ff",
   "metadata": {},
   "source": [
    "2.What is the process of BATCH NORMALIZATION?\n",
    "\n",
    "It is a process to make neural networks faster and more stable through adding extra layers in a deep neural network. The new layer performs the standardizing and normalizing operations on the input of a layer coming from a previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00bfa9",
   "metadata": {},
   "source": [
    "3.Using our own terms and diagrams, explain LENET ARCHITECTURE.\n",
    "\n",
    "The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs. It can even run on the CPU (if our system does not have a suitable GPU), making it a great 'first CNN'.\n",
    "\n",
    "However, if we do have GPU support and can access your GPU via Keras, you will enjoy extremely fast training times (in the order of 3-10 seconds per epoch, depending on your GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86afc08",
   "metadata": {},
   "source": [
    "4.Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.\n",
    "\n",
    "The Alexnet has eight layers with learnable parameters. The model consists of five layers with a combination of max pooling followed by 3 fully connected layers and they use Relu activation in each of these layers except the output layer. They found out that using the relu as an activation function accelerated the speed of the training process by almost six times. They also used the dropout layers, that prevented their model from overfitting. Further, the model is trained on the Imagenet dataset. The Imagenet dataset has almost 14 million images across a thousand classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1973d",
   "metadata": {},
   "source": [
    "5.Describe the vanishing gradient problem.\n",
    "\n",
    "It describes the situation where a deep multilayer feed-forward network or a recurrent neural network is unable to propagate useful gradient information from the output end of the model back to the layers near the input end of the model. The vanishing gradients problem is one example of the unstable behaviour of a multilayer neural network. Networks are unable to backpropagate the gradient information to the input layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e193d",
   "metadata": {},
   "source": [
    "6.What is NORMALIZATION OF LOCAL RESPONSE?\n",
    "\n",
    "Local Response Normalization(LRN) type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83d26",
   "metadata": {},
   "source": [
    "7.In AlexNet, what WEIGHT REGULARIZATION was used?\n",
    "\n",
    "Keras provides a weight regularization API that allows you to add a penalty for weight size to the loss function.\n",
    "\n",
    "Three different regularizer instances are provided; they are:\n",
    "\n",
    "L1: Sum of the absolute weights.\n",
    "L2: Sum of the squared weights.\n",
    "L1L2: Sum of the absolute and the squared weights.\n",
    "The regularizers are provided under keras.regularizers and have the names l1, l2 and l1_l2. Each takes the regularizer hyperparameter as an argument. For example:\n",
    "\n",
    "keras.regularizers.l1(0.01)\n",
    "keras.regularizers.l2(0.01)\n",
    "keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "By default, no regularizer is used in any layers.\n",
    "\n",
    "A weight regularizer can be added to each layer when the layer is defined in a Keras model.\n",
    "\n",
    "This is achieved by setting the kernel_regularizer argument on each layer. A separate regularizer can also be used for the bias via the bias_regularizer argument, although this is less often used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1542e0",
   "metadata": {},
   "source": [
    "8.Using our own terms and diagrams, explain VGGNET ARCHITECTURE.\n",
    "\n",
    "VGG stands for Visual Geometry Group; it is a standard deep Convolutional Neural Network (CNN) architecture with multiple layers. The “deep” refers to the number of layers with VGG-16 or VGG-19 consisting of 16 and 19 convolutional layers. The VGG architecture is the basis of ground-breaking object recognition models. Developed as a deep neural network, the VGGNet also surpasses baselines on many tasks and datasets beyond ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bc1fe",
   "metadata": {},
   "source": [
    "9.Describe VGGNET CONFIGURATIONS.\n",
    "\n",
    "The Convolution Neural Network configuration of the VGG net are with the\n",
    "\n",
    "following layers:  VGG-11, VGG-11 (LRN), VGG-13, VGG-16 (Conv1), VGG-16, VGG-19.\n",
    "\n",
    "The networks are referred by their names (A–E). All configurations follow the traditional design and differ only in the depth: from 11 weight layers in network A that is 8 Conv. and 3 FC layers to 19 weight layers in network E that is 16 Conv. and 3 FC layers. The width of each Conv. layer is the number of channels is rather small, which is starting from 64 in the first layer and then goes on increasing by a factor of 2 after each max-pooling layer until it reaches 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028da624",
   "metadata": {},
   "source": [
    "10.What regularization methods are used in VGGNET to prevent overfitting?\n",
    "\n",
    "The regularization methods used in VGGNET to prevent overfitting are :\n",
    "L2 and L1 Regularization: L2 and L1 are the most common types of regularization. Regularization works on the premise that smaller weights lead to simpler models which in results helps in avoiding overfitting. \n",
    "Dropout: Another most frequently used regularization technique is dropout. It essentially means that during the training, randomly selected neurons are turned off or ‘dropped’ out. \n",
    "Early Stopping: It is a kind of cross-validation strategy where one part of the training set is used as a validation set, and the performance of the model is gauged against this set.\n",
    "Data Augmentation: The simplest way to reduce overfitting is to increase the data, and this technique helps in doing so."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
