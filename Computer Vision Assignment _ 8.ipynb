{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f577565c",
   "metadata": {},
   "source": [
    "1.Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE.\n",
    "\n",
    "An inception network is a deep neural network with an architectural design that consists of repeating components referred to as Inception modules. Inception Modules are used in Convolutional Neural Networks to allow for more efficient computation and deeper Networks through a dimensionality reduction with stacked 1Ã—1 convolutions. The modules were designed to solve the problem of computational expense, as well as overfitting, among other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f89da",
   "metadata": {},
   "source": [
    "2.Describe the Inception block.\n",
    "\n",
    "An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. It allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf1bcb",
   "metadata": {},
   "source": [
    "3.What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)?\n",
    "\n",
    "A 1x1 convolution or 1 layer covlolutional simply maps an input pixel with all it's channels to an output pixel, not looking at anything around itself. It is often used to reduce the number of depth channels, since it is often very slow to multiply volumes with extremely large depths.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba0e11",
   "metadata": {},
   "source": [
    "4.THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE.\n",
    "\n",
    "Advantages of dimensionality reduction\n",
    "\n",
    "It reduces the time and storage space required. The removal of multicollinearity improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. Reduce space complexity.\n",
    "\n",
    "Disadvantages of Dimensionality Reduction\n",
    "\n",
    "It may lead to some amount of data loss. PCA tends to find linear correlations between variables, which is sometimes undesirable. PCA fails in cases where mean and covariance are not enough to define datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22ae29",
   "metadata": {},
   "source": [
    "5.Mention three components. Style GoogLeNet\n",
    "\n",
    "GoogleNet possesses seven million parameters and contains nine inception modules, four convolutional layers, four max-pooling layers, three average pooling layers, five fully-connected layers, and three softmax layers for the main auxiliary classifiers in the network [33]. The GoogleNet Architecture is 22 layers deep, with 27 pooling layers included. There are 9 inception modules stacked linearly in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe33b1",
   "metadata": {},
   "source": [
    "6.Using our own terms and diagrams, explain RESNET ARCHITECTURE.\n",
    "\n",
    "ResNet, which was proposed in 2015 by researchers at Microsoft Research introduced a new architecture called Residual Network. Residual Block: In order to solve the problem of the vanishing/exploding gradient, this architecture introduced the concept called Residual Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa5531",
   "metadata": {},
   "source": [
    "7.What do Skip Connections entail?\n",
    "\n",
    "Skip Connections (or Shortcut Connections) as the name suggests skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers. Skip Connections were introduced to solve different problems in different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322d634",
   "metadata": {},
   "source": [
    "8.What is the definition of a residual Block?\n",
    "\n",
    "A residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the output of the corresponding layer in the main path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d10f7",
   "metadata": {},
   "source": [
    "9.How can transfer learning help with problems?\n",
    "\n",
    "Transfer learning is a technique to help solve the problem. As a concept, it works by transferring as much knowledge as possible from an existing model to a new model designed for a similar task. For example, transferring the more general aspects of a model which make up the main processes for completing a task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9556c0",
   "metadata": {},
   "source": [
    "10.What is transfer learning, and how does it work?\n",
    "\n",
    "Transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task. So simply, a model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecf9dc",
   "metadata": {},
   "source": [
    "11.HOW DO NEURAL NETWORKS LEARN FEATURES?\n",
    "\n",
    "Neural networks generally perform supervised learning tasks, building knowledge from data sets where the right answer is provided in advance. The networks then learn by tuning themselves to find the right answer on their own, increasing the accuracy of their predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fdf6f",
   "metadata": {},
   "source": [
    "12.WHY IS FINE-TUNING BETTER THAN START-UP TRAINING?\n",
    "\n",
    "If it is in case of greedy training and tuning works then it works better than start-up training. For deep networks, it is very hard to train the whole network with backprop because of the vanishing gradient problem. Your last layers will learn quickly but your first layer, the one connecting the input data to the first layer of hidden units, will learn so slowly that it will be almost identical to the random initialization. This doesn't depend on the size of the dataset but the number of levels in the network. Shallow nets can be trained with backprop, deep nets are better with greedy training layer by layer and then backprop to finetune the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
