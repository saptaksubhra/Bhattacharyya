{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c88783e",
   "metadata": {},
   "source": [
    "1.Is there any way to combine five different models that have all been trained on the same training\n",
    "data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is\n",
    "the reason?\n",
    "\n",
    "If we have trained five different models and they all achieve 95% precision, then we can try combining them into a voting ensemble, which will often give us even better results. It works better if the models are very different (e.g., an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier). It is even better if they are trained on different training instances (that’s the whole point of bagging and pasting ensembles), but if not it will still work as long as the models are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7d06a",
   "metadata": {},
   "source": [
    "2.What's the difference between hard voting classifiers and soft voting classifiers?\n",
    "\n",
    "Hard voting classifier classifies data based on class labels and the weights associated with each classifier. Soft voting classifier classifies data based on the probabilities and the weights associated with each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be1acd",
   "metadata": {},
   "source": [
    "3.Is it possible to distribute a bagging ensemble's training through several servers to speed up the\n",
    "process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all\n",
    "options.\n",
    "\n",
    "It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential, and you will not gain anything by distributing training across multiple servers. Regarding stacking ensembles, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60103fc0",
   "metadata": {},
   "source": [
    "4.What is the advantage of evaluating out of the bag?\n",
    "\n",
    "Less Computation: It requires less computation as it allows one to test the data as it is being trained.\n",
    "\n",
    "No leakage of data: Since the model is validated on the OOB Sample, which means data hasn’t been used while training the model in any way, so there isn’t any leakage of data and therefore ensures a better predictive model.\n",
    "\n",
    "Better Predictive Model: OOB_Score helps in the least variance and hence it makes a much better predictive model than a model using other validation techniques.\n",
    "\n",
    "Less Variance : Since OOB_Score ensures no leakage, so there is no over-fitting of the data and hence least variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef4306",
   "metadata": {},
   "source": [
    "5.What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra\n",
    "randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random\n",
    "Forests?\n",
    "\n",
    "When we are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for Extra- Trees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. Moreover, since Extra-Trees don’t search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51ed00",
   "metadata": {},
   "source": [
    "6.Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training\n",
    "data?\n",
    "\n",
    "If our AdaBoost ensemble underfits the training data, we can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. We might also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6bb8d",
   "metadata": {},
   "source": [
    "7.Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the\n",
    "training set?\n",
    "\n",
    "If our Gradient Boosting ensemble overfits the training set, we should try decreasing the learning rate. We could also use early stopping to find the right number of predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
