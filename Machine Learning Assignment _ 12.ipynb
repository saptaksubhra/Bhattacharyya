{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d780aeb",
   "metadata": {},
   "source": [
    "1.What is prior probability? Give an example.\n",
    "\n",
    "Prior probability is a probability distribution that expresses established beliefs about an event before (i.e. prior to) new evidence is taken into account.\n",
    "\n",
    "For example,let's say that the  historical data suggests that around 60% of students who start college will graduate within 6 years. This is the prior probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12b596",
   "metadata": {},
   "source": [
    "2.What is posterior probability? Give an example.\n",
    "\n",
    "Posterior probability is the probability an event will happen after all evidence or background information has been taken into account.\n",
    "Posterior probability = prior probability + new evidence (called likelihood).\n",
    "\n",
    "For example,let's say that the  historical data suggests that around 60% of students who start college will graduate within 6 yearsof time. However,we think that figure is actually much lower, so we set out to collect new data. The evidence we collect suggests that the true figure is actually closer to 50%; This is the posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a81674",
   "metadata": {},
   "source": [
    "3.What is likelihood probability? Give an example.\n",
    "\n",
    "Likelihood refers to how well a sample provides support for particular values of a parameter in a model.\n",
    "\n",
    "Suppose we have a coin that is assumed to be fair. If we flip the coin one time, the probability that it will land on heads is 0.5.\n",
    "Now suppose we flip the coin 100 times and it only lands on heads 17 times. We would say that the likelihood that the coin is fair is quite low. If the coin was actually fair, we would expect it to land on heads much more often.\n",
    "When calculating the probability of a coin landing on heads, we simply assume that P(heads) = 0.5 on a given toss.\n",
    "However, when calculating the likelihood we’re trying to determine if the model parameter (p = 0.5) is actually correctly specified.\n",
    "In the example, a coin landing on heads only 17 out of 100 times makes us highly suspicious that the truly probability of the coin landing on heads on a given toss is actually p = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd594755",
   "metadata": {},
   "source": [
    "4.What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions. It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
    "\n",
    "Naïve Bayes classification is called Naïve because it assumes class conditional independence. The effect of an attribute value on a given class is independent of the values of the other attributes. This assumption is made to reduce computational costs and hence is considered Naïve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade108d8",
   "metadata": {},
   "source": [
    "5.What is optimal Bayes classifier?\n",
    "\n",
    "The Bayes Optimal Classifier is a probabilistic model that makes the most probable prediction for a new example. It is described using the Bayes Theorem that provides a principled way for calculating a conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d3829",
   "metadata": {},
   "source": [
    "6.Write any two features of Bayesian learning methods.\n",
    "\n",
    "Bayesian methods can accommodate hypotheses that make probabilistic predictions\n",
    "New instances can be classified by combining the predictions of multiple hypotheses,\n",
    "weighted by their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05cfdf",
   "metadata": {},
   "source": [
    "7.Define the concept of consistent learners.\n",
    "\n",
    "A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis. By definition, a consistent learner must produce a\n",
    "hypothesis in the version space for H given D. Therefore, to bound the number of examples needed by a consistent learner, we just need to bound the number of examples needed to ensure that the version-space contains no hypotheses with\n",
    "unacceptably high error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff4d8b",
   "metadata": {},
   "source": [
    "8.Write any two strengths of Bayes classifier.\n",
    "\n",
    "It is simple and easy to implement and doesn't require as much training data.\n",
    "It handles both continuous and discrete data and is highly scalable with the number of predictors and data points. It is fast and can be used to make real-time predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c7b37",
   "metadata": {},
   "source": [
    "9.Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life.\n",
    "This algorithm faces the 'zero-frequency problem' where it assigns zero probability to a categorical variable whose category in the test data set wasn't available in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19422fd8",
   "metadata": {},
   "source": [
    "10.Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1.Text classification : Naive Bayes is a learning algorithm commonly applied to text classification. Some of the applications of the Naive Bayes classifier are: (Automatic) Classification of emails in folders, so incoming email messages go into folders such as: “Family”, “Friends”, “Updates”, “Promotions”, etc.\n",
    "\n",
    "2.Spam filtering : With Bayes' Rule, we want to find the probability an email is spam, given it contains certain words. We do this by finding the probability that each word in the email is spam, and then multiply these probabilities together to get the overall email spam metric to be used in classification.\n",
    "\n",
    "3.Market sentiment analysis : Multinomial Naive Bayes classification algorithm tends to be a baseline solution for market sentiment analysis task. The basic idea of Naive Bayes technique is to find the probabilities of classes assigned to texts by using the joint probabilities of words and classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
