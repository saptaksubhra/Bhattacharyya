{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502de86f",
   "metadata": {},
   "source": [
    "1.A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "\n",
    "Here, dataset = {5,10,15,20,25,30,35}\n",
    "k = 2\n",
    "M1 and M2 are randomly selected centroid in the first set are 15 and 32 respectively\n",
    "So, M1 = 15, M2 = 32\n",
    "And the initial clusters are C1 ={15} and C2 = {32}\n",
    "The Euclidean distance is \n",
    "D = root over of (x - a)^2 = x - a \n",
    "D1 is the distance from M1 and D2 is the distance from M2\n",
    "D1 = 15 - 5 = 10 , D2 = 32 - 5 = 27\n",
    "\n",
    "And in case of second set, C1 = {12} and C2 = {30}\n",
    "\n",
    "So, D1 = 12 - 5 = 7 and D2 = 30 - 5 = 25\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "So for first set of centroid values the SSE = 10^2 + 27^2 = 100 + 729 = 829\n",
    "and for second set of centroid values the SSE = 7^2 + 25^2 = 49 + 625 = 674"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13e6e5",
   "metadata": {},
   "source": [
    "2.Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "Let I = {I1, I2,…, Im} be an itemset. Let D, the data, be a set of database transactions where each transaction T is a nonempty itemset such that T ⊆ I. Each transaction is associated with an identifier, called a TID(or Tid). Let A be a set of items(itemset). T is the Transaction which is said to contain A if A ⊆ T. An Association Rule is an implication of the form A ⇒ B, where A ⊂ I, B ⊂ I,  and A ∩B = φ.\n",
    "\n",
    "The rule A ⇒ B holds in the data set(transactions) D with supports, where ‘s’ is the percentage of transactions in D that contain A ∪ B (that is the union of set A and set B, or, both A and B). This is taken as the probability, P(A ∪ B). Rule A ⇒ B has confidence c in the transaction set D, where c is the percentage of transactions in D containing A that also contains B. This is taken to be the conditional probability, like P(B|A). That is,\n",
    "\n",
    "support(A⇒ B) =P(A ∪  B) \n",
    "\n",
    "confidence(A⇒ B) =P(B|A)\n",
    "\n",
    "Rules that satisfy both a minimum support threshold (called min sup) and a minimum confidence threshold (called min conf ) are called “Strong”.\n",
    "\n",
    "Confidence(A⇒ B) = P(B|A) =\n",
    "support(A ∪ B) /support(A) =\n",
    "support count(A ∪ B) / support count(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0595341",
   "metadata": {},
   "source": [
    "3.Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "Apriori algorithm refers to an algorithm that is used in mining frequent products sets and relevant association rules. Generally, the apriori algorithm operates on a database containing a huge number of transactions. For example, the items customers but at a Big Bazar.\n",
    "\n",
    "Example: Suppose we have the following dataset that has various transactions, and from this dataset, we need to find the frequent itemsets and generate the association rules using the Apriori algorithm:\n",
    "\n",
    "\n",
    "TID             ITEMSETS\n",
    "T1              A,B\n",
    "T2              B,D\n",
    "T3              B,C\n",
    "T4              A,B,D\n",
    "T5              A,C\n",
    "T6              B,C\n",
    "T7              A,C\n",
    "T8              A,B,C,E\n",
    "T9              A,B,C\n",
    "\n",
    "GIVEN : MINIMUMM SUPPORT = 2 AND MINIMUM CONFIDENCE = 50%\n",
    "\n",
    "Step-1: Calculating C1 and L1:, Step-2: Candidate Generation C2, and L2:, Step-3: Candidate generation C3, and L3:, Step-4: Finding the association rules for the subsets: To generate the association rules, first, we will create a new table with the possible rules from the occurred combination {A, B.C}. For all the rules, we will calculate the Confidence using formula sup( A ^B)/A. After calculating the confidence value for all rules, we will exclude the rules that have less confidence than the minimum threshold(50%).\n",
    "As the given threshold or minimum confidence is 50%, so the first three rules A ^B → C, B^C → A, and A^C → B can be considered as the strong association rules for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ef2df",
   "metadata": {},
   "source": [
    "4.In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration.\n",
    "\n",
    "For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences. However, for gene expression, correlation distance is often used.\n",
    "In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e0298",
   "metadata": {},
   "source": [
    "5.In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "Essentially, the process goes as follows:\n",
    "\n",
    "Select k centroids. These will be the center point for each segment.\n",
    "Assign data points to nearest centroid.\n",
    "Reassign centroid value to be the calculated mean value for each cluster.\n",
    "Reassign data points to nearest centroid.\n",
    "Repeat until data points stay in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504fe5b",
   "metadata": {},
   "source": [
    "6.At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters.\n",
    "\n",
    "At the start of clustering exercise, one method for determining the required number of clusters is average silhouette method. It measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.\n",
    "\n",
    "Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da48712",
   "metadata": {},
   "source": [
    "7.Discuss the k-means algorithm's advantages and disadvantages.\n",
    "\n",
    "Advantages of k-means algorithm:\n",
    "Scales to large data sets. Guarantees convergence. Can warm-start the positions of centroids. Easily adapts to new examples. Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
    "\n",
    "Disadvantages of k-means algorithm:\n",
    "It requires to specify the number of clusters (k) in advance. It can not handle noisy data and outliers. It is not suitable to identify clusters with non-convex shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb515c5",
   "metadata": {},
   "source": [
    "8.Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "The basic criterion for any clustering is distance. Objects that are near each other should belong to the same cluster, and objects that are far from each other should belong to different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6f10f",
   "metadata": {},
   "source": [
    "9.During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); \n",
    "C2: (0,4), (4,0), \n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering's SSE be?\n",
    "\n",
    "Finding centroid for data points in cluster C1 = ((2+4+6)/3, (2+4+6)/3) = (4, 4)\n",
    "\n",
    "Finding centroid for data points in cluster C2 = ((0+4)/2, (4+0)/2) = (2, 2)\n",
    "\n",
    "Finding centroid for data points in cluster C3 = ((5+9)/2, (5+9)/2) = (7, 7)\n",
    "\n",
    "Hence, C1: (4,4),  C2: (2,2), C3: (7,7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee36514",
   "metadata": {},
   "source": [
    "10.In a software project, the team is attempting to determine if software flaws discovered during\n",
    "testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "k-means algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
