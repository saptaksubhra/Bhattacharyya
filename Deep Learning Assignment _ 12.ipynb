{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed032007",
   "metadata": {},
   "source": [
    "1.How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "The size of one tensor must be equal to the size of the other tensor. If we want to broadcast in the other dimension, we have to change the shape of our vector. This is done with the unsqueeze method in PyTorch. It has been exemplified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6382f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "c = tensor([16.,25,36])\n",
    "m = tensor([[1., 2.5, 3.6], [4,5,6], [36,43,108]])\n",
    "c = c.unsqueeze(1)\n",
    "m.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb1a88ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([1, 3, 1]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "#The unsqueeze command can be replaced by None indexing.\n",
    "\n",
    "c.shape, c[None,:].shape,c[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc12a0f",
   "metadata": {},
   "source": [
    "3.How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "The commonly used way to store such data is in a single array that is laid out as a single, contiguous block within memory. More concretely, a 3x3x3 tensor would be stored simply as a single array of 27 values, one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b62e7",
   "metadata": {},
   "source": [
    "4.When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1acdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 5  7  9]\n",
      " [ 8 10 12]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    vector_row = np.array([1,2,3]) # creating a vector as a row (size 3)\n",
    "\n",
    "    matrix = np.array([[1,2,3], # creating a 3X3 matrix\n",
    "                  [4,5,6],\n",
    "                   [7,8,9]])\n",
    "\n",
    "    c  = np.add(vector_row, matrix)\n",
    "    print(c)\n",
    "    print(type(c))\n",
    "except:\n",
    "    print('addition or matrix and vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf772c4f",
   "metadata": {},
   "source": [
    "5.Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "Broadcasting should not increase the memory usage, but we would definitely need to store the result.\n",
    "Here is an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b36c6c",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "x = torch.randn(30000, 1, 10).cuda()\n",
    "y = torch.randn(20, 10).cuda()\n",
    "\n",
    "print('before sub')\n",
    "print('mem expected in MB: ', (x.nelement() + y.nelement()) * 4 / 1024**2)\n",
    "print('mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "print('max mem allocated in MR: ', torch.cuda.max_memory_allocated() / 1024**2)\n",
    "\n",
    "res = x - y\n",
    "print('after sub')\n",
    "print('mem expected in MB: ', (x.nelement() + y.nelement() + res.nelement()) * 4 / 1024**2)\n",
    "print('mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "print('max mem allocated in MR: ', torch.cuda.max_memory_allocated() / 1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbb0a7",
   "metadata": {},
   "source": [
    "before sub\n",
    "mem expected in MB:  1.145172119140625\n",
    "mem allocated in MB:  1.1455078125\n",
    "max mem allocated in MR:  1.1455078125\n",
    "after sub\n",
    "mem expected in MB:  24.033355712890625\n",
    "mem allocated in MB:  24.03369140625\n",
    "max mem allocated in MR:  24.03369140625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28005a7f",
   "metadata": {},
   "source": [
    "From the above output, we can see that the expected memory matches the allocated and maximum allocated memory closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef54f05",
   "metadata": {},
   "source": [
    "Memory leak situation happens due to expand_as. The reasonis being the meaningless models where expand is used. As a consequence, memory will keep increasing. It should be noted that even without using expand_as, the memory usauge can also increase for a while, but it will finally stablize at a level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb21c57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25,  28,  31,  34,  37],\n",
       "        [ 70,  82,  94, 106, 118],\n",
       "        [115, 136, 157, 178, 199]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.Implement matmul using Einstein summation.\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.arange(9).reshape(3,3)\n",
    "b = torch.arange(15).reshape(3,5)\n",
    "torch.einsum('ik,kj->ij', [a,b])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3459c3",
   "metadata": {},
   "source": [
    "7.What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "The left hand side represents the operands dimensions, separated by commas. Here we have two tensors that each have two dimensions (i,k and k,j). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7487ded",
   "metadata": {},
   "source": [
    "8.What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "There are essentially three rules of Einstein summation notation, namely:\n",
    "\n",
    "Repeated indices are implicitly summed over.\n",
    "Each index can appear at most twice in any term.\n",
    "Each term must contain identical non-repeated indices.\n",
    "\n",
    "Einstein summation is a very practical way of expressing operations involving indexing and sum of products. In Eistein summation, einsum function  is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34fdc4",
   "metadata": {},
   "source": [
    "9.What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "Forward Propagation or Forward Pass is the way to move from the Input layer (left) to the Output layer (right) in the neural network. The process of moving from the right to left i.e backward from the Output to the Input layer is called the Backward Propagation or Backward Pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65859ba1",
   "metadata": {},
   "source": [
    "10.Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "The activation function or activations is the most important factor in a neural network which decides whether or not a neuron will be activated or not and transferred to the next layer. This simply means that it will decide whether the neuron's input to the network is relevant or not in the process of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b787a",
   "metadata": {},
   "source": [
    "11.What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "A standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. A standard deviation close to zero indicates that data points are close to the mean, whereas a high or low standard deviation indicates data points are respectively above or below the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361a33c",
   "metadata": {},
   "source": [
    "12.How can weight initialization help avoid this problem?\n",
    "\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.\n",
    "\n",
    "There are different weight intialization techniques.\n",
    "Zero Initialization (Initialized all weights to 0), Random Initialization (Initialized weights randomly).\n",
    "Best Practices for Weight Initialization.\n",
    "\n",
    "Use RELU or leaky RELU as the activation function, as they both are relatively robust to the vanishing or exploding gradient problems (especially for networks that are not too deep). In the case of leaky RELU, they never have zero gradients. Thus they never die and training continues.\n",
    "\n",
    "Use Heuristics for weight initialization: For deep neural networks, we can use any of the following heuristics to initialize the weights depending on the chosen non-linear activation function. While these heuristics do not completely solve the exploding or vanishing gradients problems, they help to reduce it to a great extent. \n",
    "\n",
    "Gradient Clipping:  It is another way for dealing with the exploding gradient problem. In this technique, we set a threshold value, and if our chosen function of a gradient is larger than this threshold, then we set it to another value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
