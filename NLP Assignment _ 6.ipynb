{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375e3ec1",
   "metadata": {},
   "source": [
    "1.What are Vanilla autoencoders\n",
    "\n",
    "Autoencoders are learned automatically from data examples. It means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input and that it does not require any new engineering, only the appropriate training data.\n",
    "Vanilla autoencoder : In its simplest form, the autoencoder is a three layers net, i.e. a neural net with one hidden layer. The input and output are the same, and we learn how to reconstruct the input, for example using the adam optimizer and the mean squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418edfd",
   "metadata": {},
   "source": [
    "2.What are Sparse autoencoders\n",
    "\n",
    "A sparse autoencoder is one of a range of types of autoencoder artificial neural networks that work on the principle of unsupervised machine learning. Autoencoders are a type of deep network that can be used for dimensionality reduction â€“ and to reconstruct a model through backpropagation.\n",
    "Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time (thus, sparse). This constraint forces the model to respond to the unique statistical features of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3372f",
   "metadata": {},
   "source": [
    "3.What are Denoising autoencoders ?\n",
    "\n",
    "A Denoising Autoencoder is a modification on the autoencoder to prevent the network learning the identity function. Specifically, if the autoencoder is too big, then it can just learn the data, so the output equals the input, and does not perform any useful representation learning or dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf1f5d",
   "metadata": {},
   "source": [
    "4.What are Convolutional autoencoders\n",
    "\n",
    "Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddf16e",
   "metadata": {},
   "source": [
    "5.What are Stacked autoencoders\n",
    "\n",
    "Stacked Autoencoders: Autoencoder is a kind of unsupervised learning structure that owns three layers: input layer, hidden layer, and output layer as shown in Figure 1. The process of an autoencoder training consists of two parts: encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e06fd",
   "metadata": {},
   "source": [
    "6.Explain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "The simplest LSTM autoencoder is one that learns to reconstruct each input sequence.\n",
    "\n",
    "For these demonstrations, we will use a dataset of one sample of nine time steps and one feature:\n",
    "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "We can start-off by defining the sequence and reshaping it into the preferred shape of [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096451d",
   "metadata": {},
   "source": [
    "try:\n",
    "    from numpy import array\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import RepeatVector\n",
    "    from keras.layers import TimeDistributed\n",
    "    from keras.utils import plot_model\n",
    "    # define input sequence\n",
    "    sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    # reshape input into [samples, timesteps, features]\n",
    "    n_in = len(sequence)\n",
    "    sequence = sequence.reshape((1, n_in, 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "    model.add(RepeatVector(n_in))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "    plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "    # demonstrate recreation\n",
    "    yhat = model.predict(sequence, verbose=0)\n",
    "    print(yhat[0,:,0])\n",
    "except:\n",
    "    print('Reconstruction LSTM Autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb621b3",
   "metadata": {},
   "source": [
    "7.Explain Extractive summarization\n",
    "\n",
    "The extractive text summarising approach entails extracting essential words from a source material and combining them to create a summary. This approach works by detecting key chunks of the text, cutting them out, then stitching them back together to create a shortened form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369912f7",
   "metadata": {},
   "source": [
    "8.Explain Abstractive summarization\n",
    "\n",
    "Abstractive summarization is the technique of generating a summary of a text from its main ideas, not by copying verbatim most salient sentences from text. This is an important and challenge task in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72150f58",
   "metadata": {},
   "source": [
    "9.Explain Beam search\n",
    "\n",
    "Beam search is the most popular search strategy for the sequence to sequence Deep NLP algorithms like Neural Machine Translation, Image captioning, Chatbots, etc. Beam search considers multiple best options based on beamwidth using conditional probability, which is better than the sub-optimal Greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91549e1e",
   "metadata": {},
   "source": [
    "10.Explain Length normalization\n",
    "\n",
    "Document length normalization adjusts the term frequency or the relevance score in order to normalize the effect of document length on the document ranking. The reasons for employing a document length normalization method in an IR system are quite subtle. In general, the effect observed on the ranking by the presence of many lengthy documents in a collection is to favor their retrieval with respect to shorter documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebda956",
   "metadata": {},
   "source": [
    "11.Explain Coverage normalization\n",
    "\n",
    "In the field of linguistics and NLP, Morpheme is defined as a base form of the word. A token is basically made up of two components one is morphemes and the other is inflectional formlike prefix or suffix.\n",
    "For example, consider the word Antinationalist (Anti + national+ ist ) which is made up of Anti and ist as inflectional forms and national as the morpheme.\n",
    "Normalization is the process of converting a token into its base form. In the normalization process, the inflectional form of a word is removed so that the base form can be obtained. So in our above example, the normal form of antinationalist is national. Normalization is helpful in reducing the number of unique tokens present in the text, removing the variations in a text. and also cleaning the text by removing redundant information.\n",
    "Two popular methods used for normalization are stemming and lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a5464",
   "metadata": {},
   "source": [
    "12.Explain ROUGE metric evaluation\n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
