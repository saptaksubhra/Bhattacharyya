{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22d7b5c",
   "metadata": {},
   "source": [
    "1.Explain the Activation Functions in your own language\n",
    "\n",
    "a) sigmoid : This function takes any real value as input and outputs values in the range of 0 to 1. \n",
    "The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0 . Mathematically , we can say that , Sigmoid or logistic activation function is:\n",
    "\n",
    "f(x) = 1/ 1 + e^ -x\n",
    "\n",
    "\n",
    "b) tanh : Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. Mathematically , we can say that , tanh activation function is :\n",
    "\n",
    "f(x) = (e^x - e^-x )/ (e^x + e^-x)\n",
    "\n",
    "\n",
    "\n",
    "c) ReLU : ReLU stands for Rectified Linear Unit. Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient. \n",
    "The main catch here is that the ReLU function does not activate all the neurons at the same time. \n",
    "The neurons will only be deactivated if the output of the linear transformation is less than 0. Mathematically , we can say that , ReLU activation function is:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "\n",
    "d) ELU : Exponential Linear Unit, or ELU for short, is also a variant of ReLU that modifies the slope of the negative part of the function. ELU uses a log curve to define the negative values unlike the leaky ReLU and Parametric ReLU functions with a straight line. Mathematically , we can say that , ELU activation function is:\n",
    "\n",
    "f(x) = x when x >= 0\n",
    "     = α(e^x - 1) when x < 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "e) LeakyReLU : Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area. Mathematically , we can say that , Leaky ReLU activation function is:\n",
    "\n",
    "f(x) = max(0.1x, x)\n",
    "The advantages of Leaky ReLU are same as that of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f) swish : It is a self-gated activation function developed by researchers at Google. Swish consistently matches or outperforms ReLU activation function on deep networks applied to various challenging domains such as image classification, machine translation etc. Mathematically , we can say that , Swish activation function is:\n",
    "\n",
    "σ(x) = x / 1 + e ^ -x\n",
    "\n",
    "This function is bounded below but unbounded above i.e. Y approaches to a constant value as X approaches negative infinity but Y approaches to infinity as X approaches infinity.\n",
    "\n",
    "Mathematically it can be expressed as:\n",
    "\n",
    "f(x) = x * sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96114cfe",
   "metadata": {},
   "source": [
    "2.What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "If we increase the optimizer learning rate then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n",
    "\n",
    "If we decrease the optimizer learning rate then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8fd7a",
   "metadata": {},
   "source": [
    "3.What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "The number of layers in a model is referred to as its depth. Increasing the depth increases the capacity of the model. Training deep models, e.g. those with many hidden layers, can be computationally more efficient than training a single layer network with a vast number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9981232",
   "metadata": {},
   "source": [
    "4.What happens when you increase the size of batch computation?\n",
    "\n",
    "Large batch size means the model makes very large gradient updates and very small gradient updates. The size of the update depends heavily on which particular samples are drawn from the dataset. On the other hand using small batch size means the model makes updates that are all about the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b7a17",
   "metadata": {},
   "source": [
    "5.Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "Regularization assumes that least weights may produce simpler models and hence assist in avoiding overfitting. The model with the least overfitting score is accounted as the preferred choice for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291537dc",
   "metadata": {},
   "source": [
    "6.What are loss and cost functions in deep learning?\n",
    "\n",
    "A loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function.\n",
    "\n",
    "The cost function is the technique of evaluating the performance of our algorithm/model. It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction. It outputs a higher number if our predictions differ a lot from the actual values.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72927591",
   "metadata": {},
   "source": [
    "7.What do you mean by underfitting in neural networks?\n",
    "\n",
    "A model is said to be underfitting when it's not able to classify the data it was trained on. We can tell that a model is underfitting when the metrics given for the training data are poor, meaning that the training accuracy of the model is low and/or the training loss is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9316fe5",
   "metadata": {},
   "source": [
    "8.Why we use Dropout in Neural Networks?\n",
    "\n",
    "Dropout is a simple way to keep away neural networks from Overfitting. It is because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g. more nodes, may be required when using dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
